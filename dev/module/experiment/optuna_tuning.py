"""
Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
"""

import optuna
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, List
from pathlib import Path
import json
from datetime import datetime

from ..config import Config, OptunaConfig
from ..models.model import create_model
from ..data.dataset import ImageDataset
from ..data.transforms import get_train_transforms, get_val_transforms
from ..training.trainer import Trainer
from ..training.validator import Validator
from ..training.loss import get_loss_function
from ..utils.seed import set_seed
from .kfold import KFoldExperiment


class OptunaHyperparameterTuner:
    """
    Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ í´ë˜ìŠ¤
    
    Features:
        - ë² ì´ì§€ì•ˆ ìµœì í™” ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
        - ë¹ ë¥¸ ê²€ì¦ì„ ìœ„í•œ ì¶•ì†Œëœ K-Fold CV
        - ì¡°ê¸° ì¤‘ë‹¨ (Pruning) ì§€ì›
        - ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ë° ë¡œë“œ
        - íŠœë‹ ê³¼ì • ì‹œê°í™” ë°ì´í„° ì œê³µ
    """
    
    def __init__(self, config: OptunaConfig):
        """
        Args:
            config: Optuna íŠœë‹ ì„¤ì • ê°ì²´
        """
        
        self.config = config
        self.study = None
        self.best_params = None
        self.tuning_results = []
        
        print("âœ… Optuna Hyperparameter Tuner initialized")
        print(f"   - Max trials: {config.n_trials}")
        print(f"   - Timeout: {config.optuna_timeout}s" if config.optuna_timeout else "   - No timeout")
        print(f"   - Quick CV folds: {config.quick_cv_folds}")
        print(f"   - Quick epochs: {config.quick_epochs}")
    
    def objective(self, trial: optuna.Trial) -> float:
        """
        Optuna ëª©ì  í•¨ìˆ˜
        ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ë¹ ë¥¸ êµì°¨ ê²€ì¦ ìˆ˜í–‰ í›„ ì„±ëŠ¥ ë°˜í™˜
        
        Args:
            trial: Optuna trial ê°ì²´
            
        Returns:
            float: ìµœì í™”í•  ëª©í‘œ ê°’ (Macro F1 Score)
        """
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
        params = self.suggest_hyperparameters(trial)
        
        print(f"\n--- Trial {trial.number + 1} ---")
        print(f"Suggested params: {params}")
        
        # ì„¤ì • ì—…ë°ì´íŠ¸
        trial_config = self.update_config_with_params(params)
        
        # ë¹ ë¥¸ K-Fold CV ìˆ˜í–‰
        try:
            cv_score = self.quick_cross_validation(trial_config, trial)
            
            # ê²°ê³¼ ê¸°ë¡
            trial_result = {
                "trial_number": trial.number,
                "params": params,
                "cv_score": cv_score,
                "status": "COMPLETE"
            }
            self.tuning_results.append(trial_result)
            
            print(f"Trial {trial.number + 1} CV Score: {cv_score:.4f}")
            
            return cv_score
            
        except optuna.TrialPruned:
            print(f"Trial {trial.number + 1} was pruned")
            raise
        except Exception as e:
            print(f"Trial {trial.number + 1} failed: {str(e)}")
            
            # ì‹¤íŒ¨ ê¸°ë¡
            trial_result = {
                "trial_number": trial.number,
                "params": params,
                "cv_score": 0.0,
                "status": "FAILED",
                "error": str(e)
            }
            self.tuning_results.append(trial_result)
            
            return 0.0
    
    def suggest_hyperparameters(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
        
        Args:
            trial: Optuna trial ê°ì²´
            
        Returns:
            Dict[str, Any]: ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        """
        
        params = {
            # í•™ìŠµë¥ 
            "learning_rate": trial.suggest_float(
                "learning_rate",
                self.config.lr_range[0],
                self.config.lr_range[1],
                log=True
            ),
            
            # ë°°ì¹˜ í¬ê¸°
            "batch_size": trial.suggest_categorical(
                "batch_size",
                self.config.batch_size_choices
            ),
            
            # ì •ê·œí™” ê´€ë ¨
            "label_smoothing": trial.suggest_float("label_smoothing", 0.0, 0.3),
            "mixup_prob": trial.suggest_float("mixup_prob", 0.0, 0.5),
            "mixup_alpha": trial.suggest_float("mixup_alpha", 0.5, 2.0),
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            "grad_clip_norm": trial.suggest_float("grad_clip_norm", 0.5, 2.0),
            
            # ëª¨ë¸ ê´€ë ¨
            "model_name": trial.suggest_categorical(
                "model_name",
                ["efficientnet_b3", "efficientnet_b4", "resnet152", "convnext_base"]
            ),
            
            # ì´ë¯¸ì§€ í¬ê¸° (íš¨ìœ¨ì„±ì„ ìœ„í•´ ì œí•œì  ì„ íƒ)
            "img_size": trial.suggest_categorical("img_size", [224, 384]),
            
            # ì¦ê°• ê´€ë ¨
            "rotation_prob": trial.suggest_float("rotation_prob", 0.3, 0.8),
            "blur_prob": trial.suggest_float("blur_prob", 0.7, 1.0),
            "brightness_prob": trial.suggest_float("brightness_prob", 0.5, 0.9)
        }
        
        return params
    
    def update_config_with_params(self, params: Dict[str, Any]) -> Config:
        """
        ì œì•ˆëœ íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸
        
        Args:
            params: ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
            
        Returns:
            Config: ì—…ë°ì´íŠ¸ëœ ì„¤ì • ê°ì²´
        """
        
        # ê¸°ë³¸ ì„¤ì • ë³µì‚¬
        trial_config = Config()
        
        # ê¸°ë³¸ê°’ ë³µì‚¬
        for key, value in self.config.__dict__.items():
            if hasattr(trial_config, key):
                setattr(trial_config, key, value)
        
        # ì œì•ˆëœ íŒŒë¼ë¯¸í„°ë¡œ ì—…ë°ì´íŠ¸
        for key, value in params.items():
            if hasattr(trial_config, key):
                setattr(trial_config, key, value)
        
        # ë¹ ë¥¸ íŠœë‹ì„ ìœ„í•œ ì„¤ì • ì¡°ì •
        trial_config.n_folds = self.config.quick_cv_folds
        trial_config.epochs = self.config.quick_epochs
        trial_config.num_workers = min(trial_config.num_workers, 8)  # ë©”ëª¨ë¦¬ ì ˆì•½
        
        return trial_config
    
    def quick_cross_validation(self, config: Config, trial: optuna.Trial) -> float:
        """
        ë¹ ë¥¸ êµì°¨ ê²€ì¦ ìˆ˜í–‰
        
        Args:
            config: ì‹œí—˜ìš© ì„¤ì •
            trial: Optuna trial (ì¡°ê¸° ì¤‘ë‹¨ìš©)
            
        Returns:
            float: í‰ê·  CV F1 ìŠ¤ì½”ì–´
        """
        
        # ì‹œë“œ ê³ ì •
        set_seed(config.seed)
        
        # ê°€ìƒ ë°ì´í„° ë¡œë“œ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ë°ì´í„° ì‚¬ìš©)
        train_df = self.load_training_data()
        
        # ë¹ ë¥¸ K-Fold ì‹¤í—˜
        experiment = KFoldExperiment(config)
        
        # ë°ì´í„° ë¶„í• 
        splits = experiment.prepare_data_splits(train_df)
        
        # í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = ImageDataset(
            image_paths=[f"{config.train_path}{img_id}" for img_id in train_df['ID']],
            targets=train_df['target'].tolist(),
            transform=get_train_transforms(config)
        )
        
        fold_f1_scores = []
        
        # ê° Foldì— ëŒ€í•´ ë¹ ë¥¸ í•™ìŠµ
        for fold_idx, (train_indices, val_indices) in enumerate(splits):
            print(f"Quick training Fold {fold_idx + 1}/{len(splits)}...")
            
            # ë°ì´í„°ë¡œë” ìƒì„±
            train_loader, val_loader = experiment.create_fold_dataloaders(
                train_dataset, train_indices, val_indices
            )
            
            # ë¹ ë¥¸ í•™ìŠµ ìˆ˜í–‰
            fold_f1 = self.quick_train_single_fold(config, train_loader, val_loader, trial, fold_idx)
            fold_f1_scores.append(fold_f1)
            
            # ì¤‘ê°„ ê²°ê³¼ë¡œ ì¡°ê¸° ì¤‘ë‹¨ íŒë‹¨
            current_mean = np.mean(fold_f1_scores)
            trial.report(current_mean, fold_idx)
            
            if trial.should_prune():
                raise optuna.TrialPruned()
        
        # í‰ê·  F1 ìŠ¤ì½”ì–´ ë°˜í™˜
        mean_f1 = float(np.mean(fold_f1_scores))
        return mean_f1
    
    def quick_train_single_fold(
        self, 
        config: Config, 
        train_loader, 
        val_loader, 
        trial: optuna.Trial,
        fold_idx: int
    ) -> float:
        """
        ë‹¨ì¼ Fold ë¹ ë¥¸ í•™ìŠµ
        
        Args:
            config: ì„¤ì •
            train_loader: í›ˆë ¨ ë°ì´í„°ë¡œë”
            val_loader: ê²€ì¦ ë°ì´í„°ë¡œë”
            trial: Optuna trial
            fold_idx: Fold ë²ˆí˜¸
            
        Returns:
            float: ìµœê³  ê²€ì¦ F1 ìŠ¤ì½”ì–´
        """
        
        # ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤í•¨ìˆ˜ ìƒì„±
        model = create_model(config)
        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=1e-4)
        loss_fn = get_loss_function(config)
        
        trainer = Trainer(config)
        validator = Validator(config)
        
        best_val_f1 = 0.0
        
        # ë¹ ë¥¸ í•™ìŠµ (ì ì€ ì—í¬í¬)
        for epoch in range(config.epochs):
            # í•™ìŠµ
            train_results = trainer.train_one_epoch(model, train_loader, optimizer, loss_fn)
            
            # ê²€ì¦
            val_results = validator.validate_one_epoch(model, val_loader, loss_fn)
            
            # ìµœê³  ì„±ëŠ¥ ì¶”ì 
            current_val_f1 = val_results['val_f1']
            if current_val_f1 > best_val_f1:
                best_val_f1 = current_val_f1
            
            # ì¤‘ê°„ ë³´ê³  (ì—í¬í¬ë³„)
            trial.report(best_val_f1, fold_idx * config.epochs + epoch)
            
            # ì¡°ê¸° ì¤‘ë‹¨ í™•ì¸
            if trial.should_prune():
                raise optuna.TrialPruned()
        
        return best_val_f1
    
    def load_training_data(self) -> pd.DataFrame:
        """
        í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” config.get_train_csv_path()ë¥¼ ì‚¬ìš©
        
        Returns:
            pd.DataFrame: í›ˆë ¨ ë°ì´í„°í”„ë ˆì„
        """
        
        # ì‹¤ì œ êµ¬í˜„
        try:
            return pd.read_csv(self.config.get_train_csv_path())
        except FileNotFoundError:
            # í…ŒìŠ¤íŠ¸ìš© ê°€ìƒ ë°ì´í„°
            print("âš ï¸ Using fake data for testing (train.csv not found)")
            return pd.DataFrame({
                'ID': [f'img_{i:03d}.jpg' for i in range(100)],
                'target': np.random.randint(0, 17, 100)
            })
    
    def run_optimization(
        self, 
        study_name: str = None,
        direction: str = "maximize"
    ) -> Dict[str, Any]:
        """
        í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰
        
        Args:
            study_name: Optuna study ì´ë¦„
            direction: ìµœì í™” ë°©í–¥ ("maximize" or "minimize")
            
        Returns:
            Dict[str, Any]: ìµœì í™” ê²°ê³¼
        """
        
        print(f"\nğŸ” Starting hyperparameter optimization...")
        print(f"Target trials: {self.config.n_trials}")
        print(f"Timeout: {self.config.optuna_timeout}s" if self.config.optuna_timeout else "No timeout")
        
        # Study ìƒì„±
        if study_name is None:
            study_name = f"cv_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Pruner ì„¤ì • (MedianPruner ì‚¬ìš©)
        pruner = optuna.pruners.MedianPruner(
            n_startup_trials=3,  # ìµœì†Œ 3ë²ˆì˜ trial í›„ pruning ì‹œì‘
            n_warmup_steps=2,    # ìµœì†Œ 2ë²ˆì˜ fold í›„ pruning íŒë‹¨
            interval_steps=1     # ë§¤ stepë§ˆë‹¤ pruning í™•ì¸
        )
        
        self.study = optuna.create_study(
            study_name=study_name,
            direction=direction,
            pruner=pruner
        )
        
        # ìµœì í™” ì‹¤í–‰
        try:
            self.study.optimize(
                self.objective,
                n_trials=self.config.n_trials,
                timeout=self.config.optuna_timeout,
                show_progress_bar=True
            )
        except KeyboardInterrupt:
            print("\nâš ï¸ Optimization interrupted by user")
        
        # ê²°ê³¼ ì •ë¦¬
        optimization_results = self.summarize_optimization_results()
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
        self.best_params = self.study.best_params
        
        return optimization_results
    
    def summarize_optimization_results(self) -> Dict[str, Any]:
        """ìµœì í™” ê²°ê³¼ ìš”ì•½"""
        
        if self.study is None:
            return {"error": "No optimization study found"}
        
        # ê¸°ë³¸ í†µê³„
        completed_trials = [t for t in self.study.trials if t.state == optuna.trial.TrialState.COMPLETE]
        pruned_trials = [t for t in self.study.trials if t.state == optuna.trial.TrialState.PRUNED]
        failed_trials = [t for t in self.study.trials if t.state == optuna.trial.TrialState.FAIL]
        
        results = {
            "study_name": self.study.study_name,
            "total_trials": len(self.study.trials),
            "completed_trials": len(completed_trials),
            "pruned_trials": len(pruned_trials),
            "failed_trials": len(failed_trials),
            "best_value": self.study.best_value,
            "best_params": self.study.best_params,
            "best_trial_number": self.study.best_trial.number,
            "optimization_history": [
                {"trial": t.number, "value": t.value} 
                for t in completed_trials
            ],
            "param_importance": {}
        }
        
        # íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ê³„ì‚° (ì™„ë£Œëœ ì‹œë„ê°€ 10ê°œ ì´ìƒì¸ ê²½ìš°)
        if len(completed_trials) >= 10:
            try:
                importance = optuna.importance.get_param_importances(self.study)
                results["param_importance"] = importance
            except Exception as e:
                print(f"âš ï¸ Could not calculate parameter importance: {e}")
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"HYPERPARAMETER OPTIMIZATION RESULTS")
        print(f"{'='*60}")
        print(f"Best CV Score: {results['best_value']:.4f}")
        print(f"Best Trial: #{results['best_trial_number']}")
        print(f"Total Trials: {results['total_trials']} (Completed: {results['completed_trials']}, Pruned: {results['pruned_trials']})")
        print(f"\nBest Hyperparameters:")
        for param, value in results['best_params'].items():
            print(f"  {param}: {value}")
        
        if results['param_importance']:
            print(f"\nParameter Importance:")
            for param, importance in sorted(results['param_importance'].items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"  {param}: {importance:.3f}")
        
        return results
    
    def save_optimization_results(self, results: Dict[str, Any], filename: str = None):
        """ìµœì í™” ê²°ê³¼ ì €ì¥"""
        
        output_dir = Path(self.config.output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"optuna_results_{timestamp}.json"
        
        result_path = output_dir / filename
        
        # ê²°ê³¼ ì €ì¥
        save_data = {
            "optimization_results": results,
            "config": self.config.__dict__,
            "tuning_results": self.tuning_results,
            "timestamp": datetime.now().isoformat()
        }
        
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Optimization results saved: {result_path}")
    
    def get_best_config(self) -> Config:
        """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì • ê°ì²´ ìƒì„±"""
        
        if self.best_params is None:
            raise ValueError("No optimization results found. Run optimization first.")
        
        # ì „ì²´ í•™ìŠµìš© ì„¤ì •ìœ¼ë¡œ ë³µì›
        best_config = Config()
        
        # ê¸°ë³¸ê°’ ë³µì‚¬
        for key, value in self.config.__dict__.items():
            if hasattr(best_config, key) and key not in ['use_optuna', 'n_trials', 'optuna_timeout', 'quick_cv_folds', 'quick_epochs']:
                setattr(best_config, key, value)
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì ìš©
        for key, value in self.best_params.items():
            if hasattr(best_config, key):
                setattr(best_config, key, value)
        
        # ì „ì²´ í•™ìŠµìš©ìœ¼ë¡œ ë³µì›
        best_config.n_folds = 5  # ì›ë˜ fold ìˆ˜ë¡œ ë³µì›
        best_config.epochs = 15  # ì›ë˜ epoch ìˆ˜ë¡œ ë³µì›
        
        print(f"âœ… Best config created with optimized hyperparameters")
        print(f"   - CV Score: {self.study.best_value:.4f}")
        
        return best_config
    
    def create_study_visualization_data(self) -> Dict[str, Any]:
        """Study ì‹œê°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ìƒì„±"""
        
        if self.study is None:
            return {"error": "No study found"}
        
        completed_trials = [t for t in self.study.trials if t.state == optuna.trial.TrialState.COMPLETE]
        
        if not completed_trials:
            return {"error": "No completed trials found"}
        
        visualization_data = {
            "optimization_history": [
                {
                    "trial": t.number,
                    "value": t.value,
                    "params": t.params
                }
                for t in completed_trials
            ],
            "param_relationships": {},
            "best_trial_info": {
                "number": self.study.best_trial.number,
                "value": self.study.best_value,
                "params": self.study.best_params
            }
        }
        
        # íŒŒë¼ë¯¸í„°ë³„ ê´€ê³„ ë°ì´í„°
        for param_name in self.study.best_params.keys():
            param_values = []
            objective_values = []
            
            for trial in completed_trials:
                if param_name in trial.params:
                    param_values.append(trial.params[param_name])
                    objective_values.append(trial.value)
            
            visualization_data["param_relationships"][param_name] = {
                "param_values": param_values,
                "objective_values": objective_values
            }
        
        return visualization_data


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import tempfile
    
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
    config = OptunaConfig()
    config.device = "cpu"
    config.n_trials = 3  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    config.quick_cv_folds = 2
    config.quick_epochs = 1
    config.optuna_timeout = 60  # 1ë¶„ ì œí•œ
    config.lr_range = (1e-4, 1e-2)
    config.batch_size_choices = [8, 16]
    
    print("=== Optuna Hyperparameter Tuner Test ===")
    
    with tempfile.TemporaryDirectory() as tmp_dir:
        config.output_path = tmp_dir + "/"
        
        # íŠœë„ˆ ìƒì„±
        tuner = OptunaHyperparameterTuner(config)
        
        # ìµœì í™” ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš© ì§§ì€ ì‹œê°„)
        print("Testing optimization...")
        results = tuner.run_optimization(study_name="test_study")
        
        print(f"Optimization completed with {results['completed_trials']} trials")
        
        # ìµœì  ì„¤ì • ìƒì„± í…ŒìŠ¤íŠ¸
        if tuner.best_params:
            best_config = tuner.get_best_config()
            print(f"Best config created: LR={best_config.learning_rate:.6f}")
        
        # ì‹œê°í™” ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸
        viz_data = tuner.create_study_visualization_data()
        print(f"Visualization data created with {len(viz_data.get('optimization_history', []))} points")
    
    print("âœ… Optuna Hyperparameter Tuner test completed successfully")
