{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zkH9T_86lDSS"
   },
   "source": [
    "## 1. Prepare Environments\n",
    "\n",
    "* ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•œ êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ë§ˆìš´íŠ¸í•©ë‹ˆë‹¤.\n",
    "* í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8489,
     "status": "ok",
     "timestamp": 1700314558888,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "NC8V-D393wY4",
    "outputId": "e9927325-26c4-4b89-9c51-c1d6541388d6"
   },
   "outputs": [],
   "source": [
    "# # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "# !pip install timm\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install optuna\n",
    "# !apt install -y libgl1-mesa-glx\n",
    "# !pip install albumentations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXa_FPM73R9f"
   },
   "source": [
    "## 2. Import Library & Define Functions\n",
    "* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9396,
     "status": "ok",
     "timestamp": 1700314592802,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "3BaoIkv5Xwa0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import optuna, math\n",
    "import timm\n",
    "import torch\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed Precisionìš©\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œê°í™”ìš©)\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## 3. Hyper-parameters\n",
    "* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "\n",
    "# model config\n",
    "# model_name = 'tf_efficientnetv2_b3' # 'resnet50' 'efficientnet-b0', ...\n",
    "# model_name = 'swin_base_patch4_window12_384_in22k'\n",
    "model_name = 'convnext_large_384_in22ft1k'\n",
    "# model_name = 'convnextv2_base.fcmae_ft_in22k_in1k_384'\n",
    "# model_name = 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k' # openclip\n",
    "# model_name = 'vit_base_patch16_384.augreg_in1k' # augreg\n",
    "# model_name = 'eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k' # eva-02 ë©€í‹°ëª¨ë‹¬\n",
    "# model_name = 'eva02_large_patch14_448.mim_in22k_ft_in1k' #448 í…ŒìŠ¤íŠ¸ìš©\n",
    "# model_name = 'vit_base_patch14_reg4_dinov2.lvd142m' # dinov2 reg4\n",
    "\n",
    "# model_name = 'eva02_large_patch14_448.mim_in22k_ft_in1k' #448 í…ŒìŠ¤íŠ¸ìš©\n",
    "\n",
    "# training config\n",
    "img_size = 512\n",
    "LR = 2e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "num_workers = 8\n",
    "EMA = True  # Exponential Moving Average ì‚¬ìš© ì—¬ë¶€"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## 4. Load Data\n",
    "* í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ë¡œë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold ì ìš©"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmm5h3J-pXNV"
   },
   "source": [
    "## 5. Train Model\n",
    "* ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fold 1 model loaded from models/fold_1_best.pth\n",
      "âœ“ Fold 2 model loaded from models/fold_2_best.pth\n",
      "âœ“ Fold 3 model loaded from models/fold_3_best.pth\n",
      "âœ“ Fold 4 model loaded from models/fold_4_best.pth\n",
      "âœ“ Fold 5 model loaded from models/fold_5_best.pth\n",
      "Using ensemble of all 5 fold models for inference\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold ì•™ìƒë¸” ëª¨ë¸ ì¤€ë¹„\n",
    "ensemble_models = []\n",
    "for i in range(5):  # fold ê°œìˆ˜ë§Œí¼\n",
    "    fold_model = timm.create_model(model_name, pretrained=False, num_classes=17).to(device)  # pretrained=Falseë¡œ ë³€ê²½\n",
    "    \n",
    "    # foldë³„ ì €ì¥ëœ íŒŒì¼ ë¡œë“œ\n",
    "    checkpoint = torch.load(f'models/fold_{i+1}_best.pth')  # foldë³„ íŒŒì¼ ê²½ë¡œ\n",
    "    fold_model.load_state_dict(checkpoint)\n",
    "    fold_model.eval()\n",
    "    \n",
    "    ensemble_models.append(fold_model)\n",
    "    print(f\"âœ“ Fold {i+1} model loaded from models/fold_{i+1}_best.pth\")\n",
    "\n",
    "print(f\"Using ensemble of all {len(ensemble_models)} fold models for inference\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# 6. Inference & Save File\n",
    "* í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ ì„ ì§„í–‰í•˜ê³ , ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Scaling í´ë˜ìŠ¤ ì •ì˜\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "def get_classification_confidence(image_path, model):\n",
    "    \"\"\"ë¶„ë¥˜ ëª¨ë¸ ì‹ ë¢°ë„ ê¸°ë°˜ í’ˆì§ˆ ì¸¡ì •\"\"\"\n",
    "    try:\n",
    "        # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return 0.0\n",
    "        \n",
    "        # RGB ë³€í™˜\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # ëª¨ë¸ ì…ë ¥ìš© transform ì ìš©\n",
    "        transform = A.Compose([\n",
    "            A.LongestMaxSize(max_size=512),\n",
    "            A.PadIfNeeded(min_height=512, min_width=512, border_mode=0, value=0),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
    "        processed = transform(image=img_rgb)['image'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # ëª¨ë¸ ì˜ˆì¸¡ (ì²« ë²ˆì§¸ ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš©)\n",
    "        with torch.no_grad():\n",
    "            logits = ensemble_models[0](processed)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            confidence = torch.max(probs).item()\n",
    "        \n",
    "        return confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ì‹ ë¢°ë„ ì¸¡ì • ì‹¤íŒ¨ ({image_path}): {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# ë¬¸ì„œ íŠ¹í™” ì ì‘í˜• ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def adaptive_preprocessing(image_path, quality_threshold=0.99):\n",
    "    \"\"\"ë¬¸ì„œ ì´ë¯¸ì§€ ì „ë¬¸ê°€ ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ì „ì²˜ë¦¬\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None, 0.0, 0.0\n",
    "        \n",
    "        # ë¶„ë¥˜ ì‹ ë¢°ë„ ê¸°ë°˜ í’ˆì§ˆ ì¸¡ì •\n",
    "        original_confidence = get_classification_confidence(image_path, ensemble_models[0])\n",
    "        \n",
    "        # ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ì „ì²˜ë¦¬ ìŠ¤í‚µ\n",
    "        if original_confidence > quality_threshold:\n",
    "        # if False: # ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ ì „ì²˜ë¦¬ ì‹œë„\n",
    "            return img, original_confidence, original_confidence\n",
    "        \n",
    "        # 1) í’ˆì§ˆ ì§„ë‹¨\n",
    "        processed_img = img.copy()\n",
    "        gray = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        g_std = gray.std()  # ëŒ€ë¹„\n",
    "        lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()  # ë¸”ëŸ¬\n",
    "        mean_bright = gray.mean()  # ë°ê¸°\n",
    "        \n",
    "        # ìŠ¤í ê°ë„ ì¸¡ì •\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)\n",
    "        skew_angle = 0\n",
    "        if lines is not None and len(lines) > 5:\n",
    "            angles = []\n",
    "            for line in lines[:20]:\n",
    "                if len(line[0]) >= 2:\n",
    "                    rho, theta = line[0]\n",
    "                    angle = theta * 180 / np.pi - 90\n",
    "                    if abs(angle) < 45:\n",
    "                        angles.append(angle)\n",
    "            if angles:\n",
    "                skew_angle = abs(np.median(angles))\n",
    "        \n",
    "        # 2) ë¼ìš°íŒ… ê²°ì •\n",
    "        needs_deskew = skew_angle >= 8\n",
    "        low_contrast = g_std < 35\n",
    "        is_blurry = 50 <= lap_var < 150\n",
    "        very_blurry = lap_var < 50\n",
    "        too_bright = mean_bright > 180\n",
    "        has_noise = lap_var < 100\n",
    "        \n",
    "        # 3) í”„ë¦¬ì…‹ ì„ íƒ\n",
    "        if very_blurry and too_bright and has_noise:\n",
    "            preset = \"HEAVY\"\n",
    "        elif needs_deskew or skew_angle >= 5:\n",
    "            preset = \"MEDIUM\"  \n",
    "        else:\n",
    "            preset = \"LIGHT\"\n",
    "        \n",
    "        # 4) ì „ì²˜ë¦¬ ì ìš©\n",
    "        \n",
    "        # ë‹¨ê³„ 0: í”Œë¦½ ê°ì§€ ë° ë³´ì •\n",
    "        flipped = cv2.flip(processed_img, 1)\n",
    "        gray_current = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "        gray_flipped = cv2.cvtColor(flipped, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        def get_text_density(image):\n",
    "            binary = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                         cv2.THRESH_BINARY, 15, 10)\n",
    "            return np.sum(binary == 0) / binary.size\n",
    "        \n",
    "        original_density = get_text_density(gray_current)\n",
    "        flipped_density = get_text_density(gray_flipped)\n",
    "        \n",
    "        if flipped_density > original_density * 1.1:\n",
    "            processed_img = flipped\n",
    "        \n",
    "        # ë‹¨ê³„ 1: Deskew (í•„ìš”ì‹œ)\n",
    "        if needs_deskew and preset in [\"MEDIUM\", \"HEAVY\"]:\n",
    "            h, w = processed_img.shape[:2]\n",
    "            center = (w//2, h//2)\n",
    "            \n",
    "            valid_angles = []\n",
    "            if lines is not None:\n",
    "                for line in lines[:10]:\n",
    "                    if len(line[0]) >= 2:\n",
    "                        rho, theta = line[0]\n",
    "                        angle = theta * 180 / np.pi - 90\n",
    "                        if abs(angle) < 15:\n",
    "                            valid_angles.append(angle)\n",
    "            \n",
    "            if len(valid_angles) > 0:\n",
    "                rotation_angle = np.median(valid_angles)\n",
    "                if abs(rotation_angle) >= 3:\n",
    "                    M = cv2.getRotationMatrix2D(center, rotation_angle, 1.0)\n",
    "                    processed_img = cv2.warpAffine(processed_img, M, (w, h), \n",
    "                                                 borderMode=cv2.BORDER_CONSTANT, \n",
    "                                                 borderValue=(255, 255, 255))\n",
    "        \n",
    "        # ë‹¨ê³„ 2: ë°ê¸° ì¡°ì • (HEAVYë§Œ)\n",
    "        if too_bright and preset == \"HEAVY\":\n",
    "            processed_img = np.power(processed_img/255.0, 0.92) * 255\n",
    "            processed_img = processed_img.astype(np.uint8)\n",
    "        \n",
    "        # ë‹¨ê³„ 3: NLM ë…¸ì´ì¦ˆ ì œê±° (HEAVYë§Œ)\n",
    "        if has_noise and preset == \"HEAVY\":\n",
    "            processed_img = cv2.fastNlMeansDenoisingColored(processed_img, None, 3, 3, 7, 21)\n",
    "        \n",
    "        # ë‹¨ê³„ 4: CLAHE (ì €ëŒ€ë¹„ì¸ ê²½ìš°)\n",
    "        if low_contrast:\n",
    "            lab = cv2.cvtColor(processed_img, cv2.COLOR_BGR2LAB)\n",
    "            l_channel = lab[:, :, 0]\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "            l_channel = clahe.apply(l_channel)\n",
    "            lab[:, :, 0] = l_channel\n",
    "            processed_img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # ë‹¨ê³„ 5: Unsharp (ë¸”ëŸ¬ì¸ ê²½ìš°)\n",
    "        if is_blurry or very_blurry:\n",
    "            gray = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "            gaussian = cv2.GaussianBlur(gray, (0, 0), 1.0)\n",
    "            unsharp = cv2.addWeighted(gray, 1.4, gaussian, -0.4, 0)\n",
    "            unsharp = np.clip(unsharp, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            hsv = cv2.cvtColor(processed_img, cv2.COLOR_BGR2HSV)\n",
    "            hsv[:, :, 2] = unsharp\n",
    "            processed_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "        \n",
    "        # í’ˆì§ˆ ê²€ì‚¬\n",
    "        temp_path = image_path.replace('.jpg', '_temp_processed.jpg')\n",
    "        cv2.imwrite(temp_path, processed_img)\n",
    "        processed_confidence = get_classification_confidence(temp_path, ensemble_models[0])\n",
    "        os.remove(temp_path)\n",
    "\n",
    "        if processed_confidence < original_confidence * 0.8:\n",
    "            return img, original_confidence, original_confidence\n",
    "\n",
    "        return processed_img, original_confidence, processed_confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ì „ì²˜ë¦¬ ì‹¤íŒ¨ ({image_path}): {e}\")\n",
    "        img = cv2.imread(image_path)\n",
    "        original_confidence = 0.5 if img is not None else 0.0\n",
    "        return img, original_confidence, original_confidence\n",
    "\n",
    "def assess_image_quality(image_path):\n",
    "    \"\"\"ì´ë¯¸ì§€ í’ˆì§ˆì„ 0-1 ì ìˆ˜ë¡œ í‰ê°€\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return 0.0\n",
    "            \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # ë¸”ëŸ¬ ì¸¡ì •\n",
    "        blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        blur_normalized = min(blur_score / 1000.0, 1.0)\n",
    "        \n",
    "        # ëŒ€ë¹„ ì¸¡ì •\n",
    "        contrast_score = gray.std()\n",
    "        contrast_normalized = min(contrast_score / 80.0, 1.0)\n",
    "        \n",
    "        # ë°ê¸° ë¶„í¬ ì¸¡ì •\n",
    "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "        hist = hist.flatten()\n",
    "        hist = hist[hist > 0]\n",
    "        if len(hist) > 1:\n",
    "            prob = hist / hist.sum()\n",
    "            brightness_score = -np.sum(prob * np.log2(prob + 1e-8))\n",
    "            brightness_normalized = brightness_score / 8.0\n",
    "        else:\n",
    "            brightness_normalized = 0.0\n",
    "            \n",
    "        quality_score = (\n",
    "            0.5 * blur_normalized +\n",
    "            0.3 * contrast_normalized +\n",
    "            0.2 * brightness_normalized\n",
    "        )\n",
    "        \n",
    "        return min(quality_score, 1.0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "print(\"ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_tta_transforms = [\n",
    "    # ì›ë³¸\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 90ë„ íšŒì „ë“¤\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[90, 90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[180, 180], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[-90, -90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # ë°ê¸° ê°œì„ \n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[0.3, 0.3], contrast_limit=[0.3, 0.3], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA ì¶”ë¡ ì„ ìœ„í•œ Dataset í´ë˜ìŠ¤\n",
    "class TTAImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms  # ì—¬ëŸ¬ transformì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ìŒ\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        #img = np.array(Image.open(os.path.join(self.path, name)).convert('RGB'))\n",
    "\n",
    "        # ì ì‘í˜• ì „ì²˜ë¦¬ ì ìš© (ì˜¤ì—¼ë„ ë†’ì€ ì´ë¯¸ì§€ë§Œ ìë™ ì„ ë³„)\n",
    "        processed_img, original_confidence, processed_confidence = adaptive_preprocessing(img_path)\n",
    "\n",
    "        # ì „ì²˜ë¦¬ ì ìš© ì—¬ë¶€ íŒë‹¨\n",
    "        preprocessing_applied = abs(processed_confidence - original_confidence) > 0.001\n",
    "\n",
    "        # í†µê³„ ìˆ˜ì§‘\n",
    "        collect_preprocessing_stats(original_confidence, processed_confidence, preprocessing_applied)\n",
    "        \n",
    "        if processed_img is not None:\n",
    "            # OpenCV BGRì„ RGBë¡œ ë³€í™˜\n",
    "            img = cv2.cvtColor(processed_img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            # ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        # ëª¨ë“  transformì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            aug_img = transform(image=img)['image']\n",
    "            augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA Dataset size: 3140\n"
     ]
    }
   ],
   "source": [
    "# TTA Dataset ìƒì„±\n",
    "tta_dataset = TTAImageDataset(\n",
    "    \"../data/sample_submission.csv\",\n",
    "    \"../data/test/\",\n",
    "    essential_tta_transforms\n",
    ")\n",
    "\n",
    "# TTA DataLoader (ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "tta_loader = DataLoader(\n",
    "    tta_dataset,\n",
    "    batch_size=64,  # TTAëŠ” ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì¤„ì„\n",
    "    shuffle=False,\n",
    "    # num_workers=num_workers,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"TTA Dataset size: {len(tta_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œìš©\n",
    "def ensemble_tta_inference(models, loader, transforms, confidence_threshold=0.9):\n",
    "    \"\"\"5-Fold ëª¨ë¸ ì•™ìƒë¸” + TTA ì¶”ë¡ \"\"\"\n",
    "    temp_scaling = TemperatureScaling().to(device)  # Temperature Scaling ì¶”ê°€\n",
    "    all_predictions = []\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Ensemble TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        \n",
    "        ensemble_probs = torch.zeros(batch_size, 17).to(device)\n",
    "                \n",
    "        # ê° fold ëª¨ë¸ë³„ ì˜ˆì¸¡\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                # ê° TTA ë³€í˜•ë³„ ì˜ˆì¸¡\n",
    "                for images in images_list:\n",
    "                    images = images.to(device)\n",
    "                    preds = model(images)\n",
    "                    probs = torch.softmax(preds, dim=1)\n",
    "                    \n",
    "                    # Temperature Scaling ì ìš©\n",
    "                    scaled_preds = temp_scaling(preds)\n",
    "                    probs = torch.softmax(scaled_preds, dim=1)\n",
    "                    \n",
    "                    ensemble_probs += probs / (len(models) * len(images_list))\n",
    "        \n",
    "        final_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "        all_predictions.extend(final_preds.cpu().numpy())\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‹¤í—˜ìš© ë‹¨ì¼ ì¶”ë¡  í•¨ìˆ˜ ì¶”ê°€ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤í—˜ìš©: ë‹¨ì¼ ëª¨ë¸ TTA ì¶”ë¡  (5ë°° ë¹ ë¦„)\n",
    "def single_model_tta_inference(model, loader, transforms):\n",
    "    \"\"\"ë‹¨ì¼ ëª¨ë¸ TTA ì¶”ë¡  - ì‹¤í—˜ìš©\"\"\"\n",
    "    temp_scaling = TemperatureScaling().to(device)\n",
    "    all_predictions = []\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Single TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        tta_probs = torch.zeros(batch_size, 17).to(device)\n",
    "        \n",
    "        # ë‹¨ì¼ ëª¨ë¸ë¡œ TTA\n",
    "        with torch.no_grad():\n",
    "            for images in images_list:\n",
    "                images = images.to(device)\n",
    "                preds = model(images)\n",
    "                \n",
    "                # Temperature Scaling ì ìš©\n",
    "                scaled_preds = temp_scaling(preds)\n",
    "                probs = torch.softmax(scaled_preds, dim=1)\n",
    "                \n",
    "                tta_probs += probs / len(images_list)\n",
    "        \n",
    "        final_preds = torch.argmax(tta_probs, dim=1)\n",
    "        all_predictions.extend(final_preds.cpu().numpy())\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "print(\"ì‹¤í—˜ìš© ë‹¨ì¼ ì¶”ë¡  í•¨ìˆ˜ ì¶”ê°€ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ í†µê³„ ìˆ˜ì§‘ìš© ê¸€ë¡œë²Œ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "preprocessing_stats = {\n",
    "    'total_images': 0,\n",
    "    'preprocessing_applied': 0,\n",
    "    'preprocessing_skipped': 0,\n",
    "    'quality_improvements': [],\n",
    "    'quality_degradations': []\n",
    "}\n",
    "\n",
    "def collect_preprocessing_stats(original_confidence, processed_confidence, applied):\n",
    "    \"\"\"ì „ì²˜ë¦¬ í†µê³„ ìˆ˜ì§‘\"\"\"\n",
    "    global preprocessing_stats\n",
    "    preprocessing_stats['total_images'] += 1\n",
    "    \n",
    "    # ë””ë²„ê¹…ìš© ì¶œë ¥ (ì²« 5ê°œë§Œ)\n",
    "    if preprocessing_stats['total_images'] <= 5:\n",
    "        print(f\"Debug: ì´ë¯¸ì§€ {preprocessing_stats['total_images']} ì²˜ë¦¬ë¨, ì ìš©: {applied}\")\n",
    "    \n",
    "    if applied:\n",
    "        preprocessing_stats['preprocessing_applied'] += 1\n",
    "        quality_change = processed_confidence - original_confidence\n",
    "        if quality_change > 0:\n",
    "            preprocessing_stats['quality_improvements'].append(quality_change)\n",
    "        else:\n",
    "            preprocessing_stats['quality_degradations'].append(quality_change)\n",
    "    else:\n",
    "        preprocessing_stats['preprocessing_skipped'] += 1\n",
    "\n",
    "def print_preprocessing_stats():\n",
    "    \"\"\"ì „ì²˜ë¦¬ í†µê³„ ì¶œë ¥\"\"\"\n",
    "    stats = preprocessing_stats\n",
    "    total = stats['total_images']\n",
    "    applied = stats['preprocessing_applied']\n",
    "    skipped = stats['preprocessing_skipped']\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ì „ì²˜ë¦¬ í†µê³„ ë¶„ì„\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"ì´ ì´ë¯¸ì§€ ìˆ˜: {total}ê°œ\")\n",
    "    \n",
    "    # ZeroDivisionError ë°©ì§€\n",
    "    if total > 0:\n",
    "        print(f\"ì „ì²˜ë¦¬ ì ìš©: {applied}ê°œ ({applied/total*100:.1f}%)\")\n",
    "        print(f\"ì „ì²˜ë¦¬ ìŠ¤í‚µ: {skipped}ê°œ ({skipped/total*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"ì „ì²˜ë¦¬ ì ìš©: 0ê°œ (0.0%)\")\n",
    "        print(\"ì „ì²˜ë¦¬ ìŠ¤í‚µ: 0ê°œ (0.0%)\")\n",
    "        print(\"âš ï¸ í†µê³„ ìˆ˜ì§‘ì´ ì‘ë™í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    if stats['quality_improvements']:\n",
    "        avg_improvement = np.mean(stats['quality_improvements'])\n",
    "        print(f\"í’ˆì§ˆ ê°œì„  í‰ê· : +{avg_improvement:.3f}\")\n",
    "        print(f\"í’ˆì§ˆ ê°œì„  ê±´ìˆ˜: {len(stats['quality_improvements'])}ê°œ\")\n",
    "    \n",
    "    if stats['quality_degradations']:\n",
    "        avg_degradation = np.mean(stats['quality_degradations'])\n",
    "        print(f\"í’ˆì§ˆ ì•…í™” í‰ê· : {avg_degradation:.3f}\")\n",
    "        print(f\"í’ˆì§ˆ ì•…í™” ê±´ìˆ˜: {len(stats['quality_degradations'])}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT ê¸°ë°˜ F1 ë¹„êµ í•¨ìˆ˜ ìˆ˜ì • ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# GT ê¸°ë°˜ ì „ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦ í•¨ìˆ˜ - ìˆ˜ì • ë²„ì „\n",
    "class CleanTTAImageDataset(Dataset):\n",
    "    \"\"\"ì „ì²˜ë¦¬ ì—†ëŠ” ìˆœìˆ˜í•œ TTA Dataset\"\"\"\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        \n",
    "        # ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ì´ë¯¸ì§€ë§Œ ë¡œë“œ\n",
    "        img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        # ëª¨ë“  transformì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            aug_img = transform(image=img)['image']\n",
    "            augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target\n",
    "\n",
    "def evaluate_preprocessing_effect():\n",
    "    \"\"\"selected_images_500.csv GTë¡œ ì „ì²˜ë¦¬ ì „í›„ F1 ìŠ¤ì½”ì–´ ë¹„êµ - ìˆ˜ì • ë²„ì „\"\"\"\n",
    "    \n",
    "    global preprocessing_stats\n",
    "    preprocessing_stats = {\n",
    "        'total_images': 0,\n",
    "        'preprocessing_applied': 0,\n",
    "        'preprocessing_skipped': 0,\n",
    "        'quality_improvements': [],\n",
    "        'quality_degradations': []\n",
    "    }\n",
    "    \n",
    "    # GT íŒŒì¼ ë¡œë“œ\n",
    "    gt_file = \"../data/selected_images_500.csv\"\n",
    "    try:\n",
    "        gt_df = pd.read_csv(gt_file)\n",
    "        gt_df = gt_df.rename(columns={'predicted_class': 'true_label'})\n",
    "        print(f\"GT ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(gt_df)}ê°œ ì´ë¯¸ì§€\")\n",
    "    except:\n",
    "        print(\"selected_images_500.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # ë¼ë²¨ë§ëœ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "    if 'true_label' in gt_df.columns:\n",
    "        labeled_df = gt_df.dropna(subset=['true_label'])\n",
    "        print(f\"ë¼ë²¨ë§ëœ ë°ì´í„°: {len(labeled_df)}ê°œ\")\n",
    "    else:\n",
    "        print(\"true_label ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. GT íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "    \n",
    "    gt_files = labeled_df['filename'].tolist()\n",
    "    gt_labels = labeled_df['true_label'].astype(int).tolist()\n",
    "    \n",
    "    # 1) ì „ì²˜ë¦¬ OFF ì¶”ë¡  (CleanTTAImageDataset ì‚¬ìš©)\n",
    "    print(\"\\n1ë‹¨ê³„: ì „ì²˜ë¦¬ ì—†ëŠ” ì¶”ë¡  (CleanTTAImageDataset)...\")\n",
    "    clean_dataset = CleanTTAImageDataset(\n",
    "        labeled_df[['filename', 'true_label']].rename(columns={'true_label': 'target'}),\n",
    "        \"../data/test/\",\n",
    "        essential_tta_transforms\n",
    "    )\n",
    "    \n",
    "    # clean_loader = DataLoader(clean_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    clean_loader = DataLoader(clean_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    single_model = ensemble_models[0]\n",
    "    clean_preds = single_model_tta_inference(single_model, clean_loader, essential_tta_transforms)\n",
    "    \n",
    "    # 2) ì „ì²˜ë¦¬ ON ì¶”ë¡  (ê¸°ì¡´ TTAImageDataset ì‚¬ìš©)  \n",
    "    print(\"2ë‹¨ê³„: ì „ì²˜ë¦¬ ì ìš©ëœ ì¶”ë¡  (TTAImageDataset)...\")\n",
    "    preprocessed_dataset = TTAImageDataset(\n",
    "        labeled_df[['filename', 'true_label']].rename(columns={'true_label': 'target'}),\n",
    "        \"../data/test/\",\n",
    "        essential_tta_transforms\n",
    "    )\n",
    "    \n",
    "    # preprocessed_loader = DataLoader(preprocessed_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    preprocessed_loader = DataLoader(preprocessed_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    preprocessed_preds = single_model_tta_inference(single_model, preprocessed_loader, essential_tta_transforms)\n",
    "    \n",
    "    # 3) F1 ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "    from sklearn.metrics import f1_score, classification_report\n",
    "    \n",
    "    f1_clean = f1_score(gt_labels, clean_preds, average='macro')\n",
    "    f1_preprocessed = f1_score(gt_labels, preprocessed_preds, average='macro')\n",
    "    f1_improvement = f1_preprocessed - f1_clean\n",
    "    \n",
    "    # 4) ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ì „ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦ ê²°ê³¼ (ìˆ˜ì • ë²„ì „)\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"GT ë°ì´í„° ìˆ˜: {len(gt_labels)}ê°œ\")\n",
    "    print(f\"ì „ì²˜ë¦¬ ì „ F1 (Clean): {f1_clean:.4f}\")\n",
    "    print(f\"ì „ì²˜ë¦¬ í›„ F1 (Processed): {f1_preprocessed:.4f}\")\n",
    "    print(f\"F1 ê°œì„ : {f1_improvement:+.4f} ({f1_improvement/f1_clean*100:+.1f}%)\")\n",
    "    \n",
    "    if f1_improvement > 0.01:\n",
    "        print(\"âœ… ì „ì²˜ë¦¬ íš¨ê³¼ í™•ì¸! ì œì¶œìš© ì•™ìƒë¸” ëª¨ë“œ ì‹¤í–‰ ê¶Œì¥\")\n",
    "    elif f1_improvement > 0.005:\n",
    "        print(\"ğŸ“Š ë¯¸ë¯¸í•œ ê°œì„ . ì¶”ê°€ ê²€í†  í•„ìš”\")\n",
    "    else:\n",
    "        print(\"âŒ ì „ì²˜ë¦¬ íš¨ê³¼ ë¯¸ë¯¸. ì„¤ì • ì¬ê²€í†  í•„ìš”\")\n",
    "    \n",
    "    # 5) ìƒì„¸ ë¶„ì„\n",
    "    unique_labels = sorted(set(gt_labels) | set(clean_preds) | set(preprocessed_preds))\n",
    "    target_names = [f\"Class_{i}\" for i in unique_labels]\n",
    "\n",
    "    print(\"\\nì „ì²˜ë¦¬ ì „ (Clean Dataset):\")\n",
    "    print(classification_report(gt_labels, clean_preds, labels=unique_labels, target_names=target_names, digits=3))\n",
    "    print(\"\\nì „ì²˜ë¦¬ í›„ (Preprocessed Dataset):\")\n",
    "    print(classification_report(gt_labels, preprocessed_preds, labels=unique_labels, target_names=target_names, digits=3))\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ í†µê³„ ì¶œë ¥\n",
    "    print_preprocessing_stats()\n",
    "    \n",
    "    return {\n",
    "        'f1_clean': f1_clean,\n",
    "        'f1_preprocessed': f1_preprocessed, \n",
    "        'improvement': f1_improvement,\n",
    "        'gt_count': len(gt_labels)\n",
    "    }\n",
    "\n",
    "print(\"GT ê¸°ë°˜ F1 ë¹„êµ í•¨ìˆ˜ ìˆ˜ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# ì‹¤í—˜ ëª¨ë“œ ì„¤ì • (ì‹¤í—˜ìš© vs ì œì¶œìš©)\n",
    "# ===========================================\n",
    "EXPERIMENT_MODE = True  # True: ë‹¨ì¼ëª¨ë¸, False: 5-fold ì•™ìƒë¸”\n",
    "\n",
    "if EXPERIMENT_MODE:\n",
    "    print(\"ğŸ§ª ì‹¤í—˜ ëª¨ë“œ: ë‹¨ì¼ ëª¨ë¸ TTA ì¶”ë¡  (ë¹ ë¦„)\")\n",
    "    # ì²« ë²ˆì§¸ ëª¨ë¸ë§Œ ì‚¬ìš©\n",
    "    single_model = ensemble_models[0]\n",
    "    tta_predictions = single_model_tta_inference(\n",
    "        model=single_model,\n",
    "        loader=tta_loader, \n",
    "        transforms=essential_tta_transforms\n",
    "    )\n",
    "else:\n",
    "    print(\"ğŸš€ ì œì¶œ ëª¨ë“œ: 5-Fold ì•™ìƒë¸” TTA ì¶”ë¡  (ì •í™•í•¨)\")\n",
    "    tta_predictions = ensemble_tta_inference(\n",
    "        models=ensemble_models, \n",
    "        loader=tta_loader, \n",
    "        transforms=essential_tta_transforms,\n",
    "        confidence_threshold=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT ë°ì´í„° ë¡œë“œ ì™„ë£Œ: 500ê°œ ì´ë¯¸ì§€\n",
      "ë¼ë²¨ë§ëœ ë°ì´í„°: 500ê°œ\n",
      "\n",
      "1ë‹¨ê³„: ì „ì²˜ë¦¬ ì—†ëŠ” ì¶”ë¡  (CleanTTAImageDataset)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Single TTA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:15<00:00,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2ë‹¨ê³„: ì „ì²˜ë¦¬ ì ìš©ëœ ì¶”ë¡  (TTAImageDataset)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Single TTA:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: ì´ë¯¸ì§€ 1 ì²˜ë¦¬ë¨, ì ìš©: True\n",
      "Debug: ì´ë¯¸ì§€ 2 ì²˜ë¦¬ë¨, ì ìš©: True\n",
      "Debug: ì´ë¯¸ì§€ 3 ì²˜ë¦¬ë¨, ì ìš©: False\n",
      "Debug: ì´ë¯¸ì§€ 4 ì²˜ë¦¬ë¨, ì ìš©: True\n",
      "Debug: ì´ë¯¸ì§€ 5 ì²˜ë¦¬ë¨, ì ìš©: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Single TTA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:55<00:00,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ì „ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦ ê²°ê³¼ (ìˆ˜ì • ë²„ì „)\n",
      "==================================================\n",
      "GT ë°ì´í„° ìˆ˜: 500ê°œ\n",
      "ì „ì²˜ë¦¬ ì „ F1 (Clean): 0.6796\n",
      "ì „ì²˜ë¦¬ í›„ F1 (Processed): 0.5458\n",
      "F1 ê°œì„ : -0.1338 (-19.7%)\n",
      "âŒ ì „ì²˜ë¦¬ íš¨ê³¼ ë¯¸ë¯¸. ì„¤ì • ì¬ê²€í†  í•„ìš”\n",
      "\n",
      "ì „ì²˜ë¦¬ ì „ (Clean Dataset):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class_3      0.686     0.901     0.779       121\n",
      "     Class_4      0.985     0.832     0.902       161\n",
      "     Class_7      0.838     0.763     0.799       169\n",
      "    Class_10      0.000     0.000     0.000         0\n",
      "    Class_13      0.000     0.000     0.000         0\n",
      "    Class_14      0.918     0.918     0.918        49\n",
      "\n",
      "   micro avg      0.834     0.834     0.834       500\n",
      "   macro avg      0.571     0.569     0.566       500\n",
      "weighted avg      0.856     0.834     0.839       500\n",
      "\n",
      "\n",
      "ì „ì²˜ë¦¬ í›„ (Preprocessed Dataset):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class_3      0.671     0.893     0.766       121\n",
      "     Class_4      0.925     0.845     0.883       161\n",
      "     Class_7      0.845     0.710     0.772       169\n",
      "    Class_10      0.000     0.000     0.000         0\n",
      "    Class_13      0.000     0.000     0.000         0\n",
      "    Class_14      0.872     0.837     0.854        49\n",
      "\n",
      "    accuracy                          0.810       500\n",
      "   macro avg      0.552     0.547     0.546       500\n",
      "weighted avg      0.831     0.810     0.814       500\n",
      "\n",
      "\n",
      "==================================================\n",
      "ì „ì²˜ë¦¬ í†µê³„ ë¶„ì„\n",
      "==================================================\n",
      "ì´ ì´ë¯¸ì§€ ìˆ˜: 500ê°œ\n",
      "ì „ì²˜ë¦¬ ì ìš©: 259ê°œ (51.8%)\n",
      "ì „ì²˜ë¦¬ ìŠ¤í‚µ: 241ê°œ (48.2%)\n",
      "í’ˆì§ˆ ê°œì„  í‰ê· : +0.074\n",
      "í’ˆì§ˆ ê°œì„  ê±´ìˆ˜: 114ê°œ\n",
      "í’ˆì§ˆ ì•…í™” í‰ê· : -0.028\n",
      "í’ˆì§ˆ ì•…í™” ê±´ìˆ˜: 145ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_preprocessing_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA ê²°ê³¼ë¡œ submission íŒŒì¼ ìƒì„±\n",
    "tta_pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n",
    "tta_pred_df['target'] = tta_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ submissionê³¼ ë™ì¼í•œ ìˆœì„œì¸ì§€ í™•ì¸\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == tta_pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA predictions saved\n",
      "TTA Prediction sample:\n"
     ]
    }
   ],
   "source": [
    "# TTA ê²°ê³¼ ì €ì¥\n",
    "tta_pred_df.to_csv(\"../submission/choice.csv\", index=False)\n",
    "print(\"TTA predictions saved\")\n",
    "\n",
    "print(\"TTA Prediction sample:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1700315247734,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "9yMO8s6GqAwZ",
    "outputId": "9a30616f-f0ea-439f-a906-dd806737ce00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008fdb22ddce0ce.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00091bffdffd83de.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00396fbc1f6cc21d.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00471f8038d9c4b6.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00901f504008d884.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target\n",
       "0  0008fdb22ddce0ce.jpg       2\n",
       "1  00091bffdffd83de.jpg      12\n",
       "2  00396fbc1f6cc21d.jpg       5\n",
       "3  00471f8038d9c4b6.jpg      12\n",
       "4  00901f504008d884.jpg       2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tta_pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
