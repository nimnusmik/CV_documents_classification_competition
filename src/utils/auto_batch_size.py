#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒ€ í˜‘ì—…ìš© GPU ìµœì í™” ìë™ ë°°ì¹˜ í¬ê¸° ì°¾ê¸° ë„êµ¬
ë‹¤ì–‘í•œ GPU í™˜ê²½ì—ì„œ ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì£¼ëŠ” ë„êµ¬

Author: AI Team
Date: 2025-01-05
"""

import os
import sys
import torch
import gc
import argparse
from pathlib import Path
from typing import Tuple, Optional, Dict, Any

# YAML import with fallback
try:
    import yaml
except ImportError:
    print("âŒ PyYAMLì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("   pip install PyYAML")
    sys.exit(1)


def get_gpu_info_and_recommendations() -> Dict[str, Any]:
    """GPU ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ê¶Œì¥ ì„¤ì •ì„ ë°˜í™˜"""
    if not torch.cuda.is_available():
        return {
            'name': 'CPU',
            'total_memory': 0,
            'tier': 'cpu',
            'profile': {
                'batch_224': {'start': 4, 'max': 8, 'safety': 0.9},
                'batch_384': {'start': 2, 'max': 4, 'safety': 0.9},
                'batch_512': {'start': 1, 'max': 2, 'safety': 0.9}
            }
        }
    
    device_name = torch.cuda.get_device_name()
    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
    
    # GPU ë“±ê¸‰ë³„ ë¶„ë¥˜
    if any(gpu in device_name for gpu in ['RTX 4090', 'RTX 4080', 'RTX 3090', 'A100', 'V100']):
        tier = 'high_end'
        profile = {
            'batch_224': {'start': 64, 'max': 128, 'safety': 0.8},
            'batch_384': {'start': 32, 'max': 64, 'safety': 0.8},
            'batch_512': {'start': 16, 'max': 32, 'safety': 0.8}
        }
    elif any(gpu in device_name for gpu in ['RTX 3080', 'RTX 3070', 'RTX 4070']):
        tier = 'mid_range'
        profile = {
            'batch_224': {'start': 32, 'max': 64, 'safety': 0.8},
            'batch_384': {'start': 16, 'max': 32, 'safety': 0.8},
            'batch_512': {'start': 8, 'max': 16, 'safety': 0.8}
        }
    elif any(gpu in device_name for gpu in ['RTX 3060', 'RTX 2070', 'RTX 2080']):
        tier = 'budget'
        profile = {
            'batch_224': {'start': 16, 'max': 32, 'safety': 0.85},
            'batch_384': {'start': 8, 'max': 16, 'safety': 0.85},
            'batch_512': {'start': 4, 'max': 8, 'safety': 0.85}
        }
    else:  # GTX 1660, GTX 1080 ë“± êµ¬í˜• GPU
        tier = 'low_end'
        profile = {
            'batch_224': {'start': 8, 'max': 16, 'safety': 0.9},
            'batch_384': {'start': 4, 'max': 8, 'safety': 0.9},
            'batch_512': {'start': 2, 'max': 4, 'safety': 0.9}
        }
    
    return {
        'name': device_name,
        'total_memory': total_memory,
        'tier': tier,
        'profile': profile
    }


def test_batch_size(model_name: str, img_size: int, batch_size: int) -> Tuple[bool, Optional[float]]:
    """íŠ¹ì • ë°°ì¹˜ í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"""
    try:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        gc.collect()
        
        device = torch.device('cuda')
        
        # ê°„ë‹¨í•œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ëª¨ë¸ í¬ê¸°ì™€ ìœ ì‚¬í•˜ê²Œ)
        if 'swin' in model_name.lower():
            # Swin Transformer ê·¼ì‚¬ ëª¨ë¸
            model = torch.nn.Sequential(
                torch.nn.Conv2d(3, 128, 3, padding=1),
                torch.nn.BatchNorm2d(128),
                torch.nn.ReLU(),
                torch.nn.AdaptiveAvgPool2d((7, 7)),
                torch.nn.Flatten(),
                torch.nn.Linear(128 * 7 * 7, 1000),
                torch.nn.Linear(1000, 100)
            ).to(device)
        elif 'convnext' in model_name.lower():
            # ConvNext ê·¼ì‚¬ ëª¨ë¸
            model = torch.nn.Sequential(
                torch.nn.Conv2d(3, 96, 4, stride=4),
                torch.nn.LayerNorm([96, img_size//4, img_size//4]),
                torch.nn.Conv2d(96, 192, 1),
                torch.nn.AdaptiveAvgPool2d((1, 1)),
                torch.nn.Flatten(),
                torch.nn.Linear(192, 100)
            ).to(device)
        else:
            # ê¸°ë³¸ ResNet ìŠ¤íƒ€ì¼ ëª¨ë¸
            model = torch.nn.Sequential(
                torch.nn.Conv2d(3, 64, 7, stride=2, padding=3),
                torch.nn.BatchNorm2d(64),
                torch.nn.ReLU(),
                torch.nn.MaxPool2d(3, stride=2, padding=1),
                torch.nn.AdaptiveAvgPool2d((1, 1)),
                torch.nn.Flatten(),
                torch.nn.Linear(64, 100)
            ).to(device)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_input = torch.randn(batch_size, 3, img_size, img_size, device=device)
        test_target = torch.randint(0, 100, (batch_size,), device=device)
        
        # Forward pass
        output = model(test_input)
        loss = torch.nn.functional.cross_entropy(output, test_target)
        
        # Backward pass
        loss.backward()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        memory_used = torch.cuda.memory_allocated() / (1024**3)  # GB
        
        # ì •ë¦¬
        del model, test_input, test_target, output, loss
        torch.cuda.empty_cache()
        gc.collect()
        
        return True, memory_used
        
    except RuntimeError as e:
        if "out of memory" in str(e):
            torch.cuda.empty_cache()
            gc.collect()
            return False, None
        else:
            raise e
    except Exception as e:
        torch.cuda.empty_cache()
        gc.collect()
        return False, None


def find_optimal_batch_size(model_name: str, img_size: int, gpu_info: Dict[str, Any]) -> int:
    """ìµœì ì˜ ë°°ì¹˜ í¬ê¸° ì°¾ê¸° (GPU ë“±ê¸‰ë³„ ìµœì í™”)"""
    print(f"ğŸ” {gpu_info['tier']} GPU ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ ì¤‘...")
    print(f"   GPU: {gpu_info['name']}")
    print(f"   ë©”ëª¨ë¦¬: {gpu_info['total_memory']:.1f} GB")
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {img_size}")
    
    # ì´ë¯¸ì§€ í¬ê¸°ë³„ í”„ë¡œí•„ ì„ íƒ
    if img_size <= 224:
        batch_config = gpu_info['profile']['batch_224']
    elif img_size <= 384:
        batch_config = gpu_info['profile']['batch_384']
    else:
        batch_config = gpu_info['profile']['batch_512']
    
    start_batch = batch_config['start']
    max_batch = batch_config['max']
    safety_factor = batch_config['safety']
    
    print(f"   ğŸ“Š {gpu_info['tier']} GPU ê¶Œì¥ ë²”ìœ„: {start_batch} ~ {max_batch}")
    print(f"   ğŸ›¡ï¸ ì•ˆì „ ë§ˆì§„: {int((1-safety_factor)*100)}%")
    
    optimal_batch = start_batch
    
    # ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
    low, high = start_batch, max_batch
    
    while low <= high:
        mid = (low + high) // 2
        
        print(f"   ë°°ì¹˜ í¬ê¸° {mid} í…ŒìŠ¤íŠ¸ ì¤‘...", end=" ")
        
        success, memory_used = test_batch_size(model_name, img_size, mid)
        
        if success:
            optimal_batch = mid
            if memory_used:
                print(f"âœ… (ë©”ëª¨ë¦¬: {memory_used:.2f} GB)")
            else:
                print("âœ…")
            low = mid + 1  # ë” í° ë°°ì¹˜ ì‹œë„
        else:
            print("âŒ (ë©”ëª¨ë¦¬ ë¶€ì¡±)")
            high = mid - 1  # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì‹œë„
    
    # ì•ˆì „ ë§ˆì§„ ì ìš©
    final_batch = max(4, int(optimal_batch * safety_factor))
    
    # 4ì˜ ë°°ìˆ˜ë¡œ ì¡°ì • (ëª¨ë“  GPUì—ì„œ íš¨ìœ¨ì )
    final_batch = (final_batch // 4) * 4
    final_batch = max(4, final_batch)  # ìµœì†Œ 4
    
    print(f"\nğŸ¯ {gpu_info['tier']} GPU ìµœì  ë°°ì¹˜ í¬ê¸°: {final_batch}")
    
    # GPUë³„ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
    recommendations = []
    if gpu_info['total_memory'] < 8:
        recommendations.append("ğŸ’¡ ë‚®ì€ GPU ë©”ëª¨ë¦¬: gradient_accumulation_steps ì‚¬ìš© ê¶Œì¥")
    if "GTX" in gpu_info['name']:
        recommendations.append("ğŸ’¡ êµ¬í˜• GPU: mixed precision (AMP) ë¹„í™œì„±í™” ê¶Œì¥")
    if gpu_info['total_memory'] >= 20:
        recommendations.append("ğŸ’¡ ê³ ì„±ëŠ¥ GPU: ë” í° ëª¨ë¸ì´ë‚˜ ë” ë†’ì€ í•´ìƒë„ ê³ ë ¤ ê°€ëŠ¥")
    
    for rec in recommendations:
        print(f"   {rec}")
    
    return final_batch


def update_config_file(config_path: str, batch_size: int):
    """ì„¤ì • íŒŒì¼ì˜ ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        if 'training' not in config:
            config['training'] = {}
        
        config['training']['batch_size'] = batch_size
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
        
        print(f"âœ… ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: batch_size = {batch_size}")
        
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='íŒ€ í˜‘ì—…ìš© GPU ìµœì í™” ìë™ ë°°ì¹˜ í¬ê¸° ì°¾ê¸°')
    parser.add_argument('--config', type=str, default='configs/train.yaml',
                        help='YAML ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--model', type=str, help='ëª¨ë¸ ì´ë¦„ (ì˜µì…˜)')
    parser.add_argument('--img-size', type=int, help='ì´ë¯¸ì§€ í¬ê¸° (ì˜µì…˜)')
    parser.add_argument('--test-only', action='store_true',
                        help='í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ê³  ì„¤ì • íŒŒì¼ì„ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ')
    
    args = parser.parse_args()
    
    print("ğŸš€ íŒ€ í˜‘ì—…ìš© GPU ìµœì í™” ìë™ ë°°ì¹˜ í¬ê¸° ì°¾ê¸° ë„êµ¬")
    print("=" * 55)
    
    if not os.path.exists(args.config):
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        sys.exit(1)
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        sys.exit(1)
    
    # GPU ì •ë³´ ë° ê¶Œì¥ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°
    gpu_info = get_gpu_info_and_recommendations()
    
    print(f"ğŸ”§ GPU: {gpu_info['name']}")
    print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_info['total_memory']:.1f} GB")
    print(f"ğŸ† GPU ë“±ê¸‰: {gpu_info['tier']}")
    
    batch_range = gpu_info['profile']['batch_224']
    print(f"ğŸ’¡ ê¶Œì¥ ë°°ì¹˜ ë²”ìœ„: {batch_range['start']} ~ {batch_range['max']}")
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # ëª¨ë¸ ë° ì´ë¯¸ì§€ í¬ê¸° ì¶”ì¶œ
    model_name = args.model or config.get('model', {}).get('name', 'swin_base_patch4_window7_224')
    
    # ì´ë¯¸ì§€ í¬ê¸° ì°¾ê¸° (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
    img_size = args.img_size
    if not img_size:
        img_size = (config.get('model', {}).get('img_size') or 
                   config.get('train', {}).get('img_size') or 
                   config.get('training', {}).get('img_size') or 
                   config.get('data', {}).get('img_size') or
                   384)
    
    print(f"ğŸ“Š ëª¨ë¸: {model_name}")
    print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {img_size}")
    
    # ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
    optimal_batch = find_optimal_batch_size(model_name, img_size, gpu_info)
    
    print("\n" + "=" * 55)
    print(f"ğŸ‰ ìµœì¢… ê²°ê³¼:")
    print(f"   ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch}")
    print(f"   GPU ë“±ê¸‰: {gpu_info['tier']}")
    print(f"   ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : ~{(optimal_batch/batch_range['max'])*100:.0f}%")
    
    if not args.test_only:
        # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        update_config_file(args.config, optimal_batch)
        
        print(f"\nâœ… ì™„ë£Œ! ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ìµœì í™”ëœ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”:")
        print(f"   python src/training/train_main.py --mode highperf")
        
        # GPUë³„ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ {gpu_info['tier']} GPU ì¶”ê°€ ê¶Œì¥ì‚¬í•­:")
        if gpu_info['total_memory'] < 8:
            print(f"   - gradient_accumulation_steps = 2-4 ì‚¬ìš© ê¶Œì¥ (ë‚®ì€ ë©”ëª¨ë¦¬)")
            print(f"   - mixed precision ë¹„í™œì„±í™” ê³ ë ¤")
        elif gpu_info['total_memory'] >= 20:
            print(f"   - ë” í° ëª¨ë¸ì´ë‚˜ ensemble ê³ ë ¤ ê°€ëŠ¥")
            print(f"   - Multi-GPU training ê°€ëŠ¥")
        
        print(f"   - ì‹¤ì œ í›ˆë ¨ ì‹œì‘ ì „ì— ì‘ì€ epochë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”")
        print(f"   - ëª¨ë‹ˆí„°ë§: nvidia-smi -l 1 ëª…ë ¹ì–´ë¡œ GPU ì‚¬ìš©ëŸ‰ í™•ì¸")
        print(f"   - íŒ€ì›ê³¼ ë°°ì¹˜ í¬ê¸° ì„¤ì • ê³µìœ í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€")
        
    else:
        print(f"\nğŸ’¡ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì„¤ì • íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(f"   ìˆ˜ë™ìœ¼ë¡œ batch_sizeë¥¼ {optimal_batch}ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
    
    print(f"\nâœ¨ {gpu_info['tier']} GPU ìµœì í™” ì™„ë£Œ!")
    print(f"ğŸ¤ ë‹¤ë¥¸ íŒ€ì›ë“¤ê³¼ ì„¤ì •ì„ ê³µìœ í•˜ì—¬ í˜‘ì—…í•˜ì„¸ìš”!")


if __name__ == "__main__":
    main()
