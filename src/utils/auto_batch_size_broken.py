#!/usr/bin/env python3
"""
GPU ìµœì í™” ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì • ìœ í‹¸ë¦¬í‹°

GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ì„¤ì • íŒŒì¼ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
íŒ€ í˜‘ì—… í™˜ê²½ì—ì„œ ë‹¤ì–‘í•œ GPU ëª¨ë¸ì— ëŒ€í•œ ìµœì í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os                                                               # ìš´ì˜ì²´ì œ íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼
import sys                                                              # ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥
import torch                                                            # PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
import argparse                                                         # CLI ì¸ì íŒŒì‹±
from pathlib import Path                                                # ê²½ë¡œ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from typing import Tuple, Optional, Dict, Any                           # íƒ€ì… íŒíŠ¸

# YAML ëª¨ë“ˆ ì„í¬íŠ¸ (ì˜ˆì™¸ ì²˜ë¦¬)
try:                                                                    # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
    import yaml                                                         # YAML íŒŒì¼ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
except ImportError:                                                     # ì„í¬íŠ¸ ì‹¤íŒ¨ ì‹œ
    print("âŒ PyYAMLì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")        # ì„¤ì¹˜ ì•ˆë‚´ ë©”ì‹œì§€
    print("   pip install PyYAML")                                      # ì„¤ì¹˜ ëª…ë ¹ì–´ ì¶œë ¥
    sys.exit(1)                                                         # í”„ë¡œê·¸ë¨ ì¢…ë£Œ


# ==================== GPU ì •ë³´ ë° ê¶Œì¥ì‚¬í•­ í•¨ìˆ˜ ==================== #
# GPU ì •ë³´ ë° ê¶Œì¥ ì„¤ì • ë°˜í™˜ í•¨ìˆ˜ ì •ì˜
def get_gpu_info_and_recommendations() -> Optional[Dict[str, Any]]:
    """GPU ì •ë³´ ë° ê¶Œì¥ ì„¤ì • ë°˜í™˜"""
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not torch.cuda.is_available():                                   # CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
        return None                                                     # None ë°˜í™˜
    
    # GPU ì •ë³´ ìˆ˜ì§‘
    device = torch.cuda.current_device()                                # í˜„ì¬ GPU ë””ë°”ì´ìŠ¤ ë²ˆí˜¸
    props = torch.cuda.get_device_properties(device)                    # GPU ì†ì„± ì •ë³´
    device_name = props.name                                            # GPU ëª¨ë¸ëª…
    total_memory = props.total_memory / (1024**3)                       # ì´ ë©”ëª¨ë¦¬ (GB ë‹¨ìœ„)
    allocated_memory = torch.cuda.memory_allocated(device) / (1024**3)  # í• ë‹¹ëœ ë©”ëª¨ë¦¬ (GB)
    free_memory = total_memory - allocated_memory                       # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB)
    
    # GPUë³„ ê¶Œì¥ ì„¤ì • (ë©”ëª¨ë¦¬ ê¸°ë°˜)
    gpu_profiles = {                                                    # GPU í”„ë¡œí•„ ë”•ì…”ë„ˆë¦¬
        # ê³ ì„±ëŠ¥ GPU (20GB+)
        "high_end": {                                                   # ê³ ì„±ëŠ¥ GPU ì„¤ì •
            "memory_threshold": 20.0,                                   # ë©”ëª¨ë¦¬ ì„ê³„ê°’ (GB)
            "batch_224": {"start": 64, "max": 128, "safety": 0.85},     # 224px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "batch_384": {"start": 32, "max": 64, "safety": 0.80},      # 384px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "batch_512": {"start": 16, "max": 32, "safety": 0.75},      # 512px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "examples": ["RTX 4090", "RTX 3090", "A100", "V100"]        # í•´ë‹¹ GPU ì˜ˆì‹œ
        },
        # ì¤‘ê¸‰ GPU (10-20GB)
        "mid_range": {                                                  # ì¤‘ê¸‰ GPU ì„¤ì •
            "memory_threshold": 10.0,                                   # ë©”ëª¨ë¦¬ ì„ê³„ê°’ (GB)
            "batch_224": {"start": 32, "max": 64, "safety": 0.80},      # 224px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "batch_384": {"start": 16, "max": 32, "safety": 0.75},      # 384px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "batch_512": {"start": 8, "max": 16, "safety": 0.70},       # 512px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "examples": ["RTX 3080", "RTX 3070", "RTX 2080 Ti"]         # í•´ë‹¹ GPU ì˜ˆì‹œ
        },
        # ë³´ê¸‰í˜• GPU (6-10GB)
        "budget": {                                                     # ë³´ê¸‰í˜• GPU ì„¤ì •
            "memory_threshold": 6.0,                                    # ë©”ëª¨ë¦¬ ì„ê³„ê°’ (GB)
            "batch_224": {"start": 16, "max": 32, "safety": 0.75},      # 224px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "batch_384": {"start": 8, "max": 16, "safety": 0.70},       # 384px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "batch_512": {"start": 4, "max": 8, "safety": 0.65},        # 512px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "examples": ["RTX 3060", "RTX 2070", "GTX 1080 Ti"]         # í•´ë‹¹ GPU ì˜ˆì‹œ
        },
        # ì €ì‚¬ì–‘ GPU (<6GB)
        "low_end": {                                                    # ì €ì‚¬ì–‘ GPU ì„¤ì •
            "memory_threshold": 0.0,                                    # ë©”ëª¨ë¦¬ ì„ê³„ê°’ (GB)
            "batch_224": {"start": 8, "max": 16, "safety": 0.70},       # 224px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "batch_384": {"start": 4, "max": 8, "safety": 0.65},        # 384px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "batch_512": {"start": 2, "max": 4, "safety": 0.60},        # 512px ì´ë¯¸ì§€ ë°°ì¹˜ ì„¤ì •
            "examples": ["GTX 1660", "GTX 1080", "RTX 2060"]            # í•´ë‹¹ GPU ì˜ˆì‹œ
        }
    }
    
    # GPU ë“±ê¸‰ ê²°ì • ë¡œì§
    if total_memory >= 20.0:                    # 20GB ì´ìƒì¸ ê²½ìš°
        profile = gpu_profiles["high_end"]      # ê³ ì„±ëŠ¥ í”„ë¡œí•„ ì„ íƒ
        tier = "ê³ ì„±ëŠ¥"                          # ë“±ê¸‰ ì„¤ì •
    elif total_memory >= 10.0:                  # 10GB ì´ìƒì¸ ê²½ìš°
        profile = gpu_profiles["mid_range"]     # ì¤‘ê¸‰ í”„ë¡œí•„ ì„ íƒ
        tier = "ì¤‘ê¸‰"                            # ë“±ê¸‰ ì„¤ì •
    elif total_memory >= 6.0:                   # 6GB ì´ìƒì¸ ê²½ìš°
        profile = gpu_profiles["budget"]        # ë³´ê¸‰í˜• í”„ë¡œí•„ ì„ íƒ
        tier = "ë³´ê¸‰í˜•"                          # ë“±ê¸‰ ì„¤ì •
    else:                                       # 6GB ë¯¸ë§Œì¸ ê²½ìš°
        profile = gpu_profiles["low_end"]       # ì €ì‚¬ì–‘ í”„ë¡œí•„ ì„ íƒ
        tier = "ì €ì‚¬ì–‘"                          # ë“±ê¸‰ ì„¤ì •

    # GPU ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    return {                                    # GPU ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        "name": device_name,                    # GPU ëª¨ë¸ëª…
        "total_memory": total_memory,           # ì´ ë©”ëª¨ë¦¬ (GB)
        "free_memory": free_memory,             # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB)
        "tier": tier,                           # GPU ë“±ê¸‰
        "profile": profile                      # ì„ íƒëœ í”„ë¡œí•„
    }


# ==================== ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ==================== #
# íŠ¹ì • ë°°ì¹˜ í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì •ì˜
def test_batch_size(model_name: str, img_size: int, batch_size: int, device: str = "cuda") -> Tuple[bool, Optional[float]]:
    """íŠ¹ì • ë°°ì¹˜ í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"""
    # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì˜ˆì™¸ ì²˜ë¦¬
    try:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()                            # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
        
        #------------------------ ê°€ìƒ ëª¨ë¸ ìƒì„± (ì‹¤ì œ ëª¨ë¸ í¬ê¸° ì‹œë®¬ë ˆì´ì…˜) ------------------------#
        # Swin Transformer ëª¨ë¸ì¸ ê²½ìš°
        if "swin" in model_name.lower():
            # Swin TransformerëŠ” ë” ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
            dummy_model = torch.nn.Sequential(              # ê°€ìƒ Swin ëª¨ë¸ ìƒì„±
                torch.nn.Conv2d(3, 128, 7, 2, 3),           # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
                torch.nn.AdaptiveAvgPool2d(1),              # ì ì‘í˜• í‰ê·  í’€ë§
                torch.nn.Flatten(),                         # í‰íƒ„í™” ë ˆì´ì–´
                torch.nn.Linear(128, 1000)                  # ë¶„ë¥˜ í—¤ë“œ
            ).to(device)                                    # GPUë¡œ ëª¨ë¸ ì´ë™
            memory_multiplier = 1.5                         # Swinì€ ë” ë§ì€ ë©”ëª¨ë¦¬ í•„ìš”
            
        # EfficientNet ë“± ë‹¤ë¥¸ ëª¨ë¸
        else:
            dummy_model = torch.nn.Sequential(              # ê°€ìƒ ì¼ë°˜ ëª¨ë¸ ìƒì„±
                torch.nn.Conv2d(3, 64, 3, 1, 1),            # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
                torch.nn.AdaptiveAvgPool2d(1),              # ì ì‘í˜• í‰ê·  í’€ë§
                torch.nn.Flatten(),                         # í‰íƒ„í™” ë ˆì´ì–´
                torch.nn.Linear(64, 1000)                   # ë¶„ë¥˜ í—¤ë“œ
            ).to(device)                                    # GPUë¡œ ëª¨ë¸ ì´ë™
            memory_multiplier = 1.0                         # ê¸°ë³¸ ë©”ëª¨ë¦¬ ê³„ìˆ˜
        
        # ê°€ìƒ ë°°ì¹˜ ë°ì´í„° ìƒì„±
        dummy_input = torch.randn(batch_size, 3, img_size, img_size).to(device)  # ì…ë ¥ ë°ì´í„° ìƒì„±
        dummy_target = torch.randint(0, 17, (batch_size,)).to(device)  # íƒ€ê²Ÿ ë ˆì´ë¸” ìƒì„±
        
        #-------------------------------- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ --------------------------------#
        # Forward pass í…ŒìŠ¤íŠ¸
        with torch.cuda.amp.autocast():                                 # Mixed precision ì‚¬ìš©
            output = dummy_model(dummy_input)                           # ìˆœì „íŒŒ ì‹¤í–‰
            loss = torch.nn.CrossEntropyLoss()(output, dummy_target)    # ì†ì‹¤ ê³„ì‚°
        
        # Backward pass í…ŒìŠ¤íŠ¸
        loss.backward()                                                 # ì—­ì „íŒŒ ì‹¤í–‰
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_used = torch.cuda.memory_allocated() / (1024**3)         # ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ (GB)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del dummy_model, dummy_input, dummy_target, output, loss        # ë³€ìˆ˜ ì‚­ì œ
        torch.cuda.empty_cache()                                        # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
        
        # ì„±ê³µ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜
        return True, memory_used * memory_multiplier
        
    # CUDA ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜ˆì™¸ ì²˜ë¦¬
    except torch.cuda.OutOfMemoryError:
        torch.cuda.empty_cache()                                        # ë©”ëª¨ë¦¬ ì •ë¦¬
        return False, None                                              # ì‹¤íŒ¨ ë°˜í™˜
    
    # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
    except Exception as e:
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")                                 # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        torch.cuda.empty_cache()                                        # ë©”ëª¨ë¦¬ ì •ë¦¬
        return False, None                                              # ì‹¤íŒ¨ ë°˜í™˜


# ==================== ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ í•¨ìˆ˜ ==================== #
# ìµœì ì˜ ë°°ì¹˜ í¬ê¸° ì°¾ê¸° í•¨ìˆ˜ ì •ì˜ (GPU ë“±ê¸‰ë³„ ìµœì í™”)
def find_optimal_batch_size(model_name: str, img_size: int, gpu_info: Dict[str, Any]) -> int:
    print(f"ğŸ” {gpu_info['tier']} GPU ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ ì¤‘...")
    print(f"   GPU: {gpu_info['name']}")
    print(f"   ë©”ëª¨ë¦¬: {gpu_info['total_memory']:.1f} GB")
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {img_size}")

    #------------------------------- ì´ë¯¸ì§€ í¬ê¸°ë³„ ë°°ì¹˜ ì„¤ì • ì„ íƒ -------------------------------#
    # ì´ë¯¸ì§€ í¬ê¸°ë³„ ë°°ì¹˜ ì„¤ì • ì„ íƒ
    if img_size <= 224:                                             # 224 ì´í•˜ì¸ ê²½ìš°
        batch_config = gpu_info['profile']['batch_224']             # 224px ë°°ì¹˜ ì„¤ì •
    elif img_size <= 384:                                           # 224 ì´ˆê³¼ 384 ì´í•˜ì¸ ê²½ìš°
        batch_config = gpu_info['profile']['batch_384']             # 384px ë°°ì¹˜ ì„¤ì •
    else:                                                           # 384 ì´ˆê³¼ì¸ ê²½ìš°
        batch_config = gpu_info['profile']['batch_512']             # 512px ë°°ì¹˜ ì„¤ì •
    
    # ë°°ì¹˜ ì„¤ì • ê°’ ì¶”ì¶œ
    start_batch = batch_config['start']                             # ì‹œì‘ ë°°ì¹˜ í¬ê¸°
    max_batch = batch_config['max']                                 # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
    safety_factor = batch_config['safety']                          # ì•ˆì „ ê³„ìˆ˜
    
    # ì„¤ì • ì •ë³´ ì¶œë ¥
    print(f"   ğŸ“Š {gpu_info['tier']} GPU ê¶Œì¥ ë²”ìœ„: {start_batch} ~ {max_batch}")  # ê¶Œì¥ ë²”ìœ„ ì¶œë ¥
    print(f"   ğŸ›¡ï¸ ì•ˆì „ ë§ˆì§„: {int((1-safety_factor)*100)}%") # ì•ˆì „ ë§ˆì§„ ì¶œë ¥
    
    # ìµœì  ë°°ì¹˜ í¬ê¸° ì´ˆê¸°í™”
    optimal_batch = start_batch
    
    # ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
    low, high = start_batch, max_batch                      # íƒìƒ‰ ë²”ìœ„ ì„¤ì •
    
    #------------------------------ ì´ì§„ íƒìƒ‰ ë°˜ë³µ ------------------------------#
    # ì´ì§„ íƒìƒ‰ ë°˜ë³µ (íƒìƒ‰ ë²”ìœ„ê°€ ìœ íš¨í•œ ë™ì•ˆ)
    while low <= high:
        mid = (low + high) // 2                             # ì¤‘ê°„ê°’ ê³„ì‚°
        
        # ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘ ë©”ì‹œì§€
        print(f"   ë°°ì¹˜ í¬ê¸° {mid} í…ŒìŠ¤íŠ¸ ì¤‘...", end=" ")    # í…ŒìŠ¤íŠ¸ ì§„í–‰ ë©”ì‹œì§€ (ì¤„ë°”ê¿ˆ ì—†ìŒ)
        
        # ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        success, memory_used = test_batch_size(model_name, img_size, mid)  # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        # í…ŒìŠ¤íŠ¸ ì„±ê³µ ì‹œ ì²˜ë¦¬
        if success:
            optimal_batch = mid                             # ìµœì  ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶œë ¥ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
            if memory_used:
                print(f"âœ… (ë©”ëª¨ë¦¬: {memory_used:.2f} GB)")  # ì„±ê³µ ë©”ì‹œì§€ì™€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
            else:
                print("âœ…")                                 # ì„±ê³µ ë©”ì‹œì§€ë§Œ ì¶œë ¥
            low = mid + 1                                   # ë” í° ë°°ì¹˜ ì‹œë„ë¥¼ ìœ„í•´ í•˜í•œê°’ ì¦ê°€
            
        # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬
        else:                                               # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œ
            print("âŒ (ë©”ëª¨ë¦¬ ë¶€ì¡±)")                        # ì‹¤íŒ¨ ë©”ì‹œì§€ ì¶œë ¥
            high = mid - 1                                  # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì‹œë„ë¥¼ ìœ„í•´ ìƒí•œê°’ ê°ì†Œ
    
    #------------------------------ ìµœì¢… ë°°ì¹˜ í¬ê¸° ê³„ì‚° ------------------------------#
    # ì•ˆì „ ë§ˆì§„ ì ìš©
    final_batch = max(4, int(optimal_batch * safety_factor))  # ì•ˆì „ ê³„ìˆ˜ ì ìš© (ìµœì†Œ 4)
    
    # 4ì˜ ë°°ìˆ˜ë¡œ ì¡°ì • (ëª¨ë“  GPUì—ì„œ íš¨ìœ¨ì )
    final_batch = (final_batch // 4) * 4                    # 4ì˜ ë°°ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼
    final_batch = max(4, final_batch)                       # ìµœì†Œ 4 ë³´ì¥
    
    # ìµœì  ë°°ì¹˜ í¬ê¸° ì¶œë ¥
    print(f"\nğŸ¯ {gpu_info['tier']} GPU ìµœì  ë°°ì¹˜ í¬ê¸°: {final_batch}")  # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    
    #------------------------------ GPUë³„ ê¶Œì¥ì‚¬í•­ ì¶œë ¥ ------------------------------#
    # GPUë³„ ì¶”ê°€ ê¶Œì¥ì‚¬í•­ ìƒì„±
    recommendations = []                # ê¶Œì¥ì‚¬í•­ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    
    # ë‚®ì€ ë©”ëª¨ë¦¬ GPU ê¶Œì¥ì‚¬í•­
    if gpu_info['total_memory'] < 8:    # 8GB ë¯¸ë§Œì¸ ê²½ìš°
        recommendations.append("ğŸ’¡ ë‚®ì€ GPU ë©”ëª¨ë¦¬: gradient_accumulation_steps ì‚¬ìš© ê¶Œì¥")  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ê¶Œì¥
    # êµ¬í˜• GPU ê¶Œì¥ì‚¬í•­
    if "GTX" in gpu_info['name']:       # GTX ì‹œë¦¬ì¦ˆì¸ ê²½ìš°
        recommendations.append("ğŸ’¡ êµ¬í˜• GPU: mixed precision (AMP) ë¹„í™œì„±í™” ê¶Œì¥")          # AMP ë¹„í™œì„±í™” ê¶Œì¥
    # ê³ ì„±ëŠ¥ GPU ê¶Œì¥ì‚¬í•­
    if gpu_info['total_memory'] >= 20:  # 20GB ì´ìƒì¸ ê²½ìš°
        recommendations.append("ğŸ’¡ ê³ ì„±ëŠ¥ GPU: ë” í° ëª¨ë¸ì´ë‚˜ ë” ë†’ì€ í•´ìƒë„ ê³ ë ¤ ê°€ëŠ¥")        # ê³ ê¸‰ ì˜µì…˜ ê¶Œì¥
    
    # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    for rec in recommendations:
        print(f"   {rec}")      # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    
    # ìµœì¢… ë°°ì¹˜ í¬ê¸° ë°˜í™˜
    return final_batch


# ==================== ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ ==================== #
# ì„¤ì • íŒŒì¼ì˜ ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸ í•¨ìˆ˜ ì •ì˜
def update_config_file(config_path: str, batch_size: int):
    # ì„¤ì • íŒŒì¼ ì½ê¸°
    with open(config_path, 'r', encoding='utf-8') as f:     # ì„¤ì • íŒŒì¼ ì—´ê¸°
        config = yaml.safe_load(f)                          # YAML íŒŒì¼ íŒŒì‹±
    
    # ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸
    # training ì„¹ì…˜ì´ ìˆëŠ” ê²½ìš° ì—…ë°ì´íŠ¸
    if 'training' in config:
        config['training']['batch_size'] = batch_size       # batch_size ê°’ ì—…ë°ì´íŠ¸
        
    # train ì„¹ì…˜ì´ ìˆëŠ” ê²½ìš° ì—…ë°ì´íŠ¸ (train í‚¤ê°€ ìˆëŠ” ê²½ìš°)
    if 'train' in config:
        config['train']['batch_size'] = batch_size          # batch_size ê°’ ì—…ë°ì´íŠ¸
    
    # ë°±ì—… íŒŒì¼ ìƒì„±
    backup_path = config_path + '.backup'                   # ë°±ì—… íŒŒì¼ ê²½ë¡œ ìƒì„±
    
    # ë°±ì—… íŒŒì¼ ì €ì¥
    with open(backup_path, 'w', encoding='utf-8') as f:     # ë°±ì—… íŒŒì¼ ì—´ê¸°
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)  # YAML í˜•ì‹ìœ¼ë¡œ ì €ì¥
    
    # ì›ë³¸ íŒŒì¼ ì—…ë°ì´íŠ¸
    with open(config_path, 'w', encoding='utf-8') as f:     # ì›ë³¸ íŒŒì¼ ì—´ê¸°
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)  # ì—…ë°ì´íŠ¸ëœ ì„¤ì • ì €ì¥
    
    # ì—…ë°ì´íŠ¸ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
    print(f"âœ… ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸: {config_path}")          # ì—…ë°ì´íŠ¸ ì™„ë£Œ ë©”ì‹œì§€
    print(f"ğŸ“„ ë°±ì—… íŒŒì¼ ìƒì„±: {backup_path}")             # ë°±ì—… íŒŒì¼ ìƒì„± ë©”ì‹œì§€


# ==================== ë©”ì¸ í•¨ìˆ˜ ==================== #
# ë©”ì¸ í•¨ìˆ˜ ì •ì˜
def main():
    # CLI ì¸ì íŒŒì„œ ìƒì„±
    parser = argparse.ArgumentParser(description="RTX 4090 ìµœì í™” ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì •")
    
    # CLI ì¸ì ì •ì˜
    parser.add_argument("--config", required=True, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")                           # í•„ìˆ˜ ì„¤ì • íŒŒì¼ ì¸ì
    parser.add_argument("--test-only", action="store_true", help="í…ŒìŠ¤íŠ¸ë§Œ í•˜ê³  íŒŒì¼ ì—…ë°ì´íŠ¸ ì•ˆí•¨")  # í…ŒìŠ¤íŠ¸ ì „ìš© í”Œë˜ê·¸
    parser.add_argument("--model", type=str, help="ëª¨ë¸ ì´ë¦„ (ìë™ ê°ì§€ ì•ˆë  ë•Œ)")                   # ëª¨ë¸ëª… ìˆ˜ë™ ì§€ì •
    parser.add_argument("--img-size", type=int, help="ì´ë¯¸ì§€ í¬ê¸° (ìë™ ê°ì§€ ì•ˆë  ë•Œ)")              # ì´ë¯¸ì§€ í¬ê¸° ìˆ˜ë™ ì§€ì •

    # CLI ì¸ì íŒŒì‹±
    args = parser.parse_args()                                  # ì¸ì íŒŒì‹± ì‹¤í–‰
    
    # ì„¤ì • íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì„¤ì • íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
    if not os.path.exists(args.config):
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")   # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        sys.exit(1)                                             # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    
    # GPU í™•ì¸ (CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°)
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")                    # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        sys.exit(1)                                             # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    
    # GPU ì •ë³´ ì¶œë ¥
    device_name = torch.cuda.get_device_name()                 # GPU ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°
    gpu_info = get_gpu_info_and_recommendations()              # GPU ì •ë³´ ë° ê¶Œì¥ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°
    
    # GPU ì •ë³´ ìœ íš¨ì„± í™•ì¸ (GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš°)
    if gpu_info is None:
        print("âŒ GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")                # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        sys.exit(1)                                            # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    
    # GPU ì •ë³´ ì¶œë ¥
    print("ğŸš€ íŒ€ í˜‘ì—…ìš© GPU ìµœì í™” ë°°ì¹˜ í¬ê¸° ì¡°ì •ê¸°")               # í”„ë¡œê·¸ë¨ ì œëª©
    print("=" * 55)                                             # êµ¬ë¶„ì„  ì¶œë ¥
    print(f"ğŸ”§ GPU: {device_name}")                            # GPU ëª¨ë¸ëª… ì¶œë ¥
    print(f"ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {gpu_info['total_memory']:.1f} GB")   # ì´ ë©”ëª¨ë¦¬ ì¶œë ¥
    print(f"ğŸ† GPU ë“±ê¸‰: {gpu_info['tier']}")                  # GPU ë“±ê¸‰ ì¶œë ¥
    print(f"ğŸ’¡ ê¶Œì¥ ë°°ì¹˜ ë²”ìœ„: {gpu_info['profile']['batch_224']['start']} ~ {gpu_info['profile']['batch_224']['max']}")  # ê¶Œì¥ ë°°ì¹˜ ë²”ìœ„ ì¶œë ¥
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    with open(args.config, 'r', encoding='utf-8') as f:    # ì„¤ì • íŒŒì¼ ì—´ê¸°
        config = yaml.safe_load(f)                          # YAML íŒŒì¼ íŒŒì‹±
    
    # ëª¨ë¸ ë° ì´ë¯¸ì§€ í¬ê¸° ì¶”ì¶œ
    model_name = args.model or config.get('model', {}).get('name', 'swin_base_patch4_window7_224')  # ëª¨ë¸ëª… ì¶”ì¶œ
    
    # CLI ì¸ìì—ì„œ ì´ë¯¸ì§€ í¬ê¸° ì°¾ê¸° (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
    img_size = args.img_size
    
    # CLI ì¸ìì— ì´ë¯¸ì§€ í¬ê¸°ê°€ ì—†ëŠ” ê²½ìš° ì„¤ì • íŒŒì¼ì—ì„œ ì°¾ê¸° (ì´ë¯¸ì§€ í¬ê¸°ê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
    if not img_size:
        # ì„¤ì • íŒŒì¼ì˜ ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ ì´ë¯¸ì§€ í¬ê¸° íƒìƒ‰
        img_size = (config.get('model', {}).get('img_size') or     # model.img_sizeì—ì„œ ì°¾ê¸°
                   config.get('train', {}).get('img_size') or      # train.img_sizeì—ì„œ ì°¾ê¸°
                   config.get('training', {}).get('img_size') or   # training.img_sizeì—ì„œ ì°¾ê¸°
                   384)                                            # ê¸°ë³¸ê°’ 384
    
    # ëª¨ë¸ ë° ì´ë¯¸ì§€ ì •ë³´ ì¶œë ¥
    print(f"ğŸ“Š ëª¨ë¸: {model_name}")                                 # ì‚¬ìš©í•  ëª¨ë¸ëª… ì¶œë ¥
    print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {img_size}")                             # ì‚¬ìš©í•  ì´ë¯¸ì§€ í¬ê¸° ì¶œë ¥

    # RTX 4090 íŠ¹ë³„ ìµœì í™”
    if "RTX 4090" in device_name:                                   # RTX 4090ì¸ ê²½ìš°
        print("\nğŸ¯ RTX 4090 ê°ì§€! ê³ ì„±ëŠ¥ ìµœì í™” ëª¨ë“œ")                # íŠ¹ë³„ ìµœì í™” ëª¨ë“œ ì•ˆë‚´
        # ì´ë¯¸ì§€ í¬ê¸°ë³„ ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ì„¤ì •
        if img_size <= 224:                                         # 224px ì´í•˜ì¸ ê²½ìš°
            recommended_batch = 96                                  # RTX 4090ì—ì„œ 224pxëŠ” í° ë°°ì¹˜ ê°€ëŠ¥
        elif img_size <= 384:                                       # 384px ì´í•˜ì¸ ê²½ìš°
            recommended_batch = 48                                  # 384pxì—ì„œë„ ì¶©ë¶„í•œ í¬ê¸°
        else:                                                       # 512px ì´ìƒì¸ ê²½ìš°
            recommended_batch = 24                                  # 512px ì´ìƒì—ì„œëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ

        print(f"ğŸ’¡ RTX 4090 ê¶Œì¥ ì‹œì‘ ë°°ì¹˜: {recommended_batch}")    # ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ì¶œë ¥
    
    # ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
    optimal_batch = find_optimal_batch_size(model_name, img_size, gpu_info)  # ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ ì‹¤í–‰
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)                                              # ê²°ê³¼ êµ¬ë¶„ì„ 
    print(f"ğŸ‰ ìµœì¢… ê²°ê³¼:")                                              # ìµœì¢… ê²°ê³¼ ì œëª©
    print(f"   ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch}")                          # ìµœì  ë°°ì¹˜ í¬ê¸° ì¶œë ¥
    print(f"   ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½: ~{((96/optimal_batch)-1)*100:.0f}%")     # ë©”ëª¨ë¦¬ ì ˆì•½ ì˜ˆìƒì¹˜
    print(f"   ì˜ˆìƒ í›ˆë ¨ ì†ë„: {optimal_batch/32:.1f}x ê¸°ì¤€")             # í›ˆë ¨ ì†ë„ ì˜ˆìƒì¹˜
    
    # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì—¬ë¶€ í™•ì¸ (í…ŒìŠ¤íŠ¸ ì „ìš© ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°)
    if not args.test_only:
        # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        update_config_file(args.config, optimal_batch)                  # ì„¤ì • íŒŒì¼ì— ìµœì  ë°°ì¹˜ í¬ê¸° ì ìš©
        
        # ì™„ë£Œ ì•ˆë‚´ ë©”ì‹œì§€
        print(f"\nâœ… ì™„ë£Œ! ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ìµœì í™”ëœ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”:")    # ì™„ë£Œ ë©”ì‹œì§€
        print(f"   python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf")  # ì‹¤í–‰ ëª…ë ¹ì–´ ì•ˆë‚´
    # í…ŒìŠ¤íŠ¸ ì „ìš© ëª¨ë“œì¸ ê²½ìš°
    else:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì•ˆë‚´
        print(f"\nğŸ’¡ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì„¤ì • íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")      # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì•ˆë‚´
        print(f"   ìˆ˜ë™ìœ¼ë¡œ batch_sizeë¥¼ {optimal_batch}ë¡œ ì„¤ì •í•˜ì„¸ìš”.")   # ìˆ˜ë™ ì„¤ì • ì•ˆë‚´


# ==================== ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì§„ì…ì  ==================== #
# ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ ë©”ì¸ í•¨ìˆ˜ í˜¸ì¶œ
if __name__ == "__main__":  # ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ
    main()                  # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
