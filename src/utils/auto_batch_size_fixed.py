#!/usr/bin/env python3                                     # Python3 ì‹¤í–‰ í™˜ê²½ ì§€ì •
"""
íŒ€ í˜‘ì—…ìš© GPU ìµœì í™” ìë™ ë°°ì¹˜ í¬ê¸° ì°¾ê¸° ë„êµ¬ (ìˆ˜ì • ë²„ì „)
ë‹¤ì–‘í•œ GPU í™˜ê²½ì—ì„œ ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì£¼ëŠ” ë„êµ¬
"""

import os                                                   # ìš´ì˜ì²´ì œ íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼
import sys                                                  # ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥
import torch                                                # PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
import gc                                                   # ê°€ë¹„ì§€ ì»¬ë ‰í„° ë©”ëª¨ë¦¬ ê´€ë¦¬
import argparse                                             # CLI ì¸ì íŒŒì‹±
from pathlib import Path                                    # ê²½ë¡œ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from typing import Tuple, Optional, Dict, Any               # íƒ€ì… íŒíŠ¸

# YAML ëª¨ë“ˆ ì„í¬íŠ¸ (ì˜ˆì™¸ ì²˜ë¦¬)
try:
    import yaml                                             # YAML íŒŒì¼ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
except ImportError:                                         # ì„í¬íŠ¸ ì‹¤íŒ¨ ì‹œ
    print("âŒ PyYAMLì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")  # ì„¤ì¹˜ ì•ˆë‚´ ë©”ì‹œì§€
    print("   pip install PyYAML")                          # ì„¤ì¹˜ ëª…ë ¹ì–´ ì¶œë ¥
    sys.exit(1)                                             # í”„ë¡œê·¸ë¨ ì¢…ë£Œ


# ==================== GPU ì •ë³´ ë° ê¶Œì¥ì‚¬í•­ í•¨ìˆ˜ ==================== #
def get_gpu_info_and_recommendations() -> Dict[str, Any]:
    """GPU ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ê¶Œì¥ ì„¤ì •ì„ ë°˜í™˜"""
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°)
    if not torch.cuda.is_available():
        return {                                                        # CPU ëª¨ë“œ ì„¤ì • ë°˜í™˜
            'name': 'CPU',                                              # ë””ë°”ì´ìŠ¤ ì´ë¦„
            'total_memory': 0,                                          # ë©”ëª¨ë¦¬ ìš©ëŸ‰ (CPUëŠ” 0)
            'tier': 'cpu',                                              # ë“±ê¸‰: CPU
            'profile': {                                                # ë°°ì¹˜ í¬ê¸° í”„ë¡œí•„
                'batch_224': {'start': 4, 'max': 8, 'safety': 0.9},     # 224px ì´ë¯¸ì§€ ì„¤ì •
                'batch_384': {'start': 2, 'max': 4, 'safety': 0.9},     # 384px ì´ë¯¸ì§€ ì„¤ì •
                'batch_512': {'start': 1, 'max': 2, 'safety': 0.9}      # 512px ì´ë¯¸ì§€ ì„¤ì •
            }
        }
    
    # GPU ì •ë³´ ìˆ˜ì§‘
    device_name = torch.cuda.get_device_name()                          # GPU ë””ë°”ì´ìŠ¤ ì´ë¦„ ì¡°íšŒ
    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GPU ë©”ëª¨ë¦¬ ìš©ëŸ‰ (GB)
    
    # GPU ë“±ê¸‰ë³„ ë¶„ë¥˜ (í•˜ì´ì—”ë“œ GPU í™•ì¸)
    if any(gpu in device_name for gpu in ['RTX 4090', 'RTX 4080', 'RTX 3090', 'A100', 'V100']):
        tier = 'high_end'                                               # ë“±ê¸‰: í•˜ì´ì—”ë“œ
        profile = {                                                     # í•˜ì´ì—”ë“œ GPU ë°°ì¹˜ ì„¤ì •
            'batch_224': {'start': 64, 'max': 128, 'safety': 0.8},      # 224px: ì‹œì‘ 64, ìµœëŒ€ 128
            'batch_384': {'start': 32, 'max': 64, 'safety': 0.8},       # 384px: ì‹œì‘ 32, ìµœëŒ€ 64
            'batch_512': {'start': 16, 'max': 32, 'safety': 0.8}        # 512px: ì‹œì‘ 16, ìµœëŒ€ 32
        }
    # ë¯¸ë“œë ˆì¸ì§€ GPU í™•ì¸
    elif any(gpu in device_name for gpu in ['RTX 3080', 'RTX 3070', 'RTX 4070']):
        tier = 'mid_range'                                              # ë“±ê¸‰: ë¯¸ë“œë ˆì¸ì§€
        profile = {                                                     # ë¯¸ë“œë ˆì¸ì§€ GPU ë°°ì¹˜ ì„¤ì •
            'batch_224': {'start': 32, 'max': 64, 'safety': 0.8},       # 224px: ì‹œì‘ 32, ìµœëŒ€ 64
            'batch_384': {'start': 16, 'max': 32, 'safety': 0.8},       # 384px: ì‹œì‘ 16, ìµœëŒ€ 32
            'batch_512': {'start': 8, 'max': 16, 'safety': 0.8}         # 512px: ì‹œì‘ 8, ìµœëŒ€ 16
        }
    # ë³´ê¸‰í˜• GPU í™•ì¸
    elif any(gpu in device_name for gpu in ['RTX 3060', 'RTX 2070', 'RTX 2080']):
        tier = 'budget'                                                 # ë“±ê¸‰: ë³´ê¸‰í˜•
        profile = {                                                     # ë³´ê¸‰í˜• GPU ë°°ì¹˜ ì„¤ì •
            'batch_224': {'start': 16, 'max': 32, 'safety': 0.85},      # 224px: ì‹œì‘ 16, ìµœëŒ€ 32
            'batch_384': {'start': 8, 'max': 16, 'safety': 0.85},       # 384px: ì‹œì‘ 8, ìµœëŒ€ 16
            'batch_512': {'start': 4, 'max': 8, 'safety': 0.85}         # 512px: ì‹œì‘ 4, ìµœëŒ€ 8
        }
    # GTX 1660, GTX 1080 ë“± êµ¬í˜• GPU
    else:
        tier = 'low_end'                                                # ë“±ê¸‰: ë¡œìš°ì—”ë“œ
        profile = {                                                     # ë¡œìš°ì—”ë“œ GPU ë°°ì¹˜ ì„¤ì •
            'batch_224': {'start': 8, 'max': 16, 'safety': 0.9},        # 224px: ì‹œì‘ 8, ìµœëŒ€ 16
            'batch_384': {'start': 4, 'max': 8, 'safety': 0.9},         # 384px: ì‹œì‘ 4, ìµœëŒ€ 8
            'batch_512': {'start': 2, 'max': 4, 'safety': 0.9}          # 512px: ì‹œì‘ 2, ìµœëŒ€ 4
        }
    
    # GPU ì •ë³´ ë° ì„¤ì • ë°˜í™˜
    return {
        'name': device_name,                                            # GPU ë””ë°”ì´ìŠ¤ ì´ë¦„
        'total_memory': total_memory,                                   # GPU ë©”ëª¨ë¦¬ ìš©ëŸ‰
        'tier': tier,                                                   # GPU ë“±ê¸‰
        'profile': profile                                              # ë°°ì¹˜ í¬ê¸° í”„ë¡œí•„
    }


# ==================== ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ==================== #
def test_batch_size(model_name: str, img_size: int, batch_size: int) -> Tuple[bool, Optional[float]]:
    """íŠ¹ì • ë°°ì¹˜ í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"""
    # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
    try:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()        # CUDA ìºì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()                    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰

        device = torch.device('cuda')   # CUDA ë””ë°”ì´ìŠ¤ ì„¤ì •

        #------------------------- ê°„ë‹¨í•œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ëª¨ë¸ í¬ê¸°ì™€ ìœ ì‚¬í•˜ê²Œ) -------------------------#
        # Swin Transformer ëª¨ë¸ì¸ ê²½ìš°
        if 'swin' in model_name.lower():
            # Swin Transformer ê·¼ì‚¬ ëª¨ë¸
            model = torch.nn.Sequential(                            # ìˆœì°¨ì  ëª¨ë¸ ìƒì„±
                torch.nn.Conv2d(3, 128, 3, padding=1),              # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ (3â†’128 ì±„ë„)
                torch.nn.BatchNorm2d(128),                          # ë°°ì¹˜ ì •ê·œí™”
                torch.nn.ReLU(),                                    # ReLU í™œì„±í™” í•¨ìˆ˜
                torch.nn.AdaptiveAvgPool2d((7, 7)),                 # ì ì‘í˜• í‰ê·  í’€ë§
                torch.nn.Flatten(),                                 # í‰íƒ„í™”
                torch.nn.Linear(128 * 7 * 7, 1000),                 # ì™„ì „ì—°ê²°ì¸µ 1
                torch.nn.Linear(1000, 100)                          # ì™„ì „ì—°ê²°ì¸µ 2 (ì¶œë ¥)
            ).to(device)                                            # GPUë¡œ ëª¨ë¸ ì´ë™
        # ConvNext ëª¨ë¸ì¸ ê²½ìš°
        elif 'convnext' in model_name.lower():
            # ConvNext ê·¼ì‚¬ ëª¨ë¸
            model = torch.nn.Sequential(                            # ìˆœì°¨ì  ëª¨ë¸ ìƒì„±
                torch.nn.Conv2d(3, 96, 4, stride=4),                # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ (stride=4)
                torch.nn.LayerNorm([96, img_size//4, img_size//4]), # ë ˆì´ì–´ ì •ê·œí™”
                torch.nn.Conv2d(96, 192, 1),                        # 1x1 ì»¨ë³¼ë£¨ì…˜
                torch.nn.AdaptiveAvgPool2d((1, 1)),                 # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
                torch.nn.Flatten(),                                 # í‰íƒ„í™”
                torch.nn.Linear(192, 100)                           # ì™„ì „ì—°ê²°ì¸µ (ì¶œë ¥)
            ).to(device)                                            # GPUë¡œ ëª¨ë¸ ì´ë™
        # ê¸°ë³¸ ResNet ìŠ¤íƒ€ì¼ ëª¨ë¸
        else:
            # ê¸°ë³¸ ResNet ìŠ¤íƒ€ì¼ ëª¨ë¸
            model = torch.nn.Sequential(                            # ìˆœì°¨ì  ëª¨ë¸ ìƒì„±
                torch.nn.Conv2d(3, 64, 7, stride=2, padding=3),     # ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜
                torch.nn.BatchNorm2d(64),                           # ë°°ì¹˜ ì •ê·œí™”
                torch.nn.ReLU(),                                    # ReLU í™œì„±í™” í•¨ìˆ˜
                torch.nn.MaxPool2d(3, stride=2, padding=1),         # ìµœëŒ€ í’€ë§
                torch.nn.AdaptiveAvgPool2d((1, 1)),                 # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
                torch.nn.Flatten(),                                 # í‰íƒ„í™”
                torch.nn.Linear(64, 100)                            # ì™„ì „ì—°ê²°ì¸µ (ì¶œë ¥)
            ).to(device)                                            # GPUë¡œ ëª¨ë¸ ì´ë™

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_input = torch.randn(batch_size, 3, img_size, img_size, device=device)  # ëœë¤ ì…ë ¥ ë°ì´í„°
        test_target = torch.randint(0, 100, (batch_size,), device=device)           # ëœë¤ íƒ€ê²Ÿ ë ˆì´ë¸”
        
        # Forward pass
        output = model(test_input)                                      # ìˆœì „íŒŒ ì‹¤í–‰
        loss = torch.nn.functional.cross_entropy(output, test_target)   # êµì°¨ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ ê³„ì‚°
        
        # Backward pass
        loss.backward() # ì—­ì „íŒŒ ì‹¤í–‰
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        memory_used = torch.cuda.memory_allocated() / (1024**3) # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB)
        
        # ì •ë¦¬
        del model, test_input, test_target, output, loss        # ë©”ëª¨ë¦¬ í•´ì œ
        torch.cuda.empty_cache()                                # CUDA ìºì‹œ ì •ë¦¬
        gc.collect()                                            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        
        # ì„±ê³µ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜
        return True, memory_used
    
    # ëŸ°íƒ€ì„ ì—ëŸ¬ ì²˜ë¦¬
    except RuntimeError as e:
        # ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ì¸ ê²½ìš°
        if "out of memory" in str(e):
            torch.cuda.empty_cache()    # CUDA ìºì‹œ ì •ë¦¬
            gc.collect()                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            return False, None          # ì‹¤íŒ¨ ë°˜í™˜
        # ë‹¤ë¥¸ ëŸ°íƒ€ì„ ì—ëŸ¬ì¸ ê²½ìš°
        else:
            raise e                     # ì—ëŸ¬ ì¬ë°œìƒ
    # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
    except Exception as e:
        torch.cuda.empty_cache()        # CUDA ìºì‹œ ì •ë¦¬
        gc.collect()                    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        return False, None              # ì‹¤íŒ¨ ë°˜í™˜


# ==================== ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ í•¨ìˆ˜ ==================== #
def find_optimal_batch_size(model_name: str, img_size: int, gpu_info: Dict[str, Any]) -> int:
    """ìµœì ì˜ ë°°ì¹˜ í¬ê¸° ì°¾ê¸° (GPU ë“±ê¸‰ë³„ ìµœì í™”)"""
    print(f"ğŸ” {gpu_info['tier']} GPU ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ ì¤‘...")   # íƒìƒ‰ ì‹œì‘ ì•ˆë‚´
    print(f"   GPU: {gpu_info['name']}")                         # GPU ì´ë¦„ ì¶œë ¥
    print(f"   ë©”ëª¨ë¦¬: {gpu_info['total_memory']:.1f} GB")        # GPU ë©”ëª¨ë¦¬ ìš©ëŸ‰ ì¶œë ¥
    print(f"   ëª¨ë¸: {model_name}")                               # ëª¨ë¸ ì´ë¦„ ì¶œë ¥
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {img_size}")                           # ì´ë¯¸ì§€ í¬ê¸° ì¶œë ¥
    
    # ì´ë¯¸ì§€ í¬ê¸°ë³„ í”„ë¡œí•„ ì„ íƒ
    if img_size <= 224:                                     # 224px ì´í•˜ì¸ ê²½ìš°
        batch_config = gpu_info['profile']['batch_224']     # 224px í”„ë¡œí•„ ì„ íƒ
    elif img_size <= 384:                                   # 384px ì´í•˜ì¸ ê²½ìš°
        batch_config = gpu_info['profile']['batch_384']     # 384px í”„ë¡œí•„ ì„ íƒ
    else:                                                   # 512px ì´ìƒì¸ ê²½ìš°
        batch_config = gpu_info['profile']['batch_512']     # 512px í”„ë¡œí•„ ì„ íƒ
    
    start_batch = batch_config['start']                     # ì‹œì‘ ë°°ì¹˜ í¬ê¸°
    max_batch = batch_config['max']                         # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
    safety_factor = batch_config['safety']                  # ì•ˆì „ ë§ˆì§„ ê³„ìˆ˜
    
    print(f"   ğŸ“Š {gpu_info['tier']} GPU ê¶Œì¥ ë²”ìœ„: {start_batch} ~ {max_batch}")  # ê¶Œì¥ ë²”ìœ„ ì¶œë ¥
    print(f"   ğŸ›¡ï¸ ì•ˆì „ ë§ˆì§„: {int((1-safety_factor)*100)}%") # ì•ˆì „ ë§ˆì§„ í¼ì„¼íŠ¸ ì¶œë ¥
    
    optimal_batch = start_batch                             # ìµœì  ë°°ì¹˜ í¬ê¸° ì´ˆê¸°ê°’
    
    # ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
    low, high = start_batch, max_batch                      # íƒìƒ‰ ë²”ìœ„ ì„¤ì •
    
    # ì´ì§„ íƒìƒ‰ ë£¨í”„
    while low <= high:
        mid = (low + high) // 2     # ì¤‘ê°„ê°’ ê³„ì‚°
        
        print(f"   ë°°ì¹˜ í¬ê¸° {mid} í…ŒìŠ¤íŠ¸ ì¤‘...", end=" ")   # í…ŒìŠ¤íŠ¸ ì¤‘ ë©”ì‹œì§€
        
        # ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸
        success, memory_used = test_batch_size(model_name, img_size, mid)
        
        # í…ŒìŠ¤íŠ¸ ì„±ê³µ ì‹œ
        if success:
            optimal_batch = mid         # ìµœì  ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
            if memory_used:
                print(f"âœ… (ë©”ëª¨ë¦¬: {memory_used:.2f} GB)") # ì„±ê³µ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
            else:
                print("âœ…")             # ì„±ê³µ ë©”ì‹œì§€ë§Œ ì¶œë ¥
            
            low = mid + 1               # ë” í° ë°°ì¹˜ í¬ê¸° ì‹œë„
        # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œ
        else:
            print("âŒ (ë©”ëª¨ë¦¬ ë¶€ì¡±)")    # ì‹¤íŒ¨ ë©”ì‹œì§€ ì¶œë ¥
            high = mid - 1              # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì‹œë„
    
    # ì•ˆì „ ë§ˆì§„ ì ìš©
    final_batch = max(4, int(optimal_batch * safety_factor))  # ì•ˆì „ ë§ˆì§„ ì ìš© ë° ìµœì†Œê°’ 4 ë³´ì¥
    
    # 4ì˜ ë°°ìˆ˜ë¡œ ì¡°ì • (ëª¨ë“  GPUì—ì„œ íš¨ìœ¨ì )
    final_batch = (final_batch // 4) * 4    # 4ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •
    final_batch = max(4, final_batch)       # ìµœì†Œê°’ 4 ë³´ì¥
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ¯ {gpu_info['tier']} GPU ìµœì  ë°°ì¹˜ í¬ê¸°: {final_batch}")
    
    # GPUë³„ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
    recommendations = []    # ê¶Œì¥ì‚¬í•­ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    
    # ë©”ëª¨ë¦¬ê°€ 8GB ë¯¸ë§Œì¸ ê²½ìš°
    if gpu_info['total_memory'] < 8:
        # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ê¶Œì¥
        recommendations.append("ğŸ’¡ ë‚®ì€ GPU ë©”ëª¨ë¦¬: gradient_accumulation_steps ì‚¬ìš© ê¶Œì¥")
    # GTX ì‹œë¦¬ì¦ˆ GPUì¸ ê²½ìš°
    if "GTX" in gpu_info['name']:
        # AMP ë¹„í™œì„±í™” ê¶Œì¥
        recommendations.append("ğŸ’¡ êµ¬í˜• GPU: mixed precision (AMP) ë¹„í™œì„±í™” ê¶Œì¥")
    # ë©”ëª¨ë¦¬ê°€ 20GB ì´ìƒì¸ ê²½ìš°
    if gpu_info['total_memory'] >= 20:
        # ê³ ì„±ëŠ¥ í™œìš© ê¶Œì¥
        recommendations.append("ğŸ’¡ ê³ ì„±ëŠ¥ GPU: ë” í° ëª¨ë¸ì´ë‚˜ ë” ë†’ì€ í•´ìƒë„ ê³ ë ¤ ê°€ëŠ¥")
    
    # ê¶Œì¥ì‚¬í•­ ìˆœíšŒ
    for rec in recommendations:                             
        print(f"   {rec}")      # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    
    # ìµœì¢… ë°°ì¹˜ í¬ê¸° ë°˜í™˜
    return final_batch


# ==================== ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ ==================== #
def update_config_file(config_path: str, batch_size: int):
    """ì„¤ì • íŒŒì¼ì˜ ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸"""
    # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
    try:
        with open(config_path, 'r', encoding='utf-8') as f: # ì„¤ì • íŒŒì¼ ì½ê¸°
            config = yaml.safe_load(f)                      # YAML íŒŒì¼ ë¡œë“œ
        
        if 'training' not in config:                        # training ì„¹ì…˜ì´ ì—†ëŠ” ê²½ìš°
            config['training'] = {}                         # training ì„¹ì…˜ ìƒì„±
        
        config['training']['batch_size'] = batch_size       # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        
        with open(config_path, 'w', encoding='utf-8') as f: # ì„¤ì • íŒŒì¼ ì“°ê¸°
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)  # YAML íŒŒì¼ ì €ì¥
        
        print(f"âœ… ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: batch_size = {batch_size}")  # ì—…ë°ì´íŠ¸ ì™„ë£Œ ë©”ì‹œì§€
        
    # ì˜ˆì™¸ ë°œìƒ ì‹œ
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")   # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥


# ==================== ë©”ì¸ í•¨ìˆ˜ ==================== #
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # CLI ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='íŒ€ í˜‘ì—…ìš© GPU ìµœì í™” ìë™ ë°°ì¹˜ í¬ê¸° ì°¾ê¸° ë„êµ¬')   # ì¸ì íŒŒì„œ ìƒì„±
    parser.add_argument('--config', type=str, default='configs/train.yaml',                   # ì„¤ì • íŒŒì¼ ê²½ë¡œ ì¸ì
                        help='YAML ì„¤ì • íŒŒì¼ ê²½ë¡œ')                                             # ê¸°ë³¸ê°’ 'configs/train.yaml'
    parser.add_argument('--model', type=str, help='ëª¨ë¸ ì´ë¦„ (ì˜µì…˜)')                           # ëª¨ë¸ ì´ë¦„ ì¸ì
    parser.add_argument('--img-size', type=int, help='ì´ë¯¸ì§€ í¬ê¸° (ì˜µì…˜)')                      # ì´ë¯¸ì§€ í¬ê¸° ì¸ì
    parser.add_argument('--test-only', action='store_true',                                   # í…ŒìŠ¤íŠ¸ ì „ìš© ëª¨ë“œ ì¸ì
                        help='í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ê³  ì„¤ì • íŒŒì¼ì„ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ')                         # ë„ì›€ë§
    
    args = parser.parse_args()                                  # ì¸ì íŒŒì‹± ì‹¤í–‰
    
    print("ğŸš€ íŒ€ í˜‘ì—…ìš© GPU ìµœì í™” ìë™ ë°°ì¹˜ í¬ê¸° ì°¾ê¸° ë„êµ¬")        # í”„ë¡œê·¸ë¨ ì œëª©
    print("=" * 55)                                             # êµ¬ë¶„ì„  ì¶œë ¥
    
    # ì„¤ì • íŒŒì¼ ì¡´ì¬ í™•ì¸ (ì„¤ì • íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
    if not os.path.exists(args.config):
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")    # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        sys.exit(1)                                             # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    
    # GPU í™•ì¸ (CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°)
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")                      # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        sys.exit(1)                                              # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    
    # GPU ì •ë³´ ë° ê¶Œì¥ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°
    gpu_info = get_gpu_info_and_recommendations()               # GPU ì •ë³´ ì¡°íšŒ

    print(f"ğŸ”§ GPU: {gpu_info['name']}")                         # GPU ì´ë¦„ ì¶œë ¥
    print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_info['total_memory']:.1f} GB")    # GPU ë©”ëª¨ë¦¬ ìš©ëŸ‰ ì¶œë ¥
    print(f"ğŸ† GPU ë“±ê¸‰: {gpu_info['tier']}")                    # GPU ë“±ê¸‰ ì¶œë ¥

    batch_range = gpu_info['profile']['batch_224']              # 224px ë°°ì¹˜ ì„¤ì • ì¡°íšŒ
    print(f"ğŸ’¡ ê¶Œì¥ ë°°ì¹˜ ë²”ìœ„: {batch_range['start']} ~ {batch_range['max']}")  # ê¶Œì¥ ë²”ìœ„ ì¶œë ¥
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    with open(args.config, 'r', encoding='utf-8') as f:         # ì„¤ì • íŒŒì¼ ì—´ê¸°
        config = yaml.safe_load(f)                              # YAML íŒŒì¼ ë¡œë“œ
    
    # ëª¨ë¸ ë° ì´ë¯¸ì§€ í¬ê¸° ì¶”ì¶œ
    model_name = args.model or config.get('model', {}).get('name', 'swin_base_patch4_window7_224')  # ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
    
    # ì´ë¯¸ì§€ í¬ê¸° ì°¾ê¸° (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
    img_size = args.img_size                                # CLI ì¸ìì—ì„œ ì´ë¯¸ì§€ í¬ê¸° ì¡°íšŒ
    
    # CLI ì¸ìì— ì—†ëŠ” ê²½ìš°
    if not img_size:
        img_size = (config.get('model', {}).get('img_size') or      # ì„¤ì • íŒŒì¼ì˜ ì—¬ëŸ¬ ê²½ë¡œì—ì„œ ì‹œë„
                   config.get('train', {}).get('img_size') or       # train ì„¹ì…˜
                   config.get('training', {}).get('img_size') or    # training ì„¹ì…˜
                   config.get('data', {}).get('img_size') or        # data ì„¹ì…˜
                   384)                                             # ê¸°ë³¸ê°’ 384
    
    print(f"ğŸ“Š ëª¨ë¸: {model_name}")                                 # ëª¨ë¸ ì´ë¦„ ì¶œë ¥
    print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {img_size}")                             # ì´ë¯¸ì§€ í¬ê¸° ì¶œë ¥
    
    # ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
    optimal_batch = find_optimal_batch_size(model_name, img_size, gpu_info)  # ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰
    
    print("\n" + "=" * 55)                                          # êµ¬ë¶„ì„  ì¶œë ¥
    print(f"ğŸ‰ ìµœì¢… ê²°ê³¼:")                                         # ìµœì¢… ê²°ê³¼ ì œëª©
    print(f"   ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch}")                     # ìµœì  ë°°ì¹˜ í¬ê¸° ì¶œë ¥
    print(f"   GPU ë“±ê¸‰: {gpu_info['tier']}")                       # GPU ë“±ê¸‰ ì¶œë ¥
    print(f"   ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : ~{(optimal_batch/batch_range['max'])*100:.0f}%")  # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¶œë ¥
    
    # í…ŒìŠ¤íŠ¸ ì „ìš© ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°
    if not args.test_only:
        # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        update_config_file(args.config, optimal_batch)                  # ì„¤ì • íŒŒì¼ì— ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸
        
        print(f"\nâœ… ì™„ë£Œ! ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ìµœì í™”ëœ í›ˆë ¨ì„ ì‹œì‘í•˜ì„¸ìš”:")    # ì™„ë£Œ ë©”ì‹œì§€
        print(f"   python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf")  # ì‹¤í–‰ ëª…ë ¹ì–´ ì•ˆë‚´
        
        # GPUë³„ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ {gpu_info['tier']} GPU ì¶”ê°€ ê¶Œì¥ì‚¬í•­:")  # ì¶”ê°€ ê¶Œì¥ì‚¬í•­ ì œëª©
        
        # ë©”ëª¨ë¦¬ê°€ 8GB ë¯¸ë§Œì¸ ê²½ìš°
        if gpu_info['total_memory'] < 8:
            print(f"   - gradient_accumulation_steps = 2-4 ì‚¬ìš© ê¶Œì¥ (ë‚®ì€ ë©”ëª¨ë¦¬)")  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ê¶Œì¥
            print(f"   - mixed precision ë¹„í™œì„±í™” ê³ ë ¤")                             # AMP ë¹„í™œì„±í™” ê¶Œì¥
        # ë©”ëª¨ë¦¬ê°€ 20GB ì´ìƒì¸ ê²½ìš°
        elif gpu_info['total_memory'] >= 20:
            print(f"   - ë” í° ëª¨ë¸ì´ë‚˜ ensemble ê³ ë ¤ ê°€ëŠ¥")                # í° ëª¨ë¸ ê¶Œì¥
            print(f"   - Multi-GPU training ê°€ëŠ¥")                       # ë©€í‹° GPU ê¶Œì¥
        
        print(f"   - ì‹¤ì œ í›ˆë ¨ ì‹œì‘ ì „ì— ì‘ì€ epochë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”")        # í…ŒìŠ¤íŠ¸ ê¶Œì¥
        print(f"   - ëª¨ë‹ˆí„°ë§: nvidia-smi -l 1 ëª…ë ¹ì–´ë¡œ GPU ì‚¬ìš©ëŸ‰ í™•ì¸")   # ëª¨ë‹ˆí„°ë§ ê¶Œì¥
        print(f"   - íŒ€ì›ê³¼ ë°°ì¹˜ í¬ê¸° ì„¤ì • ê³µìœ í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€")             # íŒ€ í˜‘ì—… ê¶Œì¥
    
    # í…ŒìŠ¤íŠ¸ ì „ìš© ëª¨ë“œì¸ ê²½ìš° 
    else:
        print(f"\nğŸ’¡ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì„¤ì • íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")      # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì•ˆë‚´
        print(f"   ìˆ˜ë™ìœ¼ë¡œ batch_sizeë¥¼ {optimal_batch}ë¡œ ì„¤ì •í•˜ì„¸ìš”.")   # ìˆ˜ë™ ì„¤ì • ì•ˆë‚´
    
    print(f"\nâœ¨ {gpu_info['tier']} GPU ìµœì í™” ì™„ë£Œ!")                  # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
    print(f"ğŸ¤ ë‹¤ë¥¸ íŒ€ì›ë“¤ê³¼ ì„¤ì •ì„ ê³µìœ í•˜ì—¬ í˜‘ì—…í•˜ì„¸ìš”!")                  # í˜‘ì—… ê¶Œì¥ ë©”ì‹œì§€


# ==================== ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì§„ì…ì  ==================== #
if __name__ == "__main__":  # ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ
    main()                  # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
