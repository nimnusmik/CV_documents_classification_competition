#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒ€ì› GPU í˜¸í™˜ì„± ë¹ ë¥¸ ì²´í¬ ë„êµ¬
Quick GPU compatibility check for team members
"""

import torch
import sys

def check_gpu_compatibility():
    """íŒ€ í˜‘ì—…ì„ ìœ„í•œ GPU í˜¸í™˜ì„± ì²´í¬"""
    print("ğŸ” íŒ€ GPU í˜¸í™˜ì„± ì²´í¬")
    print("=" * 40)
    
    # CUDA í™•ì¸
    if not torch.cuda.is_available():
        print("âŒ CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
        print("ğŸ’¡ í•´ê²°ì±…:")
        print("   - NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸")
        print("   - CUDA ì„¤ì¹˜ í™•ì¸")
        print("   - PyTorch CUDA ë²„ì „ í™•ì¸")
        return False
    
    # GPU ì •ë³´ ì¶œë ¥
    device_count = torch.cuda.device_count()
    print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥")
    print(f"ğŸ”§ GPU ê°œìˆ˜: {device_count}")
    
    for i in range(device_count):
        device_name = torch.cuda.get_device_name(i)
        memory_gb = torch.cuda.get_device_properties(i).total_memory / (1024**3)
        
        print(f"\nğŸ“Š GPU {i}: {device_name}")
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {memory_gb:.1f} GB")
        
        # GPU ë“±ê¸‰ ë¶„ë¥˜
        if any(gpu in device_name for gpu in ['RTX 4090', 'RTX 4080', 'RTX 3090', 'A100', 'V100']):
            tier = "ğŸ† HIGH-END"
            batch_rec = "64-128 (224px), 32-64 (384px)"
            note = "ìµœê³  ì„±ëŠ¥! Multi-GPU í›ˆë ¨ ê°€ëŠ¥"
        elif any(gpu in device_name for gpu in ['RTX 3080', 'RTX 3070', 'RTX 4070']):
            tier = "ğŸ¥ˆ MID-RANGE"
            batch_rec = "32-64 (224px), 16-32 (384px)"
            note = "ìš°ìˆ˜í•œ ì„±ëŠ¥! gradient_accumulation_steps=2 ê¶Œì¥"
        elif any(gpu in device_name for gpu in ['RTX 3060', 'RTX 2070', 'RTX 2080']):
            tier = "ğŸ¥‰ BUDGET"
            batch_rec = "16-32 (224px), 8-16 (384px)"
            note = "ì ì ˆí•œ ì„±ëŠ¥! gradient_accumulation_steps=3-4 ê¶Œì¥"
        else:
            tier = "âš ï¸ LOW-END"
            batch_rec = "8-16 (224px), 4-8 (384px)"
            note = "ì£¼ì˜! mixed precision ë¹„í™œì„±í™”, gradient_accumulation_steps=6-8 ê¶Œì¥"
        
        print(f"ğŸ·ï¸ ë“±ê¸‰: {tier}")
        print(f"ğŸ“ ê¶Œì¥ ë°°ì¹˜: {batch_rec}")
        print(f"ğŸ’¡ íŒ: {note}")
    
    # ê¶Œì¥ ëª…ë ¹ì–´
    print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. ìë™ ë°°ì¹˜ í¬ê¸° ìµœì í™”:")
    print(f"      python src/utils/auto_batch_size.py --config configs/train.yaml --test-only")
    print(f"   2. ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸:")
    print(f"      python src/utils/auto_batch_size.py --config configs/train.yaml")
    print(f"   3. í›ˆë ¨ ì‹œì‘:")
    print(f"      python src/training/train_main.py --mode highperf")
    
    # PyTorch ì •ë³´
    print(f"\nğŸ PyTorch ì •ë³´:")
    print(f"   ë²„ì „: {torch.__version__}")
    print(f"   CUDA ì§€ì›: {'Yes' if torch.cuda.is_available() else 'No'}")
    
    if torch.cuda.is_available():
        print(f"   CUDA ì¥ì¹˜ ê°œìˆ˜: {torch.cuda.device_count()}")
    
    print(f"   cuDNN ì‚¬ìš© ê°€ëŠ¥: {'Yes' if torch.backends.cudnn.enabled else 'No'}")
    
    return True

if __name__ == "__main__":
    print("íŒ€ í˜‘ì—…ìš© GPU í˜¸í™˜ì„± ì²´í¬ ë„êµ¬")
    print("Team GPU Compatibility Checker")
    print()
    
    try:
        success = check_gpu_compatibility()
        if success:
            print(f"\nâœ… GPU ì„¤ì • ì™„ë£Œ! íŒ€ í˜‘ì—… ì¤€ë¹„ ì™„ë£Œ!")
        else:
            print(f"\nâŒ GPU ì„¤ì • ë¬¸ì œ ë°œê²¬. ìœ„ì˜ í•´ê²°ì±…ì„ ì°¸ê³ í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"\nğŸ’¥ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ğŸ’¡ Python í™˜ê²½ê³¼ íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
