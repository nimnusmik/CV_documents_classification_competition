# src/inference/infer_highperf.py
"""
ê³ ì„±ëŠ¥ ì¶”ë¡  íŒŒì´í”„ë¼ì¸
- Swin Transformer & ConvNext ì§€ì›: ìµœì‹  ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì›
- Test Time Augmentation (TTA): ë‹¤ì–‘í•œ ì¦ê°•ìœ¼ë¡œ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ
- ì•™ìƒë¸” ì˜ˆì¸¡: ì—¬ëŸ¬ ëª¨ë¸ ê²°ê³¼ í†µí•©
"""

# ------------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
import os                                            # íŒŒì¼/ë””ë ‰í„°ë¦¬ ê²½ë¡œ ì²˜ë¦¬
import torch                                         # PyTorch ë©”ì¸ ëª¨ë“ˆ
import pandas as pd                                  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
import numpy as np                                   # ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
from torch.utils.data import DataLoader, Dataset    # ë°ì´í„° ë¡œë” í´ë˜ìŠ¤, ë°ì´í„°ì…‹ í´ë˜ìŠ¤
import torch.nn.functional as F                      # PyTorch í•¨ìˆ˜í˜• ì¸í„°í˜ì´ìŠ¤
from typing import Optional                          # íƒ€ì… íŒíŠ¸ (ì˜µì…”ë„)
from tqdm import tqdm                                # ì§„í–‰ë¥  í‘œì‹œë°”
from pathlib import Path                             # ê²½ë¡œ ì²˜ë¦¬
from datetime import datetime                        # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬
import yaml                                          # YAML íŒŒì¼ ì²˜ë¦¬

# ------------------------- í”„ë¡œì íŠ¸ ìœ í‹¸ Import ------------------------- #
from src.utils import (
    load_yaml, resolve_path, require_file, require_dir, create_log_path
)  # í•µì‹¬ ìœ í‹¸ë¦¬í‹°
from src.logging.logger import Logger                # ë¡œê·¸ ê¸°ë¡ í´ë˜ìŠ¤
from src.data.dataset import HighPerfDocClsDataset   # ê³ ì„±ëŠ¥ ë¬¸ì„œ ë¶„ë¥˜ ë°ì´í„°ì…‹
from src.data.transforms import get_essential_tta_transforms, get_tta_transforms_by_type  # TTA ë³€í™˜ í•¨ìˆ˜ë“¤
from src.models.build import build_model, get_recommended_model  # ëª¨ë¸ ë¹Œë“œ/ì¶”ì²œ í•¨ìˆ˜

# ------------------------- ì‹œê°í™” ë° ì¶œë ¥ ê´€ë¦¬ ------------------------- #
from src.utils.visualizations import visualize_inference_pipeline, create_organized_output_structure



# ---------------------- Essential TTA ë°ì´í„°ì…‹ ---------------------- #
class ConfigurableTTADataset(Dataset):
    """ì„¤ì • ê°€ëŠ¥í•œ TTAë¥¼ ìœ„í•œ ë°ì´í„°ì…‹"""
    
    def __init__(self, csv_path, image_dir, img_size=384, tta_type="essential", test_df=None, id_col="ID"):
        """
        ì´ˆê¸°í™”
        
        Args:
            csv_path: ì¶”ë¡ í•  ë°ì´í„° CSV ê²½ë¡œ (Noneì´ë©´ test_df ì‚¬ìš©)
            image_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ  
            img_size: ì´ë¯¸ì§€ í¬ê¸°
            tta_type: "essential" (5ê°€ì§€) ë˜ëŠ” "comprehensive" (15ê°€ì§€)
            test_df: CSV ëŒ€ì‹  ì‚¬ìš©í•  DataFrame (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œìš©)
            id_col: ID ì»¬ëŸ¼ëª…
        """
        # ë°ì´í„°í”„ë ˆì„ ë¡œë“œ (CSV ë˜ëŠ” ì§ì ‘ ì „ë‹¬)
        if test_df is not None:
            self.df = test_df
        else:
            self.df = pd.read_csv(csv_path)
        self.image_dir = image_dir
        self.img_size = img_size
        self.tta_type = tta_type
        self.id_col = id_col
        self.transforms = get_tta_transforms_by_type(tta_type, img_size)  # TTA ë³€í™˜ í•¨ìˆ˜ ë¦¬ìŠ¤íŠ¸
        
    def __len__(self):
        return len(self.df)
        
    def __getitem__(self, idx):
        """
        ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œì˜ ëª¨ë“  TTA ë³€í˜• ë°˜í™˜
        
        Returns:
            (augmented_images, image_id): TTA ë³€í˜• ë¦¬ìŠ¤íŠ¸, ì´ë¯¸ì§€ ID
        """
        from PIL import Image  # ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
        # ë°ì´í„° í–‰ ì¶”ì¶œ
        row = self.df.iloc[idx]
        image_id = str(row[self.id_col])  # ì´ë¯¸ì§€ ID ì¶”ì¶œ
        # ì´ë¯¸ì§€ ë¡œë“œ
        img_path = os.path.join(self.image_dir, image_id)
        img = np.array(Image.open(img_path).convert('RGB'))  # RGB ì´ë¯¸ì§€ë¡œ ë³€í™˜
        # ëª¨ë“  TTA ë³€í˜• ì ìš©
        augmented_images = []
        for transform in self.transforms:
            aug_img = transform(image=img)['image']  # TTA ë³€í˜• ì ìš©
            augmented_images.append(aug_img)
        return augmented_images, image_id  # ë³€í˜•ëœ ì´ë¯¸ì§€ì™€ ID ë°˜í™˜


# ---------------------- TTA ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
@torch.no_grad()    # gradient ê³„ì‚° ë¹„í™œì„±í™”
# TTA ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ (ê¸°ì¡´ ë°©ì‹ - í˜¸í™˜ì„± ìœ ì§€)
def predict_with_tta(model, loader, device, num_tta=5):
    """Test Time Augmentationì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ (ê¸°ì¡´ ë°©ì‹)"""
    model.eval()                                     # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    all_preds = []                                   # ëª¨ë“  TTA ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    # TTA íšŸìˆ˜ë§Œí¼ ë°˜ë³µ
    for tta_idx in range(num_tta):
        batch_preds = []                             # ë°°ì¹˜ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        # ë°°ì¹˜ë³„ ì¶”ë¡  ì‹œì‘
        for imgs, _ in tqdm(loader, desc="TTA Inference"):
            imgs = imgs.to(device)                   # ì´ë¯¸ì§€ë¥¼ GPUë¡œ ì´ë™
            logits = model(imgs)                     # ëª¨ë¸ ìˆœì „íŒŒ
            probs = F.softmax(logits, dim=1)         # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜
            batch_preds.append(probs.cpu())          # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
        # ë°°ì¹˜ ê²°í•©
        tta_preds = torch.cat(batch_preds, dim=0)    # ëª¨ë“  ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼ ì—°ê²°
        all_preds.append(tta_preds)                  # TTA ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
    # TTA í‰ê· 
    final_preds = torch.stack(all_preds).mean(dim=0) # ëª¨ë“  TTA ê²°ê³¼ì˜ í‰ê·  ê³„ì‚°
    return final_preds                               # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜


# ---------------------- Essential TTA ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
@torch.no_grad()    # gradient ê³„ì‚° ë¹„í™œì„±í™”
def predict_with_essential_tta(model, tta_loader, device):
    """Essential TTAë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡"""
    model.eval()                                     # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    all_predictions = []                             # ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    for batch_idx, (images_list, _) in enumerate(tqdm(tta_loader, desc="Essential TTA")):
        batch_size = images_list[0].size(0)          # ë°°ì¹˜ í¬ê¸° ì¶”ì¶œ
        batch_probs = torch.zeros(batch_size, 17).to(device)  # 17ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ì´ˆê¸°í™”
        
        # ê° TTA ë³€í˜•ë³„ ì˜ˆì¸¡
        for images in images_list:                   # 5ê°€ì§€ TTA ë³€í˜• ìˆœíšŒ
            images = images.to(device)               # ì´ë¯¸ì§€ë¥¼ GPUë¡œ ì´ë™
            logits = model(images)                   # ëª¨ë¸ ìˆœì „íŒŒ  
            probs = F.softmax(logits, dim=1)         # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜
            batch_probs += probs / len(images_list)  # í‰ê· ì„ ìœ„í•´ ëˆ„ì 
            
        all_predictions.append(batch_probs.cpu())    # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
    
    # ëª¨ë“  ë°°ì¹˜ ê²°í•©
    final_predictions = torch.cat(all_predictions, dim=0)
    return final_predictions                         # ìµœì¢… ì˜ˆì¸¡ í™•ë¥  ë°˜í™˜


# ---------------------- í´ë“œ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ---------------------- #
# í´ë“œë³„ í•™ìŠµëœ ëª¨ë¸ë“¤ ë¡œë“œ í•¨ìˆ˜ ì •ì˜
def load_fold_models(fold_results_path, device):
    """
    í´ë“œ ê²°ê³¼ íŒŒì¼(ë“¤)ì—ì„œ ëª¨ë¸ì„ ë¡œë“œ
    
    Args:
        fold_results_path: ë‹¨ì¼ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—¬ëŸ¬ íŒŒì¼ ê²½ë¡œ
        device: ëª¨ë¸ì„ ë¡œë“œí•  ë””ë°”ì´ìŠ¤
    
    Returns:
        ë¡œë“œëœ ëª¨ë¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    models = []  # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    
    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—¬ëŸ¬ ê²½ë¡œ ì²˜ë¦¬
    if ',' in fold_results_path:
        fold_results_paths = [path.strip() for path in fold_results_path.split(',')]
    else:
        fold_results_paths = [fold_results_path]
    # ê° fold_results íŒŒì¼ ì²˜ë¦¬
    for path in fold_results_paths:
        if not os.path.exists(path):
            # loggerê°€ ì—†ëŠ” ê²½ìš° print ì‚¬ìš©
            if 'logger' in locals():
                logger.write(f"âš ï¸ [WARNING] Fold results file not found: {path}")
            else:
                print(f"Warning: Fold results file not found: {path}")
            continue
        fold_results = load_yaml(path)               # í´ë“œ ê²°ê³¼ YAML íŒŒì¼ ë¡œë“œ
        # ê° í´ë“œ ì •ë³´ ë°˜ë³µ
        for fold_info in fold_results["fold_results"]:
            model_path = fold_info["model_path"]     # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì¶”ì¶œ
            # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
            if os.path.exists(model_path):
                checkpoint = torch.load(model_path, map_location=device)  # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                models.append(checkpoint)  # ì²´í¬í¬ì¸íŠ¸ë¥¼ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            else:
                # loggerê°€ ì—†ëŠ” ê²½ìš° print ì‚¬ìš©
                if 'logger' in locals():
                    logger.write(f"âš ï¸ [WARNING] Model not found: {model_path}")
                else:
                    print(f"Warning: Model not found: {model_path}")
    # ë¡œë“œëœ ëª¨ë¸ë“¤ ë°˜í™˜
    return models


# ---------------------- ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
# ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ (ê¸°ì¡´ ë°©ì‹)
def ensemble_predict(models, test_loader, cfg, device, use_tta=True, logger=None):
    all_ensemble_preds = [] # ëª¨ë“  ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    # ê° ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë°˜ë³µ
    for i, checkpoint in enumerate(models):
        if logger:
            logger.write(f"[MODEL {i+1}/{len(models)}] Processing checkpoint...")
        else:
            print(f"Processing model {i+1}/{len(models)}...")           # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ëª¨ë¸ ë²ˆí˜¸ ì¶œë ¥
        
        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        fold_key = f"fold_{i}"
        # ë¨¼ì € í•´ë‹¹ foldì˜ ëª¨ë¸ëª…ì„ ì°¾ìœ¼ë ¤ ì‹œë„
        if "models" in cfg and fold_key in cfg["models"] and "name" in cfg["models"][fold_key]:
            model_name = get_recommended_model(cfg["models"][fold_key]["name"])
        # fold_0ì˜ ëª¨ë¸ëª…ì„ ì‚¬ìš© (ê°€ì¥ ì¼ë°˜ì ì¸ ì¼€ì´ìŠ¤)
        elif "models" in cfg and "fold_0" in cfg["models"] and "name" in cfg["models"]["fold_0"]:
            model_name = get_recommended_model(cfg["models"]["fold_0"]["name"])
        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì • ì‚¬ìš©
        else:
            model_name = get_recommended_model(cfg["model"]["name"])  # fallback
        
        # ëª¨ë¸ ë¹Œë“œ
        model = build_model(
            model_name,                                             # ëª¨ë¸ëª…
            cfg["data"]["num_classes"],                             # í´ë˜ìŠ¤ ìˆ˜
            pretrained=False,                                       # ê°€ì¤‘ì¹˜ëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ
            drop_rate=cfg["model"]["drop_rate"],                    # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            drop_path_rate=cfg["model"]["drop_path_rate"],          # ë“œë¡­íŒ¨ìŠ¤ ë¹„ìœ¨
            pooling=cfg["model"]["pooling"]                         # í’€ë§ íƒ€ì…
        ).to(device)                                                # GPUë¡œ ëª¨ë¸ ì´ë™
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint["model_state_dict"])
        
        # TTA ì‚¬ìš© ì‹œ
        if use_tta:                                  
            # TTA ì˜ˆì¸¡ ìˆ˜í–‰
            preds = predict_with_tta(model, test_loader, device, num_tta=3)
        # TTA ë¯¸ì‚¬ìš© ì‹œ
        else:
            model.eval()                                            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            batch_preds = []                                        # ë°°ì¹˜ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
            for imgs, _ in tqdm(test_loader, desc=f"Model {i+1} Inference"):  # ë°°ì¹˜ë³„ ì¶”ë¡  ì‹œì‘
                imgs = imgs.to(device)                              # ì´ë¯¸ì§€ë¥¼ GPUë¡œ ì´ë™
                logits = model(imgs)                                # ëª¨ë¸ ìˆœì „íŒŒ
                probs = F.softmax(logits, dim=1)                    # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜
                batch_preds.append(probs.cpu())                     # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                
            # ëª¨ë“  ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼ ê²°í•©
            preds = torch.cat(batch_preds, dim=0)
        
        # í˜„ì¬ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì•™ìƒë¸” ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        all_ensemble_preds.append(preds)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model                   # ëª¨ë¸ ê°ì²´ ì‚­ì œ
        torch.cuda.empty_cache()    # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
    
    # ì•™ìƒë¸” í‰ê· 
    ensemble_preds = torch.stack(all_ensemble_preds).mean(dim=0)    # ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì˜ í‰ê·  ê³„ì‚°
    return ensemble_preds, all_ensemble_preds                       # ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ì™€ ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜


# ---------------------- Essential TTA ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
def ensemble_predict_with_essential_tta(models, tta_loader, cfg, device, logger=None):
    """Essential TTAë¥¼ ì‚¬ìš©í•œ ì•™ìƒë¸” ì˜ˆì¸¡"""
    if logger:
        logger.write(f"ğŸš€ [ENSEMBLE] Starting Essential TTA ensemble prediction with {len(models)} models")
    else:
        print(f"ğŸš€ Essential TTA ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘ (ëª¨ë¸ ìˆ˜: {len(models)})")
    
    all_ensemble_preds = []  # ëª¨ë“  ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    # ê° ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë°˜ë³µ
    for i, checkpoint in enumerate(models):
        if logger:
            logger.write(f"[MODEL {i+1}/{len(models)}] Processing Essential TTA prediction...")
        else:
            print(f"ğŸ“Š ëª¨ë¸ {i+1}/{len(models)} ì²˜ë¦¬ ì¤‘...")
        
        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        # foldë³„ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        fold_key = f"fold_{i}"
        # ë¨¼ì € í•´ë‹¹ foldì˜ ëª¨ë¸ëª…ì„ ì°¾ìœ¼ë ¤ ì‹œë„
        if "models" in cfg and fold_key in cfg["models"] and "name" in cfg["models"][fold_key]:
            model_name = get_recommended_model(cfg["models"][fold_key]["name"])
        # fold_0ì˜ ëª¨ë¸ëª…ì„ ì‚¬ìš© (ê°€ì¥ ì¼ë°˜ì ì¸ ì¼€ì´ìŠ¤)
        elif "models" in cfg and "fold_0" in cfg["models"] and "name" in cfg["models"]["fold_0"]:
            model_name = get_recommended_model(cfg["models"]["fold_0"]["name"])
        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì • ì‚¬ìš©
        else:
            model_name = get_recommended_model(cfg["model"]["name"])  # fallback
        
        if logger:
            logger.write(f"[MODEL {i+1}] Architecture: {model_name}")
            logger.write(f"[MODEL {i+1}] Drop rate: {cfg['model']['drop_rate']}, Drop path: {cfg['model']['drop_path_rate']}")
            logger.write(f"[MODEL {i+1}] Building model and loading checkpoint...")
        
        # ëª¨ë¸ ë¹Œë“œ
        model = build_model(
            model_name,
            cfg["data"]["num_classes"],
            pretrained=False,
            drop_rate=cfg["model"]["drop_rate"],
            drop_path_rate=cfg["model"]["drop_path_rate"],
            pooling=cfg["model"]["pooling"]
        ).to(device)
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint["model_state_dict"])
        
        if logger:
            logger.write(f"[MODEL {i+1}] Model loaded successfully, starting Essential TTA prediction...")
        
        # Essential TTA ì˜ˆì¸¡ ìˆ˜í–‰
        model_preds = predict_with_essential_tta(model, tta_loader, device)
        all_ensemble_preds.append(model_preds)
        
        if logger:
            logger.write(f"[MODEL {i+1}] âœ“ Essential TTA prediction completed | shape={model_preds.shape}")
        else:
            print(f"âœ… ëª¨ë¸ {i+1} ì™„ë£Œ (ì˜ˆì¸¡ í˜•íƒœ: {model_preds.shape})")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
    
    # ì•™ìƒë¸” í‰ê·  ê³„ì‚°
    if logger:
        logger.write("[ENSEMBLE] Computing ensemble average...")
    else:
        print("ğŸ”„ ì•™ìƒë¸” í‰ê·  ê³„ì‚° ì¤‘...")
    ensemble_preds = torch.stack(all_ensemble_preds).mean(dim=0)
    
    if logger:
        logger.write(f"[ENSEMBLE] âœ“ Essential TTA ensemble prediction completed | final_shape={ensemble_preds.shape}")
    else:
        print(f"ğŸ‰ Essential TTA ì•™ìƒë¸” ì˜ˆì¸¡ ì™„ë£¼! ìµœì¢… ì˜ˆì¸¡ í˜•íƒœ: {ensemble_preds.shape}")
    return ensemble_preds, all_ensemble_preds


# ---------------------- ì„¤ì • ê°€ëŠ¥í•œ TTA í—¬í¼ í•¨ìˆ˜ ---------------------- #
def create_configurable_tta_dataloader(sample_csv, test_dir, img_size=384, tta_type="essential", batch_size=32, num_workers=8):
    """ì„¤ì • ê°€ëŠ¥í•œ TTA ë°ì´í„°ë¡œë” ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    
    # TTA ë°ì´í„°ì…‹ ìƒì„±
    tta_dataset = ConfigurableTTADataset(sample_csv, test_dir, img_size, tta_type)
    
    # TTA ë³€í˜• ìˆ˜ ê³„ì‚°
    num_tta_transforms = len(tta_dataset.transforms)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    tta_loader = DataLoader(
        tta_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=lambda x: (
            [torch.stack([item[0][i] for item in x]) for i in range(num_tta_transforms)],  # TTA ë³€í˜•ë“¤
            [item[1] for item in x]  # ì´ë¯¸ì§€ IDë“¤
        )
    )
    
    tta_type_name = "Essential (5ê°€ì§€)" if tta_type == "essential" else "Comprehensive (15ê°€ì§€)"
    # logger ì „ë‹¬ í•„ìš” - ì„ì‹œë¡œ print ìœ ì§€
    print(f"ğŸ”§ {tta_type_name} TTA ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ")
    print(f"   - ë°ì´í„°ì…‹ í¬ê¸°: {len(tta_dataset)}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")  
    print(f"   - TTA ë³€í˜• ìˆ˜: {num_tta_transforms}ê°€ì§€")
    
    return tta_loader


# ---------------------- í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ ---------------------- #
def create_essential_tta_dataloader(sample_csv, test_dir, img_size=384, batch_size=32, num_workers=8):
    """Essential TTA ë°ì´í„°ë¡œë” ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return create_configurable_tta_dataloader(sample_csv, test_dir, img_size, "essential", batch_size, num_workers)


# ---------------------- ì¶œë ¥ ë””ë ‰í„°ë¦¬ êµ¬ì¡° ìƒì„± í•¨ìˆ˜ ---------------------- #
def create_output_structure(cfg):
    """
    ì˜¬ë°”ë¥¸ ì¶œë ¥ ë””ë ‰í„°ë¦¬ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    êµ¬ì¡°:
    - experiments/infer/YYYYMMDD/YYYYMMDD_HHMM_run_name/ (ëª¨ë“  ê²°ê³¼)
    - experiments/infer/lastest-infer/ (ìµœì‹  ê²°ê³¼ ë³µì‚¬)  
    - submissions/YYYYMMDD/ (CSV íŒŒì¼ë§Œ)
    
    Returns:
        tuple: (experiments_dir, lastest_dir, submissions_dir, csv_filename)
    """
    current_date = datetime.now().strftime('%Y%m%d')
    current_time = datetime.now().strftime('%H%M')  # 4ìë¦¬ ì‹œê°„ (HHMM)
    run_name = cfg["project"]["run_name"]
    
    # ì˜¬ë°”ë¥¸ í´ë”ëª… í˜•ì‹: ë‚ ì§œ_ì‹œê°„_run_name
    folder_name = f"{current_date}_{current_time}_{run_name}"
    
    # ë””ë ‰í„°ë¦¬ ê²½ë¡œ ìƒì„±
    experiments_dir = f"experiments/infer/{current_date}/{folder_name}"
    lastest_dir = "experiments/infer/lastest-infer"  # ì˜¬ë°”ë¥¸ í´ë”ëª…
    submissions_dir = f"submissions/{current_date}"
    
    # lastest-infer í´ë” ê¸°ì¡´ ë‚´ìš© ì‚­ì œ
    if os.path.exists(lastest_dir):
        import shutil
        shutil.rmtree(lastest_dir)
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ lastest-infer í´ë” ì‚­ì œ ì™„ë£Œ")
    
    # ë””ë ‰í„°ë¦¬ ìƒì„±
    os.makedirs(experiments_dir, exist_ok=True)
    os.makedirs(lastest_dir, exist_ok=True)
    os.makedirs(submissions_dir, exist_ok=True)
    
    # CSV íŒŒì¼ëª… ìƒì„±
    csv_filename = f"{current_date}_{current_time}_{run_name}_ensemble.csv"
    
    return experiments_dir, lastest_dir, submissions_dir, csv_filename


# ---------------------- ê³ ì„±ëŠ¥ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---------------------- #
# ê³ ì„±ëŠ¥ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
def run_highperf_inference(cfg_path: str, fold_results_path: str, output_path: Optional[str] = None):
    # ì„¤ì • ë¡œë“œ
    cfg = load_yaml(cfg_path)                                       # YAML ì„¤ì • íŒŒì¼ ë¡œë“œ
    cfg_dir = os.path.dirname(os.path.abspath(cfg_path))            # ì„¤ì • íŒŒì¼ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
    
    # ë¡œê±° ì„¤ì •
    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M')
    logger = Logger(
        log_path=create_log_path("infer", f"infer_highperf_{timestamp}.log")  # ë‚ ì§œë³„ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    )
    
    # íŒŒì´í”„ë¼ì¸ ì‹œì‘ ë¡œê·¸
    logger.write("[BOOT] high-performance inference pipeline started")
    
    try:
        # GPU/CPU ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.write(f"[BOOT] device={device}")                         # ë””ë°”ì´ìŠ¤ ì •ë³´ ë¡œê·¸
        
        # ê²½ë¡œ í™•ì¸
        sample_csv = resolve_path(cfg_dir, cfg["data"]["sample_csv"])   # ìƒ˜í”Œ CSV ê²½ë¡œ í•´ê²°
        test_dir = resolve_path(cfg_dir, cfg["data"]["image_dir_test"]) # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ ê²½ë¡œ í•´ê²°
        require_file(sample_csv, "sample_csv í™•ì¸")                     # ìƒ˜í”Œ CSV íŒŒì¼ ì¡´ì¬ì„± ê²€ì¦
        require_dir(test_dir, "test_dir í™•ì¸")                          # í…ŒìŠ¤íŠ¸ ë””ë ‰í„°ë¦¬ ì¡´ì¬ì„± ê²€ì¦
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_df = pd.read_csv(sample_csv)                               # í…ŒìŠ¤íŠ¸ ë°ì´í„° CSV ë¡œë“œ
        logger.write(f"[DATA] loaded test data | shape={test_df.shape}")# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë¡œê·¸
        logger.write(f"[DATA] test image directory: {test_dir}")
        logger.write(f"[DATA] image size: {cfg['train']['img_size']}x{cfg['train']['img_size']}")
        logger.write(f"[DATA] batch size: {cfg['train']['batch_size']}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
        test_ds = HighPerfDocClsDataset(
            test_df,                                 # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
            test_dir,                                # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬
            img_size=cfg["train"]["img_size"],       # ì´ë¯¸ì§€ í¬ê¸°
            is_train=False,                          # í‰ê°€ ëª¨ë“œ í”Œë˜ê·¸
            id_col=cfg["data"]["id_col"],            # ID ì»¬ëŸ¼ëª…
            target_col=None,                         # ì¶”ë¡  ëª¨ë“œ (íƒ€ê¹ƒ ì—†ìŒ)
            logger=logger                            # ë¡œê±° ê°ì²´
        )                                            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”
        test_loader = DataLoader(                    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œë” ìƒì„±
            test_ds,                                 # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
            batch_size=cfg["train"]["batch_size"],   # ë°°ì¹˜ í¬ê¸°
            shuffle=False,                           # ì…”í”Œ ë¹„í™œì„±í™” (ì¶”ë¡ ìš©)
            num_workers=cfg["project"]["num_workers"],  # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
            pin_memory=True                          # ë©”ëª¨ë¦¬ ê³ ì • í™œì„±í™”
        )
        
        logger.write(f"[DATA] test dataset size: {len(test_ds)}")  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í¬ê¸° ë¡œê·¸
        logger.write(f"[DATA] test dataloader created | workers={cfg['project']['num_workers']}")
        
        # TTA ì„¤ì • í™•ì¸
        tta_enabled = cfg.get("inference", {}).get("tta", True)
        tta_type = cfg.get("inference", {}).get("tta_type", "essential")
        logger.write("=" * 50)
        logger.write(f"[TTA] TTA enabled: {tta_enabled}, type: {tta_type}")
        
        # í´ë“œë³„ ëª¨ë¸ ë¡œë“œ
        models = load_fold_models(fold_results_path, device)
        logger.write(f"[MODELS] loaded {len(models)} fold models")
        
        if tta_enabled:
            # ì„¤ì • ê°€ëŠ¥í•œ TTA ë°ì´í„°ë¡œë” ìƒì„±
            tta_loader = create_configurable_tta_dataloader(
                sample_csv=sample_csv,
                test_dir=test_dir,
                img_size=cfg["train"]["img_size"],
                tta_type=tta_type,
                batch_size=cfg["train"]["batch_size"],
                num_workers=cfg["project"]["num_workers"]
            )
            
            # TTA ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰
            logger.write(f"[INFERENCE] starting {tta_type} TTA ensemble prediction...")
            ensemble_preds, individual_model_preds = ensemble_predict_with_essential_tta(models, tta_loader, cfg, device, logger)
        else:
            # ê¸°ë³¸ ì•™ìƒë¸” ì˜ˆì¸¡ (TTA ì—†ìŒ)
            logger.write(f"[INFERENCE] starting basic ensemble prediction (no TTA)...")
            ensemble_preds, individual_model_preds = ensemble_predict(models, test_loader, cfg, device, use_tta=False)
        
        # ìµœì¢… ì˜ˆì¸¡ í´ë˜ìŠ¤
        final_predictions = ensemble_preds.argmax(dim=1).numpy()    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_scores = ensemble_preds.max(dim=1)[0].numpy()    # ìµœëŒ€ í™•ë¥ ê°’ì„ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
        
        #-------------- ê²°ê³¼ ì €ì¥ ë° ë¡œê·¸ ---------------------- #
        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ êµ¬ì¡° ìƒì„±
        experiments_dir, lastest_dir, submissions_dir, csv_filename = create_output_structure(cfg)
        
        # TTA íƒ€ì…ì— ë”°ë¼ íŒŒì¼ëª… ìˆ˜ì •
        tta_suffix = f"_{tta_type}_tta" if tta_enabled else "_no_tta"
        csv_filename = csv_filename.replace("_ensemble.csv", f"_ensemble{tta_suffix}.csv")
        
        # ê° ë””ë ‰í„°ë¦¬ë³„ ê²½ë¡œ ì„¤ì •
        experiments_csv_path = os.path.join(experiments_dir, csv_filename)
        lastest_csv_path = os.path.join(lastest_dir, csv_filename) 
        submissions_csv_path = os.path.join(submissions_dir, csv_filename)
        
        # ì¶œë ¥ ê²½ë¡œê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©, ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        if output_path is not None:
            main_output_path = output_path
        else:
            main_output_path = experiments_csv_path
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission = test_df.copy()                                             # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
        submission[cfg["data"]["target_col"]] = final_predictions               # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
        
        # ë©”ì¸ ê²½ë¡œì— ì €ì¥ (ì‚¬ìš©ì ì§€ì • ê²½ë¡œ ë˜ëŠ” experiments ê²½ë¡œ)
        submission.to_csv(main_output_path, index=False)
        
        # ê¸°ë³¸ êµ¬ì¡°ì— ë”°ë¼ ì¶”ê°€ ì €ì¥ (ì‚¬ìš©ì ì§€ì • ê²½ë¡œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
        if output_path is None:
            # experiments/infer/lastest-infer/ì— ë³µì‚¬
            submission.to_csv(lastest_csv_path, index=False)
            
            # submissions/ë‚ ì§œ/ì— CSVë§Œ ì €ì¥
            submission.to_csv(submissions_csv_path, index=False)
            
            logger.write(f"[SUCCESS] Files saved to:")
            logger.write(f"  - Main: {main_output_path}")
            logger.write(f"  - Lastest: {lastest_csv_path}")
            logger.write(f"  - Submission: {submissions_csv_path}")
        else:
            logger.write(f"[SUCCESS] Inference completed | output: {main_output_path}")
        
        # ì‹œê°í™”ìš© ê¸°ë³¸ ë””ë ‰í„°ë¦¬ëŠ” experiments ë””ë ‰í„°ë¦¬ ì‚¬ìš©
        viz_base_dir = experiments_dir if output_path is None else os.path.dirname(main_output_path)
        logger.write(f"[RESULT] Prediction distribution:")                      # ì˜ˆì¸¡ ë¶„í¬ ë¡œê·¸ ì‹œì‘
        
        # ê° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ìˆ˜ ê³„ì‚°
        for i, count in enumerate(np.bincount(final_predictions)):
            # í´ë˜ìŠ¤ë³„ ë¶„í¬ ë¡œê·¸
            logger.write(f"  Class {i}: {count} samples ({count/len(final_predictions)*100:.1f}%)")
        
        #-------------- ì¶”ë¡  ê²°ê³¼ ì‹œê°í™” ---------------------- #
        try:
            # ì‹œê°í™”ë¥¼ ìœ„í•œ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì„¤ì •
            
            if "models" in cfg:
                # ë¡œë“œëœ ëª¨ë¸ ìˆ˜ì— ë§ëŠ” ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸ ìƒì„±
                model_names = []
                for i in range(len(individual_model_preds)):
                    fold_key = f"fold_{i}"
                    # ë¨¼ì € í•´ë‹¹ foldì˜ ëª¨ë¸ëª…ì„ ì°¾ìœ¼ë ¤ ì‹œë„
                    if fold_key in cfg["models"] and "name" in cfg["models"][fold_key]:
                        model_names.append(cfg["models"][fold_key]["name"])
                    # fold_0ì˜ ëª¨ë¸ëª…ì„ ì‚¬ìš© (ê°€ì¥ ì¼ë°˜ì ì¸ ì¼€ì´ìŠ¤)
                    elif "fold_0" in cfg["models"] and "name" in cfg["models"]["fold_0"]:
                        model_names.append(cfg["models"]["fold_0"]["name"])
                    # ê¸°ë³¸ ëª¨ë¸ ì„¤ì • ì‚¬ìš©
                    else:
                        model_names.append(cfg.get("model", {}).get("name", "unknown_model"))
                
                logger.write("=" * 50)
                logger.write("[VISUALIZATION] Starting individual model visualizations...")
                for i in range(len(individual_model_preds)):
                    # ê° ëª¨ë¸ë³„ ê³ ìœ  ë””ë ‰í„°ë¦¬ ìƒì„±
                    model_viz_dir = os.path.join(viz_base_dir, f"model_{i+1}_{model_names[i]}")
                    os.makedirs(model_viz_dir, exist_ok=True)
                    logger.write(f"[VISUALIZATION] Creating visualizations for model_{i+1}_{model_names[i]} at {model_viz_dir}")
                    
                    # ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™” ìƒì„±
                    individual_preds = individual_model_preds[i].argmax(dim=1).numpy()
                    visualize_inference_pipeline(
                        predictions=individual_preds,
                        model_name=model_names[i],
                        output_dir=model_viz_dir,
                        confidence_scores=individual_model_preds[i].max(dim=1)[0].numpy()
                    )
                    logger.write(f"[VISUALIZATION] âœ“ Model {i+1} visualization completed")
                
                # ì•™ìƒë¸” ê²°ê³¼ ì‹œê°í™”
                logger.write("[VISUALIZATION] Creating ensemble visualizations...")
                ensemble_viz_dir = os.path.join(viz_base_dir, "ensemble")
                os.makedirs(ensemble_viz_dir, exist_ok=True)
                logger.write(f"[VISUALIZATION] Ensemble visualization directory: {ensemble_viz_dir}")
                visualize_inference_pipeline(
                    predictions=final_predictions,
                    model_name="ensemble", 
                    output_dir=ensemble_viz_dir,
                    confidence_scores=confidence_scores
                )
                logger.write("[VISUALIZATION] âœ“ Ensemble visualization completed")
                
            else:
                model_names = [cfg["model"].get("name", "unknown")]
                
                # ë‹¨ì¼ ëª¨ë¸ ì‹œê°í™”
                single_model_viz_dir = os.path.join(viz_base_dir, f"single_{model_names[0]}")
                os.makedirs(single_model_viz_dir, exist_ok=True)
            
                visualize_inference_pipeline(
                    predictions=ensemble_preds.numpy(),
                    model_name=model_names[0],
                    output_dir=single_model_viz_dir,
                    confidence_scores=confidence_scores
                )
            
            logger.write(f"[VIZ] Inference visualizations created in {viz_base_dir}")
            
            # lastest-inferì— ì „ì²´ ê²°ê³¼ ë³µì‚¬ (ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©ì‹œì—ë§Œ)
            if output_path is None:
                try:
                    import shutil
                    
                    # ëª¨ë“  íŒŒì¼ê³¼ í´ë”ë¥¼ lastest-inferì— ë³µì‚¬
                    for item in os.listdir(viz_base_dir):
                        source_item = os.path.join(viz_base_dir, item)
                        dest_item = os.path.join(lastest_dir, item)
                        
                        if os.path.isdir(source_item):
                            # í´ë”ì¸ ê²½ìš° ë³µì‚¬
                            shutil.copytree(source_item, dest_item, dirs_exist_ok=True)
                        else:
                            # íŒŒì¼ì¸ ê²½ìš° ë³µì‚¬ (CSV íŒŒì¼ í¬í•¨)
                            shutil.copy2(source_item, dest_item)
                    
                    logger.write(f"[SUCCESS] All results copied to lastest-infer folder")
                
                except Exception as copy_error:
                    logger.write(f"[WARNING] Failed to copy to lastest-infer: {str(copy_error)}")
            
        except Exception as viz_error:
            logger.write(f"[WARNING] Visualization failed: {str(viz_error)}")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
        return main_output_path
        
    # ì˜ˆì™¸ ë°œìƒ ì‹œ
    except Exception as e:
        logger.write(f"[ERROR] Inference failed: {str(e)}") # ì—ëŸ¬ ë¡œê·¸
        raise                                               # ì˜ˆì™¸ ì¬ë°œìƒ
    # ìµœì¢…ì ìœ¼ë¡œ ì‹¤í–‰
    finally:
        logger.write("[SHUTDOWN] Inference pipeline ended") # íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ë¡œê·¸


# ---------------------- ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---------------------- #
if __name__ == "__main__":
    import sys  # sys ëª¨ë“ˆ import
    
    # ì¸ì ê°œìˆ˜ í™•ì¸
    if len(sys.argv) < 3:
        print("Usage: python infer_highperf.py <config_path> <fold_results_path> [output_path]")    # ì‚¬ìš©ë²• ì¶œë ¥
        sys.exit(1)                                                                                 # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    
    cfg_path = sys.argv[1]                                                          # ì„¤ì • íŒŒì¼ ê²½ë¡œ
    fold_results_path = sys.argv[2]                                                 # í´ë“œ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
    output_path = sys.argv[3] if len(sys.argv) > 3 else None                        # ì¶œë ¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    
    result_path = run_highperf_inference(cfg_path, fold_results_path, output_path)  # ê³ ì„±ëŠ¥ ì¶”ë¡  ì‹¤í–‰
    print(f"Inference completed! Results saved to: {result_path}")                  # ì¶”ë¡  ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
    
    run_highperf_inference(cfg_path, fold_results_path, output_path)                # ê³ ì„±ëŠ¥ ì¶”ë¡  ì‹¤í–‰
