# src/inference/infer_highperf.py
"""
ê³ ì„±ëŠ¥ ì¶”ë¡  íŒŒì´í”„ë¼ì¸
- Swin Transformer & ConvNext ì§€ì›: ìµœì‹  ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì›
- Test Time Augmentation (TTA): ë‹¤ì–‘í•œ ì¦ê°•ìœ¼ë¡œ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ
- ì•™ìƒë¸” ì˜ˆì¸¡: ì—¬ëŸ¬ ëª¨ë¸ ê²°ê³¼ í†µí•©
"""

# ------------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
import os                                            # íŒŒì¼/ë””ë ‰í„°ë¦¬ ê²½ë¡œ ì²˜ë¦¬
import torch                                         # PyTorch ë©”ì¸ ëª¨ë“ˆ
import pandas as pd                                  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
import numpy as np                                   # ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
from torch.utils.data import DataLoader              # ë°ì´í„° ë¡œë” í´ë˜ìŠ¤
import torch.nn.functional as F                      # PyTorch í•¨ìˆ˜í˜• ì¸í„°í˜ì´ìŠ¤
from typing import Optional                          # íƒ€ì… íŒíŠ¸ (ì˜µì…”ë„)
from tqdm import tqdm                                # ì§„í–‰ë¥  í‘œì‹œë°”

# ------------------------- í”„ë¡œì íŠ¸ ìœ í‹¸ Import ------------------------- #
from src.utils import (
    load_yaml, resolve_path, require_file, require_dir, create_log_path
)  # í•µì‹¬ ìœ í‹¸ë¦¬í‹°
from src.logging.logger import Logger                # ë¡œê·¸ ê¸°ë¡ í´ë˜ìŠ¤
from src.data.dataset import HighPerfDocClsDataset   # ê³ ì„±ëŠ¥ ë¬¸ì„œ ë¶„ë¥˜ ë°ì´í„°ì…‹
from src.models.build import build_model, get_recommended_model  # ëª¨ë¸ ë¹Œë“œ/ì¶”ì²œ í•¨ìˆ˜

# ------------------------- ì‹œê°í™” ë° ì¶œë ¥ ê´€ë¦¬ ------------------------- #
from src.utils.visualizations import visualize_inference_pipeline, create_organized_output_structure


# ---------------------- TTA ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
@torch.no_grad()    # gradient ê³„ì‚° ë¹„í™œì„±í™”
# TTA ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
def predict_with_tta(model, loader, device, num_tta=5):
    """Test Time Augmentationì„ ì‚¬ìš©í•œ ì˜ˆì¸¡"""
    model.eval()                                     # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    all_preds = []                                   # ëª¨ë“  TTA ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    # TTA íšŸìˆ˜ë§Œí¼ ë°˜ë³µ
    for _ in range(num_tta):
        batch_preds = []                             # ë°°ì¹˜ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        
        # ë°°ì¹˜ë³„ ì¶”ë¡  ì‹œì‘
        for imgs, _ in tqdm(loader, desc="TTA Inference"):
            imgs = imgs.to(device)                   # ì´ë¯¸ì§€ë¥¼ GPUë¡œ ì´ë™
            logits = model(imgs)                     # ëª¨ë¸ ìˆœì „íŒŒ
            probs = F.softmax(logits, dim=1)         # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜
            batch_preds.append(probs.cpu())          # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
        
        # ë°°ì¹˜ ê²°í•©
        tta_preds = torch.cat(batch_preds, dim=0)    # ëª¨ë“  ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼ ì—°ê²°
        all_preds.append(tta_preds)                  # TTA ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
    
    # TTA í‰ê· 
    final_preds = torch.stack(all_preds).mean(dim=0) # ëª¨ë“  TTA ê²°ê³¼ì˜ í‰ê·  ê³„ì‚°
    return final_preds                               # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜


# ---------------------- í´ë“œ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ---------------------- #
# í´ë“œë³„ í•™ìŠµëœ ëª¨ë¸ë“¤ ë¡œë“œ í•¨ìˆ˜ ì •ì˜
def load_fold_models(fold_results_path, device):
    fold_results = load_yaml(fold_results_path)      # í´ë“œ ê²°ê³¼ YAML íŒŒì¼ ë¡œë“œ
    models = []                                      # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    
    # ê° í´ë“œ ì •ë³´ ë°˜ë³µ
    for fold_info in fold_results["fold_results"]:
        model_path = fold_info["model_path"]         # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì¶”ì¶œ
        
        # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if os.path.exists(model_path):
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = torch.load(model_path, map_location=device)
            # ì²´í¬í¬ì¸íŠ¸ë¥¼ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            models.append(checkpoint)
        # ëª¨ë¸ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
        else:
            # ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
            print(f"Warning: Model not found: {model_path}")
    
    # ë¡œë“œëœ ëª¨ë¸ë“¤ ë°˜í™˜
    return models


# ---------------------- ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
# ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
def ensemble_predict(models, test_loader, cfg, device, use_tta=True):
    all_ensemble_preds = [] # ëª¨ë“  ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    # ê° ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë°˜ë³µ
    for i, checkpoint in enumerate(models):
        print(f"Processing model {i+1}/{len(models)}...")           # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ëª¨ë¸ ë²ˆí˜¸ ì¶œë ¥
        
        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        model_name = get_recommended_model(cfg["model"]["name"])    # ê¶Œì¥ ëª¨ë¸ëª… ì¶”ì¶œ
        
        # ëª¨ë¸ ë¹Œë“œ
        model = build_model(
            model_name,                                             # ëª¨ë¸ëª…
            cfg["data"]["num_classes"],                             # í´ë˜ìŠ¤ ìˆ˜
            pretrained=False,                                       # ê°€ì¤‘ì¹˜ëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ
            drop_rate=cfg["model"]["drop_rate"],                    # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            drop_path_rate=cfg["model"]["drop_path_rate"],          # ë“œë¡­íŒ¨ìŠ¤ ë¹„ìœ¨
            pooling=cfg["model"]["pooling"]                         # í’€ë§ íƒ€ì…
        ).to(device)                                                # GPUë¡œ ëª¨ë¸ ì´ë™
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint["model_state_dict"])
        
        # TTA ì‚¬ìš© ì‹œ
        if use_tta:                                  
            # TTA ì˜ˆì¸¡ ìˆ˜í–‰
            preds = predict_with_tta(model, test_loader, device, num_tta=3)
        # TTA ë¯¸ì‚¬ìš© ì‹œ
        else:
            model.eval()                                            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            batch_preds = []                                        # ë°°ì¹˜ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
            for imgs, _ in tqdm(test_loader, desc=f"Model {i+1} Inference"):  # ë°°ì¹˜ë³„ ì¶”ë¡  ì‹œì‘
                imgs = imgs.to(device)                              # ì´ë¯¸ì§€ë¥¼ GPUë¡œ ì´ë™
                logits = model(imgs)                                # ëª¨ë¸ ìˆœì „íŒŒ
                probs = F.softmax(logits, dim=1)                    # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜
                batch_preds.append(probs.cpu())                     # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                
            # ëª¨ë“  ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼ ê²°í•©
            preds = torch.cat(batch_preds, dim=0)
        
        # í˜„ì¬ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì•™ìƒë¸” ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        all_ensemble_preds.append(preds)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model                   # ëª¨ë¸ ê°ì²´ ì‚­ì œ
        torch.cuda.empty_cache()    # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
    
    # ì•™ìƒë¸” í‰ê· 
    ensemble_preds = torch.stack(all_ensemble_preds).mean(dim=0)    # ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì˜ í‰ê·  ê³„ì‚°
    return ensemble_preds                                           # ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜


# ---------------------- ê³ ì„±ëŠ¥ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---------------------- #
# ê³ ì„±ëŠ¥ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
def run_highperf_inference(cfg_path: str, fold_results_path: str, output_path: Optional[str] = None):
    # ì„¤ì • ë¡œë“œ
    cfg = load_yaml(cfg_path)                                       # YAML ì„¤ì • íŒŒì¼ ë¡œë“œ
    cfg_dir = os.path.dirname(os.path.abspath(cfg_path))            # ì„¤ì • íŒŒì¼ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
    
    # ë¡œê±° ì„¤ì •
    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M')
    logger = Logger(
        log_path=create_log_path("infer", f"infer_highperf_{timestamp}.log")  # ë‚ ì§œë³„ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    )
    
    # íŒŒì´í”„ë¼ì¸ ì‹œì‘ ë¡œê·¸
    logger.write("[BOOT] high-performance inference pipeline started")
    
    try:
        # GPU/CPU ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.write(f"[BOOT] device={device}")                         # ë””ë°”ì´ìŠ¤ ì •ë³´ ë¡œê·¸
        
        # ê²½ë¡œ í™•ì¸
        sample_csv = resolve_path(cfg_dir, cfg["data"]["sample_csv"])   # ìƒ˜í”Œ CSV ê²½ë¡œ í•´ê²°
        test_dir = resolve_path(cfg_dir, cfg["data"]["image_dir_test"]) # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ ê²½ë¡œ í•´ê²°
        require_file(sample_csv, "sample_csv í™•ì¸")                     # ìƒ˜í”Œ CSV íŒŒì¼ ì¡´ì¬ì„± ê²€ì¦
        require_dir(test_dir, "test_dir í™•ì¸")                          # í…ŒìŠ¤íŠ¸ ë””ë ‰í„°ë¦¬ ì¡´ì¬ì„± ê²€ì¦
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_df = pd.read_csv(sample_csv)                               # í…ŒìŠ¤íŠ¸ ë°ì´í„° CSV ë¡œë“œ
        logger.write(f"[DATA] loaded test data | shape={test_df.shape}")# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë¡œê·¸
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
        test_ds = HighPerfDocClsDataset(
            test_df,                                 # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
            test_dir,                                # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬
            img_size=cfg["train"]["img_size"],       # ì´ë¯¸ì§€ í¬ê¸°
            is_train=False,                          # í‰ê°€ ëª¨ë“œ í”Œë˜ê·¸
            id_col=cfg["data"]["id_col"],            # ID ì»¬ëŸ¼ëª…
            target_col=None,                         # ì¶”ë¡  ëª¨ë“œ (íƒ€ê¹ƒ ì—†ìŒ)
            logger=logger                            # ë¡œê±° ê°ì²´
        )                                            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”
        test_loader = DataLoader(                    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œë” ìƒì„±
            test_ds,                                 # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
            batch_size=cfg["train"]["batch_size"],   # ë°°ì¹˜ í¬ê¸°
            shuffle=False,                           # ì…”í”Œ ë¹„í™œì„±í™” (ì¶”ë¡ ìš©)
            num_workers=cfg["project"]["num_workers"],  # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
            pin_memory=True                          # ë©”ëª¨ë¦¬ ê³ ì • í™œì„±í™”
        )
        
        logger.write(f"[DATA] test dataset size: {len(test_ds)}")  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í¬ê¸° ë¡œê·¸
        
        # ëª¨ë¸ ì•™ìƒë¸” ì˜ˆì¸¡
        logger.write(f"[INFERENCE] starting ensemble prediction...")# ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘ ë¡œê·¸
        
        # í´ë“œë³„ ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡
        models = load_fold_models(fold_results_path, device)                                # í´ë“œë³„ ëª¨ë¸ ë¡œë“œ
        ensemble_preds = ensemble_predict(models, test_loader, cfg, device, use_tta=True)   # ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰
        
        # ìµœì¢… ì˜ˆì¸¡ í´ë˜ìŠ¤
        final_predictions = ensemble_preds.argmax(dim=1).numpy()    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_scores = ensemble_preds.max(dim=1)[0].numpy()    # ìµœëŒ€ í™•ë¥ ê°’ì„ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
        
        #-------------- ê²°ê³¼ ì €ì¥ ë° ë¡œê·¸ ---------------------- #
        # ì¶œë ¥ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ë™ì  íŒŒì¼ëª… ìƒì„±
        if output_path is None:
            current_date = pd.Timestamp.now().strftime('%Y%m%d')
            current_time = pd.Timestamp.now().strftime('%H%M')
            model_name = cfg["model"]["name"]
            
            # ì¦ê°• íƒ€ì… ê²°ì • (í•™ìŠµ ì„¤ì •ê³¼ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©)
            aug_type = "advanced_augmentation" if cfg["train"].get("use_advanced_augmentation", False) else "basic_augmentation"
            
            filename = f"{current_date}_{current_time}_{model_name}_ensemble_tta_{aug_type}.csv"
            output_path = f"submissions/{current_date}/{filename}"
        
        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission = test_df.copy()                                             # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
        submission[cfg["data"]["target_col"]] = final_predictions               # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
        submission.to_csv(output_path, index=False)                             # CSV íŒŒì¼ë¡œ ì €ì¥
        
        logger.write(f"[SUCCESS] Inference completed | output: {output_path}")  # ì¶”ë¡  ì™„ë£Œ ë¡œê·¸
        logger.write(f"[RESULT] Prediction distribution:")                      # ì˜ˆì¸¡ ë¶„í¬ ë¡œê·¸ ì‹œì‘
        
        # ê° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ìˆ˜ ê³„ì‚°
        for i, count in enumerate(np.bincount(final_predictions)):
            # í´ë˜ìŠ¤ë³„ ë¶„í¬ ë¡œê·¸
            logger.write(f"  Class {i}: {count} samples ({count/len(final_predictions)*100:.1f}%)")
        
        #-------------- ì¶”ë¡  ê²°ê³¼ ì‹œê°í™” ---------------------- #
        try:
            # ì‹œê°í™”ë¥¼ ìœ„í•œ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì„¤ì •
            viz_output_dir = os.path.dirname(output_path)
            model_name = cfg["model"]["name"]
            
            # ì‹œê°í™” ìƒì„±
            visualize_inference_pipeline(
                predictions=ensemble_preds.numpy(),
                model_name=model_name,
                output_dir=viz_output_dir,
                confidence_scores=confidence_scores
            )
            logger.write(f"[VIZ] Inference visualizations created in {viz_output_dir}")
            
        except Exception as viz_error:
            logger.write(f"[WARNING] Visualization failed: {str(viz_error)}")
        
        #-------------- lastest-infer í´ë”ì— ê²°ê³¼ ì €ì¥ ---------------------- #
        try:
            import shutil
            import time
            
            # experiments/infer/ë‚ ì§œ/ì‹¤í—˜ëª…/ êµ¬ì¡° ìƒì„±
            date_str = time.strftime('%Y%m%d')
            timestamp = time.strftime('%Y%m%d_%H%M')
            run_name = cfg.get("project", {}).get("run_name", "inference")
            
            # ë‚ ì§œë³„ infer ê²°ê³¼ ë””ë ‰í„°ë¦¬
            infer_output_dir = f"experiments/infer/{date_str}/{timestamp}_{run_name}"
            os.makedirs(infer_output_dir, exist_ok=True)
            
            # lastest-infer í´ë”ì— ì§ì ‘ ì €ì¥ (ê¸°ì¡´ ë‚´ìš© ì‚­ì œ í›„)
            lastest_infer_dir = "experiments/infer/lastest-infer"
            
            # ê¸°ì¡´ lastest-infer í´ë” ì‚­ì œ (ì™„ì „ êµì²´)
            if os.path.exists(lastest_infer_dir):
                shutil.rmtree(lastest_infer_dir)
                logger.write(f"[CLEANUP] Removed existing lastest-infer folder")
            
            os.makedirs(lastest_infer_dir, exist_ok=True)
            
            # ì¶”ë¡  ê²°ê³¼ CSVë¥¼ lastest-inferì— ë³µì‚¬
            import copy
            lastest_output_path = os.path.join(lastest_infer_dir, f"submission_{timestamp}.csv")
            shutil.copy2(output_path, lastest_output_path)
            
            # ì„¤ì • íŒŒì¼ë„ ë³µì‚¬
            import yaml
            config_copy_path = os.path.join(lastest_infer_dir, "config.yaml")
            with open(config_copy_path, 'w') as f:
                yaml.dump(cfg, f, default_flow_style=False, allow_unicode=True)
            
            # ì‹œê°í™” ê²°ê³¼ë„ ë³µì‚¬ (ìˆë‹¤ë©´)
            viz_source_dir = os.path.dirname(output_path)
            if os.path.exists(os.path.join(viz_source_dir, "images")):
                shutil.copytree(os.path.join(viz_source_dir, "images"), 
                               os.path.join(lastest_infer_dir, "images"))
            
            logger.write(f"[COPY] Results copied directly to lastest-infer")
            logger.write(f"ğŸ“ Latest inference results: {lastest_infer_dir}")
            
        except Exception as copy_error:
            logger.write(f"[WARNING] Failed to copy to lastest-infer: {str(copy_error)}")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
        return output_path
        
    # ì˜ˆì™¸ ë°œìƒ ì‹œ
    except Exception as e:
        logger.write(f"[ERROR] Inference failed: {str(e)}") # ì—ëŸ¬ ë¡œê·¸
        raise                                               # ì˜ˆì™¸ ì¬ë°œìƒ
    # ìµœì¢…ì ìœ¼ë¡œ ì‹¤í–‰
    finally:
        logger.write("[SHUTDOWN] Inference pipeline ended") # íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ë¡œê·¸


# ---------------------- ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---------------------- #
if __name__ == "__main__":
    import sys  # sys ëª¨ë“ˆ import
    
    # ì¸ì ê°œìˆ˜ í™•ì¸
    if len(sys.argv) < 3:
        print("Usage: python infer_highperf.py <config_path> <fold_results_path> [output_path]")    # ì‚¬ìš©ë²• ì¶œë ¥
        sys.exit(1)                                                                                 # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    
    cfg_path = sys.argv[1]                                                          # ì„¤ì • íŒŒì¼ ê²½ë¡œ
    fold_results_path = sys.argv[2]                                                 # í´ë“œ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
    output_path = sys.argv[3] if len(sys.argv) > 3 else None                        # ì¶œë ¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    
    result_path = run_highperf_inference(cfg_path, fold_results_path, output_path)  # ê³ ì„±ëŠ¥ ì¶”ë¡  ì‹¤í–‰
    print(f"Inference completed! Results saved to: {result_path}")                  # ì¶”ë¡  ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
    
    run_highperf_inference(cfg_path, fold_results_path, output_path)                # ê³ ì„±ëŠ¥ ì¶”ë¡  ì‹¤í–‰
