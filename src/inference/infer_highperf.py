# src/inference/infer_highperf.py
"""
ê³ ì„±ëŠ¥ ì¶”ë¡  íŒŒì´í”„ë¼ì¸
- Swin Transformer & ConvNext ì§€ì›: ìµœì‹  ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì›
- Test Time Augmentation (TTA): ë‹¤ì–‘í•œ ì¦ê°•ìœ¼ë¡œ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ
- ì•™ìƒë¸” ì˜ˆì¸¡: ì—¬ëŸ¬ ëª¨ë¸ ê²°ê³¼ í†µí•©
"""

# ------------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
import os                                            # íŒŒì¼/ë””ë ‰í„°ë¦¬ ê²½ë¡œ ì²˜ë¦¬
import torch                                         # PyTorch ë©”ì¸ ëª¨ë“ˆ
import pandas as pd                                  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
import numpy as np                                   # ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
from torch.utils.data import DataLoader, Dataset    # ë°ì´í„° ë¡œë” í´ë˜ìŠ¤, ë°ì´í„°ì…‹ í´ë˜ìŠ¤
import torch.nn.functional as F                      # PyTorch í•¨ìˆ˜í˜• ì¸í„°í˜ì´ìŠ¤
from typing import Optional                          # íƒ€ì… íŒíŠ¸ (ì˜µì…”ë„)
from tqdm import tqdm                                # ì§„í–‰ë¥  í‘œì‹œë°”

# ------------------------- í”„ë¡œì íŠ¸ ìœ í‹¸ Import ------------------------- #
from src.utils import (
    load_yaml, resolve_path, require_file, require_dir, create_log_path
)  # í•µì‹¬ ìœ í‹¸ë¦¬í‹°
from src.logging.logger import Logger                # ë¡œê·¸ ê¸°ë¡ í´ë˜ìŠ¤
from src.data.dataset import HighPerfDocClsDataset   # ê³ ì„±ëŠ¥ ë¬¸ì„œ ë¶„ë¥˜ ë°ì´í„°ì…‹
from src.data.transforms import get_essential_tta_transforms, get_tta_transforms_by_type  # TTA ë³€í™˜ í•¨ìˆ˜ë“¤
from src.models.build import build_model, get_recommended_model  # ëª¨ë¸ ë¹Œë“œ/ì¶”ì²œ í•¨ìˆ˜

# ------------------------- ì‹œê°í™” ë° ì¶œë ¥ ê´€ë¦¬ ------------------------- #
from src.utils.visualizations import visualize_inference_pipeline, create_organized_output_structure



# ---------------------- Essential TTA ë°ì´í„°ì…‹ ---------------------- #
class ConfigurableTTADataset(Dataset):
    """ì„¤ì • ê°€ëŠ¥í•œ TTAë¥¼ ìœ„í•œ ë°ì´í„°ì…‹"""
    
    def __init__(self, csv_path, image_dir, img_size=384, tta_type="essential", test_df=None, id_col="ID"):
        """
        ì´ˆê¸°í™”
        
        Args:
            csv_path: ì¶”ë¡ í•  ë°ì´í„° CSV ê²½ë¡œ (Noneì´ë©´ test_df ì‚¬ìš©)
            image_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ  
            img_size: ì´ë¯¸ì§€ í¬ê¸°
            tta_type: "essential" (5ê°€ì§€) ë˜ëŠ” "comprehensive" (15ê°€ì§€)
            test_df: CSV ëŒ€ì‹  ì‚¬ìš©í•  DataFrame (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œìš©)
            id_col: ID ì»¬ëŸ¼ëª…
        """
        if test_df is not None:
            self.df = test_df
        else:
            self.df = pd.read_csv(csv_path)
        self.image_dir = image_dir
        self.img_size = img_size
        self.tta_type = tta_type
        self.id_col = id_col
        self.transforms = get_tta_transforms_by_type(tta_type, img_size)
        
    def __len__(self):
        return len(self.df)
        
    def __getitem__(self, idx):
        """
        ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œì˜ ëª¨ë“  TTA ë³€í˜• ë°˜í™˜
        
        Returns:
            (augmented_images, image_id): TTA ë³€í˜• ë¦¬ìŠ¤íŠ¸, ì´ë¯¸ì§€ ID
        """
        from PIL import Image
        
        row = self.df.iloc[idx]
        image_id = str(row[self.id_col])
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        img_path = os.path.join(self.image_dir, image_id)
        img = np.array(Image.open(img_path).convert('RGB'))
        
        # ëª¨ë“  TTA ë³€í˜• ì ìš©
        augmented_images = []
        for transform in self.transforms:
            aug_img = transform(image=img)['image']
            augmented_images.append(aug_img)
            
        return augmented_images, image_id


# ---------------------- TTA ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
@torch.no_grad()    # gradient ê³„ì‚° ë¹„í™œì„±í™”
# TTA ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ (ê¸°ì¡´ ë°©ì‹ - í˜¸í™˜ì„± ìœ ì§€)
def predict_with_tta(model, loader, device, num_tta=5):
    """Test Time Augmentationì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ (ê¸°ì¡´ ë°©ì‹)"""
    model.eval()                                     # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    all_preds = []                                   # ëª¨ë“  TTA ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    # TTA íšŸìˆ˜ë§Œí¼ ë°˜ë³µ
    for _ in range(num_tta):
        batch_preds = []                             # ë°°ì¹˜ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        
        # ë°°ì¹˜ë³„ ì¶”ë¡  ì‹œì‘
        for imgs, _ in tqdm(loader, desc="TTA Inference"):
            imgs = imgs.to(device)                   # ì´ë¯¸ì§€ë¥¼ GPUë¡œ ì´ë™
            logits = model(imgs)                     # ëª¨ë¸ ìˆœì „íŒŒ
            probs = F.softmax(logits, dim=1)         # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜
            batch_preds.append(probs.cpu())          # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
        
        # ë°°ì¹˜ ê²°í•©
        tta_preds = torch.cat(batch_preds, dim=0)    # ëª¨ë“  ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼ ì—°ê²°
        all_preds.append(tta_preds)                  # TTA ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
    
    # TTA í‰ê· 
    final_preds = torch.stack(all_preds).mean(dim=0) # ëª¨ë“  TTA ê²°ê³¼ì˜ í‰ê·  ê³„ì‚°
    return final_preds                               # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜


# ---------------------- Essential TTA ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
@torch.no_grad()    # gradient ê³„ì‚° ë¹„í™œì„±í™”
def predict_with_essential_tta(model, tta_loader, device):
    """íŒ€ì›ì˜ Essential TTAë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡"""
    model.eval()                                     # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    all_predictions = []                             # ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    for batch_idx, (images_list, _) in enumerate(tqdm(tta_loader, desc="Essential TTA")):
        batch_size = images_list[0].size(0)          # ë°°ì¹˜ í¬ê¸° ì¶”ì¶œ
        batch_probs = torch.zeros(batch_size, 17).to(device)  # 17ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ì´ˆê¸°í™”
        
        # ê° TTA ë³€í˜•ë³„ ì˜ˆì¸¡
        for images in images_list:                   # 5ê°€ì§€ TTA ë³€í˜• ìˆœíšŒ
            images = images.to(device)               # ì´ë¯¸ì§€ë¥¼ GPUë¡œ ì´ë™
            logits = model(images)                   # ëª¨ë¸ ìˆœì „íŒŒ  
            probs = F.softmax(logits, dim=1)         # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜
            batch_probs += probs / len(images_list)  # í‰ê· ì„ ìœ„í•´ ëˆ„ì 
            
        all_predictions.append(batch_probs.cpu())    # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
    
    # ëª¨ë“  ë°°ì¹˜ ê²°í•©
    final_predictions = torch.cat(all_predictions, dim=0)
    return final_predictions                         # ìµœì¢… ì˜ˆì¸¡ í™•ë¥  ë°˜í™˜


# ---------------------- í´ë“œ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ---------------------- #
# í´ë“œë³„ í•™ìŠµëœ ëª¨ë¸ë“¤ ë¡œë“œ í•¨ìˆ˜ ì •ì˜
def load_fold_models(fold_results_path, device):
    fold_results = load_yaml(fold_results_path)      # í´ë“œ ê²°ê³¼ YAML íŒŒì¼ ë¡œë“œ
    models = []                                      # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    
    # ê° í´ë“œ ì •ë³´ ë°˜ë³µ
    for fold_info in fold_results["fold_results"]:
        model_path = fold_info["model_path"]         # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì¶”ì¶œ
        
        # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if os.path.exists(model_path):
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = torch.load(model_path, map_location=device)
            # ì²´í¬í¬ì¸íŠ¸ë¥¼ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            models.append(checkpoint)
        # ëª¨ë¸ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
        else:
            # ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
            print(f"Warning: Model not found: {model_path}")
    
    # ë¡œë“œëœ ëª¨ë¸ë“¤ ë°˜í™˜
    return models


# ---------------------- ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
# ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ (ê¸°ì¡´ ë°©ì‹)
def ensemble_predict(models, test_loader, cfg, device, use_tta=True):
    all_ensemble_preds = [] # ëª¨ë“  ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    # ê° ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë°˜ë³µ
    for i, checkpoint in enumerate(models):
        print(f"Processing model {i+1}/{len(models)}...")           # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ëª¨ë¸ ë²ˆí˜¸ ì¶œë ¥
        
        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        model_name = get_recommended_model(cfg["model"]["name"])    # ê¶Œì¥ ëª¨ë¸ëª… ì¶”ì¶œ
        
        # ëª¨ë¸ ë¹Œë“œ
        model = build_model(
            model_name,                                             # ëª¨ë¸ëª…
            cfg["data"]["num_classes"],                             # í´ë˜ìŠ¤ ìˆ˜
            pretrained=False,                                       # ê°€ì¤‘ì¹˜ëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ
            drop_rate=cfg["model"]["drop_rate"],                    # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            drop_path_rate=cfg["model"]["drop_path_rate"],          # ë“œë¡­íŒ¨ìŠ¤ ë¹„ìœ¨
            pooling=cfg["model"]["pooling"]                         # í’€ë§ íƒ€ì…
        ).to(device)                                                # GPUë¡œ ëª¨ë¸ ì´ë™
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint["model_state_dict"])
        
        # TTA ì‚¬ìš© ì‹œ
        if use_tta:                                  
            # TTA ì˜ˆì¸¡ ìˆ˜í–‰
            preds = predict_with_tta(model, test_loader, device, num_tta=3)
        # TTA ë¯¸ì‚¬ìš© ì‹œ
        else:
            model.eval()                                            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            batch_preds = []                                        # ë°°ì¹˜ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
            for imgs, _ in tqdm(test_loader, desc=f"Model {i+1} Inference"):  # ë°°ì¹˜ë³„ ì¶”ë¡  ì‹œì‘
                imgs = imgs.to(device)                              # ì´ë¯¸ì§€ë¥¼ GPUë¡œ ì´ë™
                logits = model(imgs)                                # ëª¨ë¸ ìˆœì „íŒŒ
                probs = F.softmax(logits, dim=1)                    # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜
                batch_preds.append(probs.cpu())                     # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                
            # ëª¨ë“  ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼ ê²°í•©
            preds = torch.cat(batch_preds, dim=0)
        
        # í˜„ì¬ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì•™ìƒë¸” ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        all_ensemble_preds.append(preds)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model                   # ëª¨ë¸ ê°ì²´ ì‚­ì œ
        torch.cuda.empty_cache()    # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
    
    # ì•™ìƒë¸” í‰ê· 
    ensemble_preds = torch.stack(all_ensemble_preds).mean(dim=0)    # ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì˜ í‰ê·  ê³„ì‚°
    return ensemble_preds                                           # ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜


# ---------------------- Essential TTA ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ---------------------- #
def ensemble_predict_with_essential_tta(models, tta_loader, cfg, device):
    """íŒ€ì›ì˜ Essential TTAë¥¼ ì‚¬ìš©í•œ ì•™ìƒë¸” ì˜ˆì¸¡"""
    print(f"ğŸš€ Essential TTA ì•™ìƒë¸” ì˜ˆì¸¡ ì‹œì‘ (ëª¨ë¸ ìˆ˜: {len(models)})")
    
    all_ensemble_preds = []  # ëª¨ë“  ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    
    # ê° ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë°˜ë³µ
    for i, checkpoint in enumerate(models):
        print(f"ğŸ“Š ëª¨ë¸ {i+1}/{len(models)} ì²˜ë¦¬ ì¤‘...")
        
        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        model_name = get_recommended_model(cfg["model"]["name"])
        
        # ëª¨ë¸ ë¹Œë“œ
        model = build_model(
            model_name,
            cfg["data"]["num_classes"],
            pretrained=False,
            drop_rate=cfg["model"]["drop_rate"],
            drop_path_rate=cfg["model"]["drop_path_rate"],
            pooling=cfg["model"]["pooling"]
        ).to(device)
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint["model_state_dict"])
        
        # Essential TTA ì˜ˆì¸¡ ìˆ˜í–‰
        model_preds = predict_with_essential_tta(model, tta_loader, device)
        all_ensemble_preds.append(model_preds)
        
        print(f"âœ… ëª¨ë¸ {i+1} ì™„ë£Œ (ì˜ˆì¸¡ í˜•íƒœ: {model_preds.shape})")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        torch.cuda.empty_cache()
    
    # ì•™ìƒë¸” í‰ê·  ê³„ì‚°
    print("ğŸ”„ ì•™ìƒë¸” í‰ê·  ê³„ì‚° ì¤‘...")
    ensemble_preds = torch.stack(all_ensemble_preds).mean(dim=0)
    
    print(f"ğŸ‰ Essential TTA ì•™ìƒë¸” ì˜ˆì¸¡ ì™„ë£Œ! ìµœì¢… ì˜ˆì¸¡ í˜•íƒœ: {ensemble_preds.shape}")
    return ensemble_preds


# ---------------------- ì„¤ì • ê°€ëŠ¥í•œ TTA í—¬í¼ í•¨ìˆ˜ ---------------------- #
def create_configurable_tta_dataloader(sample_csv, test_dir, img_size=384, tta_type="essential", batch_size=32, num_workers=8):
    """ì„¤ì • ê°€ëŠ¥í•œ TTA ë°ì´í„°ë¡œë” ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    
    # TTA ë°ì´í„°ì…‹ ìƒì„±
    tta_dataset = ConfigurableTTADataset(sample_csv, test_dir, img_size, tta_type)
    
    # TTA ë³€í˜• ìˆ˜ ê³„ì‚°
    num_tta_transforms = len(tta_dataset.transforms)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    tta_loader = DataLoader(
        tta_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=lambda x: (
            [torch.stack([item[0][i] for item in x]) for i in range(num_tta_transforms)],  # TTA ë³€í˜•ë“¤
            [item[1] for item in x]  # ì´ë¯¸ì§€ IDë“¤
        )
    )
    
    tta_type_name = "Essential (5ê°€ì§€)" if tta_type == "essential" else "Comprehensive (15ê°€ì§€)"
    print(f"ğŸ”§ {tta_type_name} TTA ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ")
    print(f"   - ë°ì´í„°ì…‹ í¬ê¸°: {len(tta_dataset)}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")  
    print(f"   - TTA ë³€í˜• ìˆ˜: {num_tta_transforms}ê°€ì§€")
    
    return tta_loader


# ---------------------- í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ ---------------------- #
def create_essential_tta_dataloader(sample_csv, test_dir, img_size=384, batch_size=32, num_workers=8):
    """Essential TTA ë°ì´í„°ë¡œë” ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return create_configurable_tta_dataloader(sample_csv, test_dir, img_size, "essential", batch_size, num_workers)


# ---------------------- ì‚¬ìš© ì˜ˆì œ (ì£¼ì„) ---------------------- #
"""
íŒ€ì›ì˜ Essential TTAë¥¼ ì‚¬ìš©í•œ ì¶”ë¡  ì˜ˆì œ:

```python
from src.inference.infer_highperf import (
    load_fold_models, 
    create_essential_tta_dataloader,
    ensemble_predict_with_essential_tta
)
from src.data.transforms import get_essential_tta_transforms

# 1. í´ë“œ ëª¨ë¸ë“¤ ë¡œë“œ
models = load_fold_models("./experiments/train/lastest-train/fold_results.yaml", device)

# 2. Essential TTA ë°ì´í„°ë¡œë” ìƒì„±
tta_loader = create_essential_tta_dataloader(
    sample_csv="../data/raw/sample_submission.csv",
    test_dir="../data/raw/test",
    img_size=384,
    batch_size=32
)

# 3. Essential TTA ì•™ìƒë¸” ì˜ˆì¸¡
ensemble_probs = ensemble_predict_with_essential_tta(models, tta_loader, cfg, device)

# 4. ìµœì¢… ì˜ˆì¸¡ ë° ì €ì¥
predictions = torch.argmax(ensemble_probs, dim=1).numpy()
```

ì£¼ìš” ê°œì„ ì‚¬í•­:
- âœ… íŒ€ì›ì˜ 5ê°€ì§€ Essential TTA êµ¬í˜„ (ì›ë³¸, 90Â°, 180Â°, 270Â°, ë°ê¸°ê°œì„ )
- âœ… ê¸°ì¡´ ë‹¨ìˆœ ë°˜ë³µ TTA ëŒ€ì‹  ë‹¤ì–‘í•œ ë³€í˜• ì ìš©
- âœ… ì•™ìƒë¸” + Essential TTA ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€
- âœ… ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„± ìœ ì§€ (predict_with_tta í•¨ìˆ˜ ë³´ì¡´)
"""


# ---------------------- ê³ ì„±ëŠ¥ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---------------------- #
# ê³ ì„±ëŠ¥ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
def run_highperf_inference(cfg_path: str, fold_results_path: str, output_path: Optional[str] = None):
    # ì„¤ì • ë¡œë“œ
    cfg = load_yaml(cfg_path)                                       # YAML ì„¤ì • íŒŒì¼ ë¡œë“œ
    cfg_dir = os.path.dirname(os.path.abspath(cfg_path))            # ì„¤ì • íŒŒì¼ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
    
    # ë¡œê±° ì„¤ì •
    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M')
    logger = Logger(
        log_path=create_log_path("infer", f"infer_highperf_{timestamp}.log")  # ë‚ ì§œë³„ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    )
    
    # íŒŒì´í”„ë¼ì¸ ì‹œì‘ ë¡œê·¸
    logger.write("[BOOT] high-performance inference pipeline started")
    
    try:
        # GPU/CPU ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.write(f"[BOOT] device={device}")                         # ë””ë°”ì´ìŠ¤ ì •ë³´ ë¡œê·¸
        
        # ê²½ë¡œ í™•ì¸
        sample_csv = resolve_path(cfg_dir, cfg["data"]["sample_csv"])   # ìƒ˜í”Œ CSV ê²½ë¡œ í•´ê²°
        test_dir = resolve_path(cfg_dir, cfg["data"]["image_dir_test"]) # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ ê²½ë¡œ í•´ê²°
        require_file(sample_csv, "sample_csv í™•ì¸")                     # ìƒ˜í”Œ CSV íŒŒì¼ ì¡´ì¬ì„± ê²€ì¦
        require_dir(test_dir, "test_dir í™•ì¸")                          # í…ŒìŠ¤íŠ¸ ë””ë ‰í„°ë¦¬ ì¡´ì¬ì„± ê²€ì¦
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_df = pd.read_csv(sample_csv)                               # í…ŒìŠ¤íŠ¸ ë°ì´í„° CSV ë¡œë“œ
        logger.write(f"[DATA] loaded test data | shape={test_df.shape}")# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë¡œê·¸
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
        test_ds = HighPerfDocClsDataset(
            test_df,                                 # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
            test_dir,                                # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬
            img_size=cfg["train"]["img_size"],       # ì´ë¯¸ì§€ í¬ê¸°
            is_train=False,                          # í‰ê°€ ëª¨ë“œ í”Œë˜ê·¸
            id_col=cfg["data"]["id_col"],            # ID ì»¬ëŸ¼ëª…
            target_col=None,                         # ì¶”ë¡  ëª¨ë“œ (íƒ€ê¹ƒ ì—†ìŒ)
            logger=logger                            # ë¡œê±° ê°ì²´
        )                                            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”
        test_loader = DataLoader(                    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œë” ìƒì„±
            test_ds,                                 # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
            batch_size=cfg["train"]["batch_size"],   # ë°°ì¹˜ í¬ê¸°
            shuffle=False,                           # ì…”í”Œ ë¹„í™œì„±í™” (ì¶”ë¡ ìš©)
            num_workers=cfg["project"]["num_workers"],  # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
            pin_memory=True                          # ë©”ëª¨ë¦¬ ê³ ì • í™œì„±í™”
        )
        
        logger.write(f"[DATA] test dataset size: {len(test_ds)}")  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í¬ê¸° ë¡œê·¸
        
        # TTA ì„¤ì • í™•ì¸
        tta_enabled = cfg.get("inference", {}).get("tta", True)
        tta_type = cfg.get("inference", {}).get("tta_type", "essential")
        logger.write(f"[TTA] TTA enabled: {tta_enabled}, type: {tta_type}")
        
        # í´ë“œë³„ ëª¨ë¸ ë¡œë“œ
        models = load_fold_models(fold_results_path, device)
        logger.write(f"[MODELS] loaded {len(models)} fold models")
        
        if tta_enabled:
            # ì„¤ì • ê°€ëŠ¥í•œ TTA ë°ì´í„°ë¡œë” ìƒì„±
            tta_loader = create_configurable_tta_dataloader(
                sample_csv=sample_csv,
                test_dir=test_dir,
                img_size=cfg["train"]["img_size"],
                tta_type=tta_type,
                batch_size=cfg["train"]["batch_size"],
                num_workers=cfg["project"]["num_workers"]
            )
            
            # TTA ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰
            logger.write(f"[INFERENCE] starting {tta_type} TTA ensemble prediction...")
            ensemble_preds = ensemble_predict_with_essential_tta(models, tta_loader, cfg, device)
        else:
            # ê¸°ë³¸ ì•™ìƒë¸” ì˜ˆì¸¡ (TTA ì—†ìŒ)
            logger.write(f"[INFERENCE] starting basic ensemble prediction (no TTA)...")
            ensemble_preds = ensemble_predict(models, test_loader, cfg, device, use_tta=False)
        
        # ìµœì¢… ì˜ˆì¸¡ í´ë˜ìŠ¤
        final_predictions = ensemble_preds.argmax(dim=1).numpy()    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_scores = ensemble_preds.max(dim=1)[0].numpy()    # ìµœëŒ€ í™•ë¥ ê°’ì„ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
        
        #-------------- ê²°ê³¼ ì €ì¥ ë° ë¡œê·¸ ---------------------- #
        # ì¶œë ¥ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ë™ì  íŒŒì¼ëª… ìƒì„±
        if output_path is None:
            current_date = pd.Timestamp.now().strftime('%Y%m%d')
            current_time = pd.Timestamp.now().strftime('%H%M')
            model_name = cfg["model"]["name"]
            
            # TTA íƒ€ì… í¬í•¨í•œ íŒŒì¼ëª… ìƒì„±
            tta_suffix = f"_{tta_type}_tta" if tta_enabled else "_no_tta"
            
            filename = f"{current_date}_{current_time}_{model_name}_ensemble{tta_suffix}.csv"
            output_path = f"submissions/{current_date}/{filename}"
        
        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission = test_df.copy()                                             # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
        submission[cfg["data"]["target_col"]] = final_predictions               # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
        submission.to_csv(output_path, index=False)                             # CSV íŒŒì¼ë¡œ ì €ì¥
        
        logger.write(f"[SUCCESS] Inference completed | output: {output_path}")  # ì¶”ë¡  ì™„ë£Œ ë¡œê·¸
        logger.write(f"[RESULT] Prediction distribution:")                      # ì˜ˆì¸¡ ë¶„í¬ ë¡œê·¸ ì‹œì‘
        
        # ê° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ìˆ˜ ê³„ì‚°
        for i, count in enumerate(np.bincount(final_predictions)):
            # í´ë˜ìŠ¤ë³„ ë¶„í¬ ë¡œê·¸
            logger.write(f"  Class {i}: {count} samples ({count/len(final_predictions)*100:.1f}%)")
        
        #-------------- ì¶”ë¡  ê²°ê³¼ ì‹œê°í™” ---------------------- #
        try:
            # ì‹œê°í™”ë¥¼ ìœ„í•œ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì„¤ì •
            viz_output_dir = os.path.dirname(output_path)
            model_name = cfg["model"]["name"]
            
            # ì‹œê°í™” ìƒì„±
            visualize_inference_pipeline(
                predictions=ensemble_preds.numpy(),
                model_name=model_name,
                output_dir=viz_output_dir,
                confidence_scores=confidence_scores
            )
            logger.write(f"[VIZ] Inference visualizations created in {viz_output_dir}")
            
        except Exception as viz_error:
            logger.write(f"[WARNING] Visualization failed: {str(viz_error)}")
        
        #-------------- lastest-infer í´ë”ì— ê²°ê³¼ ì €ì¥ ---------------------- #
        try:
            import shutil
            import time
            
            # experiments/infer/ë‚ ì§œ/ì‹¤í—˜ëª…/ êµ¬ì¡° ìƒì„±
            date_str = time.strftime('%Y%m%d')
            timestamp = time.strftime('%Y%m%d_%H%M')
            run_name = cfg.get("project", {}).get("run_name", "inference")
            
            # ë‚ ì§œë³„ infer ê²°ê³¼ ë””ë ‰í„°ë¦¬
            infer_output_dir = f"experiments/infer/{date_str}/{timestamp}_{run_name}"
            os.makedirs(infer_output_dir, exist_ok=True)
            
            # lastest-infer í´ë”ì— ì§ì ‘ ì €ì¥ (ê¸°ì¡´ ë‚´ìš© ì‚­ì œ í›„)
            lastest_infer_dir = "experiments/infer/lastest-infer"
            
            # ê¸°ì¡´ lastest-infer í´ë” ì‚­ì œ (ì™„ì „ êµì²´)
            if os.path.exists(lastest_infer_dir):
                shutil.rmtree(lastest_infer_dir)
                logger.write(f"[CLEANUP] Removed existing lastest-infer folder")
            
            os.makedirs(lastest_infer_dir, exist_ok=True)
            
            # ì¶”ë¡  ê²°ê³¼ CSVë¥¼ lastest-inferì— ë³µì‚¬
            import copy
            lastest_output_path = os.path.join(lastest_infer_dir, f"submission_{timestamp}.csv")
            shutil.copy2(output_path, lastest_output_path)
            
            # ì„¤ì • íŒŒì¼ë„ ë³µì‚¬
            import yaml
            config_copy_path = os.path.join(lastest_infer_dir, "config.yaml")
            with open(config_copy_path, 'w') as f:
                yaml.dump(cfg, f, default_flow_style=False, allow_unicode=True)
            
            # ì‹œê°í™” ê²°ê³¼ë„ ë³µì‚¬ (ìˆë‹¤ë©´)
            viz_source_dir = os.path.dirname(output_path)
            if os.path.exists(os.path.join(viz_source_dir, "images")):
                shutil.copytree(os.path.join(viz_source_dir, "images"), 
                               os.path.join(lastest_infer_dir, "images"))
            
            logger.write(f"[COPY] Results copied directly to lastest-infer")
            logger.write(f"ğŸ“ Latest inference results: {lastest_infer_dir}")
            
        except Exception as copy_error:
            logger.write(f"[WARNING] Failed to copy to lastest-infer: {str(copy_error)}")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
        return output_path
        
    # ì˜ˆì™¸ ë°œìƒ ì‹œ
    except Exception as e:
        logger.write(f"[ERROR] Inference failed: {str(e)}") # ì—ëŸ¬ ë¡œê·¸
        raise                                               # ì˜ˆì™¸ ì¬ë°œìƒ
    # ìµœì¢…ì ìœ¼ë¡œ ì‹¤í–‰
    finally:
        logger.write("[SHUTDOWN] Inference pipeline ended") # íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ë¡œê·¸


# ---------------------- ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---------------------- #
if __name__ == "__main__":
    import sys  # sys ëª¨ë“ˆ import
    
    # ì¸ì ê°œìˆ˜ í™•ì¸
    if len(sys.argv) < 3:
        print("Usage: python infer_highperf.py <config_path> <fold_results_path> [output_path]")    # ì‚¬ìš©ë²• ì¶œë ¥
        sys.exit(1)                                                                                 # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    
    cfg_path = sys.argv[1]                                                          # ì„¤ì • íŒŒì¼ ê²½ë¡œ
    fold_results_path = sys.argv[2]                                                 # í´ë“œ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
    output_path = sys.argv[3] if len(sys.argv) > 3 else None                        # ì¶œë ¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    
    result_path = run_highperf_inference(cfg_path, fold_results_path, output_path)  # ê³ ì„±ëŠ¥ ì¶”ë¡  ì‹¤í–‰
    print(f"Inference completed! Results saved to: {result_path}")                  # ì¶”ë¡  ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
    
    run_highperf_inference(cfg_path, fold_results_path, output_path)                # ê³ ì„±ëŠ¥ ì¶”ë¡  ì‹¤í–‰
