# ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import os, time, numpy as np, torch, pandas as pd         # ê²½ë¡œ/ì‹œê°„/ìˆ˜ì¹˜ê³„ì‚°/ë”¥ëŸ¬ë‹/ë°ì´í„° ì²˜ë¦¬
from torch.utils.data import DataLoader                  # PyTorch DataLoader
from tqdm import tqdm                                    # ì§„í–‰ë¥  í‘œì‹œ
import torchvision.transforms.functional as TF           # torchvision ì´ë¯¸ì§€ ë³€í™˜ í•¨ìˆ˜
import PIL.Image as Image                                # ì´ë¯¸ì§€ ì²˜ë¦¬

# í”„ë¡œì íŠ¸ ë‚´ë¶€ ìœ í‹¸ import
from src.logging.logger import Logger                      # ë¡œê·¸ ê¸°ë¡ ìœ í‹¸
from src.utils import load_yaml, ensure_dir, resolve_path, require_file, require_dir  # í•µì‹¬ ìœ í‹¸
from src.data.dataset import DocClsDataset               # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
from src.data.transforms import build_valid_tfms, get_tta_transforms_by_type         # ê²€ì¦ìš© ë³€í™˜ íŒŒì´í”„ë¼ì¸, TTA ë³€í™˜
from src.models.build import build_model                 # ëª¨ë¸ ë¹Œë“œ í•¨ìˆ˜

# ---------------------- ë¡œê±° ìƒì„± í•¨ìˆ˜ ---------------------- #
def _make_logger(cfg):
    logs_dir = ensure_dir(cfg["output"]["logs_dir"])     # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± ë³´ì¥
    log_name = f"infer_{time.strftime('%Y%m%d-%H%M')}_{cfg['project']['run_name']}.log"
    log_path = os.path.join(logs_dir, log_name)  # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    logger = Logger(log_path)                            # Logger ê°ì²´ ìƒì„±
    logger.start_redirect()                              # í‘œì¤€ ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰íŠ¸
    if hasattr(logger, "tqdm_redirect"):                 # tqdm ë¦¬ë‹¤ì´ë ‰íŠ¸ ì§€ì› ì—¬ë¶€ í™•ì¸
        logger.tqdm_redirect()
    logger.write(f">> Inference logger: {log_path}")     # ë¡œê±° ì‹œì‘ ë¡œê·¸ ì¶œë ¥
    return logger                                        # ë¡œê±° ë°˜í™˜

# ---------------------- í…ì„œ íšŒì „(TTAìš©) ---------------------- #
def _rotate_tensor(x, deg):
    imgs = []                                            # íšŒì „ëœ ì´ë¯¸ì§€ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    for i in range(x.size(0)):                           # ë°°ì¹˜ ë‚´ ê° ì´ë¯¸ì§€ ë°˜ë³µ
        pil = TF.to_pil_image(x[i].cpu())                # í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        pil = pil.rotate(deg, resample=Image.BILINEAR)   # ì§€ì • ê°ë„ë¡œ íšŒì „
        imgs.append(TF.to_tensor(pil).to(x.device))      # ë‹¤ì‹œ í…ì„œë¡œ ë³€í™˜ í›„ ë””ë°”ì´ìŠ¤ì— ì˜¬ë¦¼
    return torch.stack(imgs, 0)                          # ë°°ì¹˜ í…ì„œë¡œ ê²°í•©í•˜ì—¬ ë°˜í™˜

# ---------------------- ì¶”ë¡  ì‹¤í–‰ í•¨ìˆ˜ ---------------------- #
@torch.no_grad()                                         # ì¶”ë¡  ì¤‘ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
def run_inference(cfg_path: str, out: str|None=None, ckpt: str|None=None):
    cfg = load_yaml(cfg_path)                            # YAML ì„¤ì • ë¡œë“œ
    cfg_dir = os.path.dirname(os.path.abspath(cfg_path)) # ì„¤ì • íŒŒì¼ ë””ë ‰í† ë¦¬ ì ˆëŒ€ê²½ë¡œ
    logger = _make_logger(cfg)                           # ë¡œê±° ìƒì„±

    logger.write("[BOOT] inference pipeline started")    # íŒŒì´í”„ë¼ì¸ ì‹œì‘ ë¡œê·¸
    exit_status = "SUCCESS"                              # ì¢…ë£Œ ìƒíƒœ ê¸°ë³¸ê°’
    exit_code = 0                                        # ì¢…ë£Œ ì½”ë“œ ê¸°ë³¸ê°’
    
    try:
        # ---------------------- ê²½ë¡œ í•´ì„ ---------------------- #
        # ìƒ˜í”Œ CSV ì ˆëŒ€ê²½ë¡œ
        sample_csv = resolve_path(cfg_dir, cfg["data"]["sample_csv"])
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ
        image_dir  = resolve_path(cfg_dir, cfg["data"].get("image_dir_test", cfg["data"].get("image_dir", "data/raw/test")))
        require_file(sample_csv, "data.sample_csv í™•ì¸")    # CSV íŒŒì¼ ì¡´ì¬ í™•ì¸
        require_dir(image_dir,  "data.image_dir_test í™•ì¸") # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        # ê²½ë¡œ ë¡œê·¸
        logger.write(f"[PATH] OK | sample_csv={sample_csv} | image_dir_test={image_dir}")

        # ---------------------- ì„¤ì • ì¶œë ¥ ---------------------- #
        logger.write(f"[CFG] data={cfg['data']}")           # ë°ì´í„° ì„¤ì • ë¡œê·¸
        logger.write(f"[CFG] model={cfg['model']}")         # ëª¨ë¸ ì„¤ì • ë¡œê·¸
        logger.write(f"[CFG] inference={cfg['inference']}") # ì¶”ë¡  ì„¤ì • ë¡œê·¸

        # ---------------------- ë°ì´í„° ì¤€ë¹„ ---------------------- #
        # ì œì¶œìš© ìƒ˜í”Œ CSV ë¡œë“œ
        df_sub  = pd.read_csv(sample_csv)                

        # í…ŒìŠ¤íŠ¸ ID ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•˜ì—¬ ë³„ë„ DataFrame ìƒì„±
        test_df = df_sub[[cfg["data"]["id_col"]]].copy() 

        # ë°ì´í„° í¬ê¸°ì™€ ìƒìœ„ 3ê°œ ìƒ˜í”Œ ë¡œê·¸ ê¸°ë¡
        logger.write(
            f"[DATA] test size={len(test_df)} | head={test_df.head(3).to_dict(orient='records')}"
        )  

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹(DocClsDataset) ìƒì„±
        ds = DocClsDataset(                              
            test_df,                                     # ì¶”ì¶œëœ í…ŒìŠ¤íŠ¸ ID DataFrame
            image_dir,                                   # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬
            cfg["data"]["image_ext"],                    # ì´ë¯¸ì§€ í™•ì¥ì ì„¤ì •
            cfg["data"]["id_col"],                       # ID ì»¬ëŸ¼ëª…
            target_col=None,                             # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì´ë¯€ë¡œ ë¼ë²¨ ì—†ìŒ
            transform=build_valid_tfms(cfg["train"]["img_size"])  # ê²€ì¦ìš© ë³€í™˜ íŒŒì´í”„ë¼ì¸
        )

        # DataLoader ìƒì„± (ë°°ì¹˜ ë‹¨ìœ„ ë¡œë”©)
        ld = DataLoader(
            ds,                                          # ìœ„ì—ì„œ ì •ì˜í•œ Dataset
            batch_size=cfg["train"]["batch_size"],       # ë°°ì¹˜ í¬ê¸°
            shuffle=False,                               # ìˆœì„œ ìœ ì§€ (shuffle ë¹„í™œì„±)
            num_workers=cfg["project"]["num_workers"],   # ë©€í‹°í”„ë¡œì„¸ìŠ¤ ë¡œë”©
            pin_memory=True                              # CUDA ì „ì†¡ ìµœì í™”
        )

        # DataLoader ìƒíƒœ ë¡œê·¸ ê¸°ë¡ (ìŠ¤í… ìˆ˜, ë°°ì¹˜ í¬ê¸°)
        logger.write(
            f"[DATA] dataloader built | steps={len(ld)} bs={cfg['train']['batch_size']}"
        )

        # ---------------------- ëª¨ë¸ ì¤€ë¹„ ---------------------- #
        # ë””ë°”ì´ìŠ¤ ì„ íƒ
        device = "cuda" if (cfg["project"]["device"]=="cuda" and torch.cuda.is_available()) else "cpu"
        # ëª¨ë¸ ë¹Œë“œ í›„ eval ëª¨ë“œ
        model = build_model(cfg["model"]["name"], cfg["data"]["num_classes"], cfg["model"]["pretrained"]).to(device).eval()

        # ---------------------- ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ---------------------- #
        # ckpt ì¸ìê°€ ì§ì ‘ ì§€ì •ëœ ê²½ìš°
        if ckpt:
            # ì‚¬ìš©ì ì…ë ¥ ckpt ê²½ë¡œë¥¼ config ê¸°ì¤€ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
            ckpt_path = resolve_path(cfg_dir, ckpt)

        # ckpt ì¸ìê°€ ì—†ëŠ” ê²½ìš° - config íŒŒì¼ì˜ ckpt.path í™•ì¸
        elif "ckpt" in cfg and "path" in cfg["ckpt"]:
            # config íŒŒì¼ì˜ ckpt.path ì„¤ì • ì‚¬ìš©
            ckpt_path = resolve_path(cfg_dir, cfg["ckpt"]["path"])
            logger.write(f"[CKPT] Using config ckpt.path: {cfg['ckpt']['path']}")

        # configì—ë„ ckpt.pathê°€ ì—†ëŠ” ê²½ìš° (ê¸°ë³¸ best_fold0.pth ì‚¬ìš©)
        else:
            # í•™ìŠµ ê²°ê³¼ ë””ë ‰í„°ë¦¬ íŒ¨í„´ ê²€ìƒ‰
            import glob
            day = time.strftime(cfg["project"]["date_format"])      # ë‚ ì§œ ë¬¸ìì—´
            run_name = cfg['project']['run_name']                   # ì‹¤í–‰ ì´ë¦„
            
            # íŒ¨í„´: exp_dir/ë‚ ì§œ/run_name_ë‚ ì§œ_ì‹œê°„/ckpt/best_fold0.pth
            pattern = resolve_path(cfg_dir, os.path.join(
                cfg["output"]["exp_dir"],                           # ì‹¤í—˜ ê²°ê³¼ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬
                day,                                                # ë‚ ì§œ í´ë”
                f"{run_name}_{day}_*",                             # run_name_ë‚ ì§œ_ì‹œê°„ íŒ¨í„´
                "ckpt",                                             # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í„°ë¦¬
                "best_fold0.pth"                                    # ê¸°ë³¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…
            ))
            
            # íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ ê²€ìƒ‰
            matching_files = glob.glob(pattern)
            if matching_files:
                # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ (ì‹œê°„ìˆœ ì •ë ¬)
                ckpt_path = sorted(matching_files)[-1]
            else:
                # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
                ckpt_path = resolve_path(cfg_dir, os.path.join(
                    cfg["output"]["exp_dir"],                       # ì‹¤í—˜ ê²°ê³¼ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬
                    day,                                            # ë‚ ì§œ í˜•ì‹ì— ë§ì¶˜ í•˜ìœ„ í´ë”
                    f"{run_name}",                                  # ì‹¤í–‰ ì´ë¦„(run_name) í´ë” (ê¸°ì¡´ ë°©ì‹)
                    "ckpt",                                         # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í„°ë¦¬
                    "best_fold0.pth"                                # ê¸°ë³¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ëª…
                ))
            
        # ckpt ì¡´ì¬ í™•ì¸
        require_file(ckpt_path, "--ckptë¡œ ì§ì ‘ ì§€ì •í•˜ê±°ë‚˜ í•™ìŠµ ê²°ê³¼ ê²½ë¡œ í™•ì¸")
        state = torch.load(ckpt_path, map_location=device)          # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        
        # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        if "model" in state:
            model.load_state_dict(state["model"], strict=True)      # êµ¬í˜• ì²´í¬í¬ì¸íŠ¸ í˜•ì‹
        elif "model_state_dict" in state:
            model.load_state_dict(state["model_state_dict"], strict=True)  # ì‹ í˜• ì²´í¬í¬ì¸íŠ¸ í˜•ì‹
        else:
            model.load_state_dict(state, strict=True)               # state_dict ì§ì ‘ ì €ì¥ëœ ê²½ìš°
        logger.write(f"[CKPT] loaded: {ckpt_path}")                 # ë¡œë“œ ë¡œê·¸

        # ---------------------- TTA ì„¤ì • ---------------------- #
        # Configurable TTA ì‚¬ìš© ì—¬ë¶€ ì²´í¬
        use_configurable_tta = cfg["inference"].get("tta_type") is not None
        
        if use_configurable_tta:
            # ìƒˆë¡œìš´ configurable TTA ì‹œìŠ¤í…œ ì‚¬ìš©
            tta_type = cfg["inference"].get("tta_type", "essential")
            tta_transforms = get_tta_transforms_by_type(tta_type, cfg["train"]["img_size"])
            logger.write(f"[TTA] configurable mode: {tta_type} ({len(tta_transforms)} transforms)")
        else:
            # ê¸°ì¡´ íšŒì „ ê¸°ë°˜ TTA ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
            degs = cfg["inference"]["tta_rot_degrees"] if cfg["inference"]["tta"] else [0]
            logger.write(f"[TTA] legacy rotation mode: degs={degs}")

        # ---------------------- ì¶”ë¡  ë£¨í”„ ---------------------- #
        logits_all = []                     # ì „ì²´ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        logger.write("[INFER] >>> start")   # ì¶”ë¡  ì‹œì‘ ë¡œê·¸
        
        # DataLoader ë°˜ë³µ
        for step, (imgs, ids) in enumerate(tqdm(ld, desc="infer"), 1):
            imgs = imgs.to(device)  # ì´ë¯¸ì§€ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            probs_accum = None      # ëˆ„ì  í™•ë¥  ì´ˆê¸°í™”
            
            if use_configurable_tta:
                # Configurable TTA ì‚¬ìš©
                for transform_func in tta_transforms:
                    # ê° TTA ë³€í™˜ ì ìš©
                    x_list = []
                    for i in range(imgs.size(0)):
                        img_pil = TF.to_pil_image(imgs[i].cpu())
                        transformed = transform_func(image=np.array(img_pil))["image"]
                        if isinstance(transformed, np.ndarray):
                            transformed = TF.to_tensor(transformed)
                        x_list.append(transformed.to(device))
                    
                    x = torch.stack(x_list, 0)
                    logits = model(x)                               # ëª¨ë¸ ì¶”ë¡ 
                    probs  = torch.softmax(logits, dim=1)           # í™•ë¥  ë³€í™˜
                    # í™•ë¥  ëˆ„ì 
                    probs_accum = probs if probs_accum is None else probs_accum + probs
                    
                # í‰ê·  ê³„ì‚°
                if probs_accum is not None:
                    probs = (probs_accum / len(tta_transforms)).cpu().numpy()
                else:
                    probs = torch.zeros((imgs.size(0), cfg["data"]["num_classes"])).cpu().numpy()
            else:
                # ê¸°ì¡´ íšŒì „ ê¸°ë°˜ TTA ì‚¬ìš©
                for d in degs:
                    x = imgs if d==0 else _rotate_tensor(imgs, d)   # íšŒì „ ì ìš©
                    logits = model(x)                               # ëª¨ë¸ ì¶”ë¡ 
                    probs  = torch.softmax(logits, dim=1)           # í™•ë¥  ë³€í™˜
                    # í™•ë¥  ëˆ„ì 
                    probs_accum = probs if probs_accum is None else probs_accum + probs
                    
                # í‰ê·  ê³„ì‚°
                if probs_accum is not None:
                    probs = (probs_accum / len(degs)).cpu().numpy()     # í‰ê·  í™•ë¥  ê³„ì‚°
                else:
                    # ë°±ì—…: probs_accumì´ Noneì¸ ê²½ìš° (ì—ëŸ¬ ë°©ì§€)
                    probs = torch.zeros((imgs.size(0), cfg["data"]["num_classes"])).cpu().numpy()
                    
            logits_all.append(probs)                            # ê²°ê³¼ ì €ì¥
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ë¡œê·¸
            if step == 1 or (step % 20) == 0 or step == len(ld):
                # í˜„ì¬ ì§„í–‰ ìƒí™© ë¡œê·¸
                logger.write(f"[INFER] step {step}/{len(ld)} processed")

        probs = np.concatenate(logits_all, axis=0)       # ê²°ê³¼ ê²°í•©
        preds = probs.argmax(axis=1)                     # ì˜ˆì¸¡ í´ë˜ìŠ¤ ì‚°ì¶œ

        # ---------------------- ê²°ê³¼ ì €ì¥ ---------------------- #
        # ë™ì  íŒŒì¼ëª… ìƒì„± (ë‚ ì§œ_ëª¨ë¸ëª… í˜•ì‹)
        if out is None:
            current_date = pd.Timestamp.now().strftime('%Y%m%d')
            current_time = pd.Timestamp.now().strftime('%H%M')
            model_name = cfg["model"]["name"]
            tta_suffix = "_tta" if cfg.get("inference", {}).get("tta", False) else ""
            
            # ì¦ê°• íƒ€ì… ê²°ì • (í•™ìŠµ ì„¤ì •ê³¼ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©)
            aug_type = "advanced_augmentation" if cfg["train"].get("use_advanced_augmentation", False) else "basic_augmentation"
            
            filename = f"{current_date}_{current_time}_{model_name}{tta_suffix}_{aug_type}.csv"
            out_path = f"submissions/{current_date}/{filename}"
        else:
            out_path = resolve_path(cfg_dir, out)
            
        # ë””ë ‰í† ë¦¬ ë³´ì¥
        ensure_dir(os.path.dirname(out_path))
        
        # ì œì¶œ DataFrame ìƒì„±
        sub = pd.DataFrame({
            cfg["data"]["id_col"]: test_df[cfg["data"]["id_col"]].values,   # ID ì—´
            cfg["data"]["target_col"]: preds                                # ì˜ˆì¸¡ ì—´
        })
        
        sub.to_csv(out_path, index=False)   # CSV ì €ì¥
        logger.write(f"[OUT] submission saved: {out_path} | shape={sub.shape}") # CSV ì €ì¥ ë¡œê·¸

        #-------------- lastest-infer í´ë”ì— ê²°ê³¼ ì €ì¥ ---------------------- #
        try:
            import shutil
            import time
            
            # experiments/infer/ë‚ ì§œ/ì‹¤í—˜ëª…/ êµ¬ì¡° ìƒì„±
            date_str = time.strftime('%Y%m%d')
            timestamp = time.strftime('%Y%m%d_%H%M')
            run_name = cfg.get("project", {}).get("run_name", "inference")
            
            # lastest-infer í´ë”ì— ì§ì ‘ ì €ì¥ (ê¸°ì¡´ ë‚´ìš© ì‚­ì œ í›„)
            lastest_infer_dir = "experiments/infer/lastest-infer"
            
            # ê¸°ì¡´ lastest-infer í´ë” ì‚­ì œ (ì™„ì „ êµì²´)
            if os.path.exists(lastest_infer_dir):
                shutil.rmtree(lastest_infer_dir)
                logger.write(f"[CLEANUP] Removed existing lastest-infer folder")
            
            os.makedirs(lastest_infer_dir, exist_ok=True)
            
            # ì¶”ë¡  ê²°ê³¼ CSVë¥¼ lastest-inferì— ë³µì‚¬
            lastest_output_path = os.path.join(lastest_infer_dir, f"submission_{timestamp}.csv")
            shutil.copy2(out_path, lastest_output_path)
            
            # ì„¤ì • íŒŒì¼ë„ ë³µì‚¬
            import yaml
            config_copy_path = os.path.join(lastest_infer_dir, "config.yaml")
            with open(config_copy_path, 'w') as f:
                yaml.dump(cfg, f, default_flow_style=False, allow_unicode=True)
            
            logger.write(f"[COPY] Results copied directly to lastest-infer")
            logger.write(f"ğŸ“ Latest inference results: {lastest_infer_dir}")
            
        except Exception as copy_error:
            logger.write(f"[WARNING] Failed to copy to lastest-infer: {str(copy_error)}")

        # ì¶”ë¡  ì™„ë£Œ ë¡œê·¸
        logger.write("[INFER] <<< finished successfully")

    except Exception as e:                                                  # ì˜ˆì™¸ ì²˜ë¦¬
        exit_status = "ERROR"                                               # ìƒíƒœ ERROR
        exit_code = 1                                                       # ì¢…ë£Œ ì½”ë“œ 1
        logger.write(f"[ERROR] {type(e).__name__}: {e}", print_error=True)  # ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡
        raise                                                               # ì˜ˆì™¸ ì¬ë°œìƒ
    finally:
        logger.write(f"[EXIT] INFERENCE {exit_status} code={exit_code}")    # ì¢…ë£Œ ìƒíƒœ ë¡œê·¸
        logger.write(">> Stopping logger and restoring stdio")              # ë¡œê±° ì¢…ë£Œ ë¡œê·¸
        logger.stop_redirect()                                              # ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰íŠ¸ í•´ì œ
        logger.close()                                                      # ë¡œê±° ë‹«ê¸°