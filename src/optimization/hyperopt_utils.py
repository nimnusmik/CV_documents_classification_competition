# src/optimization/hyperopt_utils.py
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìœ í‹¸ë¦¬í‹°

Optuna ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì • ë° ë„ìš°ë¯¸ í•¨ìˆ˜ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

from dataclasses import dataclass
from typing import Dict, Any, List, Union
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler


@dataclass
class OptimizationConfig:
    """ìµœì í™” ì„¤ì • í´ë˜ìŠ¤"""
    n_trials: int = 10                          # ì‹œë„ íšŸìˆ˜
    timeout: int = 3600                         # ìµœëŒ€ ì‹œê°„ (ì´ˆ)
    study_name: str = "doc-classification"      # ìŠ¤í„°ë”” ì´ë¦„
    direction: str = "maximize"                 # ìµœì í™” ë°©í–¥ (maximize/minimize)
    pruner_patience: int = 5                    # ì¡°ê¸° ì¤‘ë‹¨ patience
    sampler_n_startup_trials: int = 10          # ì´ˆê¸° ëœë¤ íƒìƒ‰ íšŸìˆ˜
    
    # íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„
    lr_range: List[float] = None                # í•™ìŠµë¥  ë²”ìœ„ [min, max]
    batch_size_choices: List[int] = None        # ë°°ì¹˜ í¬ê¸° ì„ íƒì§€
    weight_decay_range: List[float] = None      # weight decay ë²”ìœ„ [min, max]
    dropout_range: List[float] = None           # dropout ë²”ìœ„ [min, max]
    
    def __post_init__(self):
        # ê¸°ë³¸ê°’ ì„¤ì •
        if self.lr_range is None:
            self.lr_range = [1e-5, 1e-2]
        if self.batch_size_choices is None:
            self.batch_size_choices = [16, 32, 64, 128]
        if self.weight_decay_range is None:
            self.weight_decay_range = [0.0, 0.1]
        if self.dropout_range is None:
            self.dropout_range = [0.0, 0.3]


def create_search_space(trial: optuna.Trial, config: OptimizationConfig) -> Dict[str, Any]:
    """
    Optuna trialì„ ì‚¬ìš©í•´ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„ì„ ìƒì„±
    
    Args:
        trial: Optuna trial ê°ì²´
        config: ìµœì í™” ì„¤ì •
        
    Returns:
        ìƒ˜í”Œë§ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
    """
    params = {}
    
    # í•™ìŠµë¥  (ë¡œê·¸ ìŠ¤ì¼€ì¼)
    params['lr'] = trial.suggest_float(
        'lr', 
        config.lr_range[0], 
        config.lr_range[1],
        log=True
    )
    
    # ë°°ì¹˜ í¬ê¸° (ì¹´í…Œê³ ë¦¬)
    params['batch_size'] = trial.suggest_categorical(
        'batch_size', 
        config.batch_size_choices
    )
    
    # Weight decay (ê· ë“± ë¶„í¬)
    params['weight_decay'] = trial.suggest_float(
        'weight_decay',
        config.weight_decay_range[0],
        config.weight_decay_range[1]
    )
    
    # Dropout (ê· ë“± ë¶„í¬) 
    params['dropout'] = trial.suggest_float(
        'dropout',
        config.dropout_range[0], 
        config.dropout_range[1]
    )
    
    return params


def create_study(config: OptimizationConfig) -> optuna.Study:
    """
    Optuna Study ê°ì²´ ìƒì„±
    
    Args:
        config: ìµœì í™” ì„¤ì •
        
    Returns:
        ì„¤ì •ëœ optuna.Study ê°ì²´
    """
    # Pruner ì„¤ì • (ì„±ëŠ¥ì´ ì•ˆ ì¢‹ì€ trial ì¡°ê¸° ì¤‘ë‹¨)
    pruner = MedianPruner(
        n_startup_trials=config.sampler_n_startup_trials,
        n_warmup_steps=config.pruner_patience
    )
    
    # Sampler ì„¤ì • (TPE: Tree-structured Parzen Estimator)
    sampler = TPESampler(
        n_startup_trials=config.sampler_n_startup_trials
    )
    
    # Study ìƒì„±
    study = optuna.create_study(
        study_name=config.study_name,
        direction=config.direction,
        pruner=pruner,
        sampler=sampler
    )
    
    return study


def print_optimization_summary(study: optuna.Study) -> None:
    """
    ìµœì í™” ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    
    Args:
        study: ì™„ë£Œëœ optuna.Study ê°ì²´
    """
    print("=" * 60)
    print("ğŸ¯ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ!")
    print("=" * 60)
    
    print(f"ğŸ“Š ì´ ì‹œë„ íšŸìˆ˜: {len(study.trials)}")
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥: {study.best_value:.4f}")
    print(f"âš™ï¸ ìµœì  íŒŒë¼ë¯¸í„°:")
    
    for key, value in study.best_params.items():
        if isinstance(value, float):
            print(f"   - {key}: {value:.6f}")
        else:
            print(f"   - {key}: {value}")
    
    print("=" * 60)


def update_config_with_best_params(config: Dict[str, Any], best_params: Dict[str, Any]) -> Dict[str, Any]:
    """
    ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì • ë”•ì…”ë„ˆë¦¬ ì—…ë°ì´íŠ¸
    
    Args:
        config: ì›ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        best_params: Optunaì—ì„œ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„°
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    updated_config = config.copy()
    
    # train ì„¹ì…˜ ì—…ë°ì´íŠ¸
    if 'train' not in updated_config:
        updated_config['train'] = {}
    
    # ìµœì  íŒŒë¼ë¯¸í„° ì ìš©
    for key, value in best_params.items():
        updated_config['train'][key] = value
    
    return updated_config
