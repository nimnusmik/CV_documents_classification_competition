# src/optimization/optuna_tuner.py
"""
Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹

ë² ì´ìŠ¤ë¼ì¸ ë…¸íŠ¸ë¶ì˜ Optuna ì½”ë“œë¥¼ ëª¨ë“ˆí™”í•˜ì—¬ ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import time
import copy
from typing import Dict, Any, Optional, Callable
import numpy as np
from sklearn.model_selection import StratifiedKFold

# Optuna import (ì„¤ì¹˜ í•„ìš”ì‹œ ìë™ ì„¤ì¹˜ ì•ˆë‚´)
try:
    import optuna
    from optuna.pruners import MedianPruner
    from optuna.samplers import TPESampler
except ImportError:
    print("âŒ Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ“¥ ì„¤ì¹˜ ëª…ë ¹ì–´: pip install optuna")
    print("ğŸ’¡ ë˜ëŠ” requirements.txtì— optuna ì¶”ê°€ í›„ pip install -r requirements.txt")
    raise

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from src.utils.common import load_yaml, dump_yaml, create_log_path
from src.logging.logger import Logger
# from src.training.train_highperf import run_fold_training  # ê°œë³„ í´ë“œ í•™ìŠµ í•¨ìˆ˜ (í–¥í›„ êµ¬í˜„)
from .hyperopt_utils import (
    OptimizationConfig, 
    create_search_space, 
    create_study, 
    print_optimization_summary,
    update_config_with_best_params
)

# ì‹œê°í™” ëª¨ë“ˆ import
from src.utils.visualizations import visualize_optimization_pipeline, create_organized_output_structure


class OptunaTrainer:
    """
    Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™” í´ë˜ìŠ¤
    """
    
    def __init__(self, config_path: str, optimization_config: OptimizationConfig):
        """
        ì´ˆê¸°í™”
        
        Args:
            config_path: ê¸°ë³¸ í•™ìŠµ ì„¤ì • íŒŒì¼ ê²½ë¡œ
            optimization_config: ìµœì í™” ì„¤ì •
        """
        self.config_path = config_path
        self.base_config = load_yaml(config_path)
        self.opt_config = optimization_config
        
        # ë¡œê±° ì„¤ì •
        timestamp = time.strftime("%Y%m%d_%H%M")
        log_path = create_log_path("optimization", f"optuna_{timestamp}.log")
        os.makedirs(os.path.dirname(log_path), exist_ok=True)
        self.logger = Logger(log_path=log_path)
        
        # Optuna study ìƒì„±
        self.study = create_study(optimization_config)
        
        self.logger.write("ğŸ” Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.write(f"ğŸ“‹ Base config: {config_path}")
        self.logger.write(f"ğŸ¯ Target trials: {optimization_config.n_trials}")
    
    def objective(self, trial: optuna.Trial) -> float:
        """
        Optuna objective í•¨ìˆ˜
        
        Args:
            trial: Optuna trial ê°ì²´
            
        Returns:
            ìµœì í™”í•  ë©”íŠ¸ë¦­ ê°’ (F1 score)
        """
        try:
            # 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
            sampled_params = create_search_space(trial, self.opt_config)
            
            self.logger.write(f"ğŸ”¬ Trial {trial.number}: {sampled_params}")
            
            # 2. ì„¤ì • ì—…ë°ì´íŠ¸
            trial_config = copy.deepcopy(self.base_config)
            trial_config['train'].update(sampled_params)
            
            # 3. ë¹ ë¥¸ ê²€ì¦ì„ ìœ„í•œ 3-fold CV
            f1_scores = self._quick_cross_validation(trial_config, trial)
            
            # 4. í‰ê·  F1 ì ìˆ˜ ê³„ì‚°
            mean_f1 = np.mean(f1_scores)
            
            self.logger.write(f"âœ… Trial {trial.number} ì™„ë£Œ: F1 {mean_f1:.4f} (fold scores: {f1_scores})")
            
            return mean_f1
            
        except Exception as e:
            self.logger.write(f"âŒ Trial {trial.number} ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨í•œ trialì€ ë‚®ì€ ì ìˆ˜ ë°˜í™˜
            return 0.0
    
    def _quick_cross_validation(self, config: Dict[str, Any], trial: optuna.Trial) -> list:
        """
        ë¹ ë¥¸ êµì°¨ ê²€ì¦ (3-fold, ì§§ì€ epoch)
        
        Args:
            config: í•™ìŠµ ì„¤ì •
            trial: Optuna trial (ì¡°ê¸° ì¤‘ë‹¨ìš©)
            
        Returns:
            ê° foldì˜ F1 ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        # ë¹ ë¥¸ ê²€ì¦ì„ ìœ„í•œ ì„¤ì • ì¡°ì •
        quick_config = copy.deepcopy(config)
        quick_config['train']['epochs'] = 3  # ì§§ì€ epoch
        quick_config['data']['folds'] = 3    # 3-foldë§Œ ì‚¬ìš©
        
        # CSV ë°ì´í„° ë¡œë“œ
        import pandas as pd
        train_df = pd.read_csv(config['data']['train_csv'])
        
        # Stratified K-Fold ì„¤ì •
        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        fold_scores = []
        
        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df[config['data']['target_col']])):
            # ì¡°ê¸° ì¤‘ë‹¨ ì²´í¬ (Optuna pruning)
            if trial.should_prune():
                raise optuna.TrialPruned()
            
            self.logger.write(f"  ğŸ“ Fold {fold_idx + 1}/3 ì‹œì‘...")
            
            # ê°œë³„ í´ë“œ í•™ìŠµ ì‹¤í–‰ (ë¹ ë¥¸ ë²„ì „)
            fold_f1 = self._train_single_fold(
                quick_config, 
                train_df.iloc[train_idx], 
                train_df.iloc[val_idx],
                fold_idx
            )
            
            fold_scores.append(fold_f1)
            
            # ì¤‘ê°„ ê²°ê³¼ ë³´ê³  (Optuna pruning íŒë‹¨ìš©)
            trial.report(fold_f1, fold_idx)
            
            self.logger.write(f"  âœ… Fold {fold_idx + 1}/3 ì™„ë£Œ: F1 {fold_f1:.4f}")
        
        return fold_scores
    
    def _train_single_fold(self, config: Dict[str, Any], train_df, val_df, fold_idx: int) -> float:
        """
        ë‹¨ì¼ í´ë“œ í•™ìŠµ (ë¹ ë¥¸ ê²€ì¦ìš©)
        
        Args:
            config: í•™ìŠµ ì„¤ì •
            train_df: í•™ìŠµ ë°ì´í„°í”„ë ˆì„
            val_df: ê²€ì¦ ë°ì´í„°í”„ë ˆì„
            fold_idx: í´ë“œ ì¸ë±ìŠ¤
            
        Returns:
            ê²€ì¦ F1 ì ìˆ˜
        """
        # TODO: ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” train_highperf.pyì˜ ê°œë³„ í´ë“œ í•™ìŠµ í•¨ìˆ˜ í˜¸ì¶œ
        # í˜„ì¬ëŠ” í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ëœë¤ ì ìˆ˜ ë°˜í™˜ (ë°ëª¨ìš©)
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œ:
        # from src.training.train_highperf import train_single_fold_quick
        # return train_single_fold_quick(config, train_df, val_df, fold_idx)
        
        # í”Œë ˆì´ìŠ¤í™€ë”: ì‹¤ì œ í•™ìŠµ ëŒ€ì‹  ì‹œë®¬ë ˆì´ì…˜
        import random
        time.sleep(1)  # í•™ìŠµ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ ê°€ìƒì˜ ì„±ëŠ¥ ê³„ì‚°
        lr = config['train']['lr']
        batch_size = config['train']['batch_size']
        
        # ê°„ë‹¨í•œ ì„±ëŠ¥ ì¶”ì • ê³µì‹ (ì‹¤ì œë¡œëŠ” ì§„ì§œ í•™ìŠµ ê²°ê³¼)
        base_score = 0.85
        lr_bonus = max(0, 0.05 - abs(lr - 0.0003) * 100)  # 0.0003 ê·¼ì²˜ì—ì„œ ìµœì 
        batch_bonus = 0.02 if batch_size == 64 else 0.0   # 64ê°€ ìµœì 
        noise = random.uniform(-0.02, 0.02)               # ëœë¤ ë…¸ì´ì¦ˆ
        
        simulated_f1 = base_score + lr_bonus + batch_bonus + noise
        return max(0.5, min(0.95, simulated_f1))  # 0.5~0.95 ë²”ìœ„ë¡œ ì œí•œ
    
    def optimize(self) -> Dict[str, Any]:
        """
        í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰
        
        Returns:
            ìµœì  íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        """
        self.logger.write("ğŸš€ Optuna ìµœì í™” ì‹œì‘...")
        start_time = time.time()
        
        try:
            # ìµœì í™” ì‹¤í–‰
            self.study.optimize(
                self.objective,
                n_trials=self.opt_config.n_trials,
                timeout=self.opt_config.timeout
            )
            
            # ê²°ê³¼ ì •ë¦¬
            elapsed_time = time.time() - start_time
            
            self.logger.write(f"â±ï¸ ìµœì í™” ì™„ë£Œ: {elapsed_time:.1f}ì´ˆ ì†Œìš”")
            print_optimization_summary(self.study)
            
            # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
            best_params_path = f"experiments/optimization/best_params_{time.strftime('%Y%m%d_%H%M')}.yaml"
            os.makedirs(os.path.dirname(best_params_path), exist_ok=True)
            
            best_config = update_config_with_best_params(self.base_config, self.study.best_params)
            dump_yaml(best_config, best_params_path)
            
            self.logger.write(f"ğŸ’¾ ìµœì  ì„¤ì • ì €ì¥: {best_params_path}")
            
            #-------------- ìµœì í™” ê²°ê³¼ ì‹œê°í™” ---------------------- #
            try:
                # ì‹œê°í™”ë¥¼ ìœ„í•œ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì„¤ì •
                viz_output_dir = os.path.dirname(best_params_path)
                model_name = self.base_config.get("model", {}).get("name", "unknown")
                
                # Study ê°ì²´ ì €ì¥ (ì‹œê°í™”ìš©)
                import pickle
                study_path = os.path.join(viz_output_dir, f"study_{time.strftime('%Y%m%d_%H%M')}.pkl")
                with open(study_path, 'wb') as f:
                    pickle.dump(self.study, f)
                
                # ì‹œê°í™” ìƒì„±
                visualize_optimization_pipeline(
                    study_path=study_path,
                    model_name=model_name,
                    output_dir=viz_output_dir,
                    trials_df=None
                )
                self.logger.write(f"[VIZ] Optimization visualizations created in {viz_output_dir}")
                
            except Exception as viz_error:
                self.logger.write(f"[WARNING] Visualization failed: {str(viz_error)}")
            
            return self.study.best_params
            
        except Exception as e:
            self.logger.write(f"âŒ ìµœì í™” ì‹¤íŒ¨: {str(e)}")
            raise


def run_hyperparameter_optimization(
    config_path: str,
    n_trials: int = 20,
    timeout: int = 3600,
    output_path: Optional[str] = None
) -> str:
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        n_trials: ì‹œë„ íšŸìˆ˜
        timeout: ìµœëŒ€ ì‹œê°„ (ì´ˆ)
        output_path: ìµœì  ì„¤ì • íŒŒì¼ ì €ì¥ ê²½ë¡œ
        
    Returns:
        ìµœì í™”ëœ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    """
    # ìµœì í™” ì„¤ì • ìƒì„±
    opt_config = OptimizationConfig(
        n_trials=n_trials,
        timeout=timeout
    )
    
    # Optuna íŠœë„ˆ ìƒì„± ë° ì‹¤í–‰
    tuner = OptunaTrainer(config_path, opt_config)
    best_params = tuner.optimize()
    
    # ìµœì  ì„¤ì •ìœ¼ë¡œ ìƒˆ ì„¤ì • íŒŒì¼ ìƒì„±
    if output_path is None:
        timestamp = time.strftime("%Y%m%d_%H%M")
        output_path = f"configs/train_optimized_{timestamp}.yaml"
    
    base_config = load_yaml(config_path)
    optimized_config = update_config_with_best_params(base_config, best_params)
    dump_yaml(optimized_config, output_path)
    
    print(f"ğŸ¯ ìµœì í™” ì™„ë£Œ! ìƒˆ ì„¤ì • íŒŒì¼: {output_path}")
    return output_path
