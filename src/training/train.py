# ------------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
import os, time, numpy as np, torch, torch.nn as nn, pandas as pd, psutil
import shutil                                                       # íŒŒì¼/í´ë” ë³µì‚¬ ìœ í‹¸
# os       : íŒŒì¼/ë””ë ‰í„°ë¦¬ ê²½ë¡œ, ì‹œìŠ¤í…œ ìœ í‹¸
# time     : ì‹œê°„ ì¸¡ì •, ë¡œê¹…
# numpy    : ìˆ˜ì¹˜ ê³„ì‚°, ë°°ì—´ ì—°ì‚°
# torch    : PyTorch ë©”ì¸ ëª¨ë“ˆ
# torch.nn : ì‹ ê²½ë§ ê³„ì¸µ/ì†ì‹¤ í•¨ìˆ˜ ëª¨ë“ˆ
# pandas   : ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
# psutil   : ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
# shutil   : íŒŒì¼/í´ë” ë³µì‚¬, ì´ë™

# ------------------------- PyTorch ìœ í‹¸ ------------------------- #
from torch.utils.data import DataLoader                             # ë°ì´í„° ë¡œë”
from sklearn.model_selection import StratifiedKFold                 # ê³„ì¸µì  K-í´ë“œ ë¶„í• 
from torch.cuda.amp import autocast, GradScaler                     # AMP (ìë™ í˜¼í•© ì •ë°€ë„) ì§€ì›
from torch.optim import Adam, AdamW                                 # ì˜µí‹°ë§ˆì´ì € (Adam, AdamW)
from torch.optim.lr_scheduler import CosineAnnealingLR              # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ì½”ì‚¬ì¸ ê°ì‡ )
from tqdm import tqdm                                # ì§„í–‰ë°” ì‹œê°í™”

# ------------------------- í”„ë¡œì íŠ¸ ìœ í‹¸ ------------------------- #
from src.utils.config import set_seed                               # ëœë¤ ì‹œë“œ ê³ ì •
from src.logging.logger import Logger                               # ë¡œê·¸ ê¸°ë¡ í´ë˜ìŠ¤
from src.utils.common import (                                      # ê³µí†µ ìœ í‹¸ í•¨ìˆ˜ë“¤
    load_yaml, ensure_dir, dump_yaml, jsonl_append, short_uid,
    resolve_path, require_file, require_dir, create_log_path
)

# ------------------------- ë°ì´í„°/ëª¨ë¸ ê´€ë ¨ ------------------------- #
from src.data.dataset import DocClsDataset                          # ë¬¸ì„œ ë¶„ë¥˜ Dataset í´ë˜ìŠ¤
from src.data.transforms import (                                    # í•™ìŠµ/ê²€ì¦ ë³€í™˜ í•¨ìˆ˜ë“¤
    build_train_tfms, build_valid_tfms, build_advanced_train_tfms   # ê¸°ë³¸/ê³ ê¸‰ ë³€í™˜ íŒŒì´í”„ë¼ì¸
)
from src.models.build import build_model                            # ëª¨ë¸ ìƒì„±ê¸°
from src.metrics.f1 import macro_f1_from_logits                     # ë§¤í¬ë¡œ F1 ìŠ¤ì½”ì–´ ê³„ì‚° í•¨ìˆ˜


# ---------------------------
# helpers
# ---------------------------

# ---------------------- ì‹¤í–‰ ë””ë ‰í† ë¦¬/ì•„í‹°íŒ©íŠ¸ ìƒì„± ---------------------- #
def _make_run_dirs(cfg, run_id, logger):
    # ë‚ ì§œ ë¬¸ìì—´ í¬ë§·íŒ… (ì˜ˆ: 20250101)
    day = time.strftime(cfg["project"]["date_format"])
    # ì‹œê°„ ë¬¸ìì—´ í¬ë§·íŒ… (ì˜ˆ: 1530)
    time_str = time.strftime(cfg["project"]["time_format"])
    # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ëœ í´ë”ëª… ìƒì„± (ì˜ˆ: 20250907_1530_swin-highperf)
    folder_name = f"{day}_{time_str}_{cfg['project']['run_name']}"
    # ì‹¤í—˜ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬ ìƒì„±
    exp_root = ensure_dir(os.path.join(cfg["output"]["exp_dir"], day, folder_name))
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í„°ë¦¬ ìƒì„±
    ckpt_dir = ensure_dir(os.path.join(exp_root, "ckpt"))
    # ë©”íŠ¸ë¦­ ê¸°ë¡ íŒŒì¼ ê²½ë¡œ
    metrics_path = os.path.join(exp_root, "metrics.jsonl")
    # ì„¤ì • ìŠ¤ëƒ…ìƒ· ì €ì¥ ê²½ë¡œ
    cfg_path = os.path.join(exp_root, "config.yaml")
    # í˜„ì¬ ì„¤ì •ì„ YAMLë¡œ ì €ì¥
    dump_yaml(cfg, cfg_path)
    
    # ë¡œê·¸ ê¸°ë¡
    logger.write(f"[ARTIFACTS] exp_root={exp_root}")            # ì‹¤í—˜ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬
    logger.write(f"[ARTIFACTS] ckpt_dir={ckpt_dir}")            # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í„°ë¦¬
    logger.write(f"[ARTIFACTS] metrics_path={metrics_path}")    # ë©”íŠ¸ë¦­ ê¸°ë¡ íŒŒì¼ ê²½ë¡œ
    logger.write(f"[ARTIFACTS] cfg_snapshot={cfg_path}")        # ì„¤ì • ìŠ¤ëƒ…ìƒ· ì €ì¥ ê²½ë¡œ
    # ê²½ë¡œ ë°˜í™˜
    return exp_root, ckpt_dir, metrics_path, cfg_path

# ---------------------- ë¡œê±° ìƒì„± ---------------------- #
def _make_logger(cfg, run_id):
    # ì¦ê°• íƒ€ì…ì— ë”°ë¥¸ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
    aug_type = "advanced_augmentation" if cfg["train"].get("use_advanced_augmentation", False) else "basic_augmentation"
    log_name = f"train_{time.strftime('%Y%m%d-%H%M')}_{cfg['project']['run_name']}_{aug_type}.log"
    # ë‚ ì§œë³„ ë¡œê·¸ íŒŒì¼ ì „ì²´ ê²½ë¡œ
    log_path = create_log_path("train", log_name)
    # Logger ê°ì²´ ìƒì„±
    logger = Logger(log_path)
    # í‘œì¤€ ì…ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹œì‘
    logger.start_redirect()
    
    # tqdm ì¶œë ¥ë„ ë¡œê±°ì— ë¦¬ë‹¤ì´ë ‰íŠ¸í•  ìˆ˜ ìˆëŠ” ê²½ìš°
    if hasattr(logger, "tqdm_redirect"):
        logger.tqdm_redirect()  # tqdm ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰íŠ¸
        
    # ë¡œê·¸ ì‹œì‘ ë©”ì‹œì§€ ê¸°ë¡
    logger.write(f">> Logger started: {log_path}")
    
    # logger ë°˜í™˜
    return logger

# ---------------------- ë””ë°”ì´ìŠ¤ ì„ íƒ ---------------------- #
def _device(cfg):
    # cfgì— 'cuda' ì§€ì • && CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ 'cuda', ì•„ë‹ˆë©´ 'cpu'
    return "cuda" if (cfg["project"]["device"]=="cuda" and torch.cuda.is_available()) else "cpu"

# ---------------------- ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° ---------------------- #
def _count_params(model):
    # ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜
    total = sum(p.numel() for p in model.parameters())
    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    # íŠœí”Œë¡œ ë°˜í™˜
    return total, trainable

# ---------------------- ì˜µí‹°ë§ˆì´ì €/ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ---------------------- #
def _opt_and_sch(params, cfg, steps_per_epoch, logger):
    # ì˜µí‹°ë§ˆì´ì € ì´ë¦„ ì†Œë¬¸ì ë³€í™˜
    opt_name = cfg["train"]["optimizer"].lower()
    # í•™ìŠµë¥ ê³¼ weight decay ê°€ì ¸ì˜¤ê¸°
    lr = cfg["train"]["lr"]; wd = cfg["train"]["weight_decay"]
    # AdamW ë˜ëŠ” Adam ì˜µí‹°ë§ˆì´ì € ìƒì„±
    opt = AdamW(params, lr=lr, weight_decay=wd) if opt_name=="adamw" else Adam(params, lr=lr, weight_decay=wd)
    # CosineAnnealingLR ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± (epochs * steps ê¸°ì¤€)
    sch = CosineAnnealingLR(opt, T_max=max(1, cfg["train"]["epochs"]*max(1, steps_per_epoch))) \
          if cfg["train"]["scheduler"]=="cosine" else None
          
    # ë¡œê·¸ ê¸°ë¡
    logger.write(
        f"[OPTIM] optimizer={opt.__class__.__name__}, lr={lr}, weight_decay={wd}, " # ì˜µí‹°ë§ˆì´ì €
        f"scheduler={sch.__class__.__name__ if sch else 'none'}"                    # ìŠ¤ì¼€ì¤„ëŸ¬
    )
    
    # ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ë°˜í™˜
    return opt, sch

# ---------------------- í•œ ì—í­ í•™ìŠµ ---------------------- #
def train_one_epoch(model, loader, criterion, optimizer, scaler, device,
                    logger, epoch, max_grad_norm=None, log_interval=50):
    # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
    model.train()
    # ì†ì‹¤ê°’ ëˆ„ì  ì´ˆê¸°í™”
    running_loss = 0.0
    # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ í•¸ë“¤ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì ìš©)
    p = psutil.Process(os.getpid())
    # ì—í­ ì‹œì‘ ë¡œê·¸ ê¸°ë¡
    logger.write(f"[EPOCH {epoch}] >>> TRAIN start | steps={len(loader)}")

    # ë°°ì¹˜ ë‹¨ìœ„ í•™ìŠµ ë£¨í”„
    for step, (imgs, labels) in enumerate(loader, 1):
        # ë°ì´í„°ë¥¼ GPU/CPU ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        imgs, labels = imgs.to(device), labels.to(device)
        # ì˜µí‹°ë§ˆì´ì € gradient ì´ˆê¸°í™”
        optimizer.zero_grad(set_to_none=True)

        # ìë™ í˜¼í•©ì •ë°€(AMP) í•™ìŠµ ì˜ì—­
        with autocast(enabled=scaler is not None):
            # ëª¨ë¸ forward â†’ ì˜ˆì¸¡ ë¡œì§“
            logits = model(imgs)
            # ì†ì‹¤ ê³„ì‚°
            loss = criterion(logits, labels)

        # AMP í™œì„±í™”ëœ ê²½ìš°
        if scaler:
            # ì†ì‹¤ ìŠ¤ì¼€ì¼ë§ í›„ backward
            scaler.scale(loss).backward()
            # gradient clipping í•„ìš”í•  ê²½ìš°
            if max_grad_norm:
                scaler.unscale_(optimizer)  # clip ì „ì— unscale í•„ìš”
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            # ì˜µí‹°ë§ˆì´ì € ìŠ¤í… ë° ìŠ¤ì¼€ì¼ëŸ¬ ì—…ë°ì´íŠ¸
            scaler.step(optimizer)
            scaler.update()
        # AMP ë¹„í™œì„±í™”ëœ ê²½ìš° (ì¼ë°˜ í•™ìŠµ)
        else:
            loss.backward()
            if max_grad_norm:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()

        # ëˆ„ì  ì†ì‹¤ ì—…ë°ì´íŠ¸ (ë°°ì¹˜ í¬ê¸° ë°˜ì˜)
        running_loss += loss.item() * imgs.size(0)

        # ë¡œê·¸ ì¶œë ¥ ê°„ê²©ë§ˆë‹¤ ê¸°ë¡
        if (step % max(1, log_interval)) == 0 or step == 1 or step == len(loader):
            lr = optimizer.param_groups[0]["lr"]
            logger.write(
                f"[EPOCH {epoch}][TRAIN step {step}/{len(loader)}] "
                f"loss={loss.item():.5f} lr={lr:.6f} bs={imgs.size(0)}"
            )

    # ì—í­ ì¢…ë£Œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰(MB)
    mem = p.memory_info().rss / (1024*1024)
    # í‰ê·  ì†ì‹¤ ê³„ì‚°
    epoch_loss = running_loss / len(loader.dataset)
    # ì—í­ ì¢…ë£Œ ë¡œê·¸ ê¸°ë¡
    logger.write(f"[EPOCH {epoch}] <<< TRAIN end | loss={epoch_loss:.5f} mem={mem:.0f}MiB")
    # ì—í­ ì†ì‹¤ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜
    return epoch_loss, mem


# ---------------------- ê²€ì¦ ---------------------- #
@torch.no_grad()  # ê²€ì¦ì€ gradient ê³„ì‚° ë¹„í™œì„±í™”
def validate(model, loader, criterion, device, logger, epoch=None):
    # phase ì´ë¦„ ì •ì˜ (EVAL ë˜ëŠ” EPOCH n)
    phase = f"EPOCH {epoch}" if epoch is not None else "EVAL"
    # ê²€ì¦ ì‹œì‘ ë¡œê·¸
    logger.write(f"[{phase}] >>> VALID start | steps={len(loader)}")
    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
    model.eval()
    # ëˆ„ì  ì†ì‹¤ ì´ˆê¸°í™”
    running_loss = 0.0
    # ë¡œì§“ê³¼ íƒ€ê¹ƒ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    all_logits = []
    all_targets = []

    # ë°°ì¹˜ ë‹¨ìœ„ ê²€ì¦ ë£¨í”„
    for step, (imgs, labels) in enumerate(loader, 1):
        # ë°ì´í„°ë¥¼ GPU/CPU ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        imgs, labels = imgs.to(device), labels.to(device)
        # forward â†’ ë¡œì§“ ê³„ì‚°
        logits = model(imgs)
        # ì†ì‹¤ ê³„ì‚°
        loss = criterion(logits, labels)
        # ëˆ„ì  ì†ì‹¤ ì—…ë°ì´íŠ¸
        running_loss += loss.item() * imgs.size(0)
        # ê²°ê³¼ ì €ì¥ (CPUë¡œ ì´ë™)
        all_logits.append(logits.cpu())
        all_targets.append(labels.cpu())

    # ì „ì²´ ë°°ì¹˜ ë¡œì§“/íƒ€ê¹ƒ ê²°í•©
    logits = torch.cat(all_logits, dim=0)
    targets = torch.cat(all_targets, dim=0)
    # macro-F1 ê³„ì‚°
    f1 = macro_f1_from_logits(logits, targets)
    # í‰ê·  ì†ì‹¤ ê³„ì‚°
    epoch_loss = running_loss / len(loader.dataset)
    # ê²€ì¦ ì¢…ë£Œ ë¡œê·¸ ê¸°ë¡
    logger.write(f"[{phase}] <<< VALID end | loss={epoch_loss:.5f} macro_f1={f1:.5f}")
    # ì†ì‹¤, F1, ë¡œì§“, íƒ€ê¹ƒ ë°˜í™˜
    return epoch_loss, f1, logits, targets


# ---------------------- DataLoader ë¹Œë“œ ---------------------- #
def _build_loaders(cfg, trn_df, val_df, image_dir, logger):
    # ë°ì´í„°ì…‹/ë¡œë” ìƒì„± ë¡œê·¸ ê¸°ë¡
    logger.write(
        f"[DATA] build loaders | img_dir={image_dir} | "
        f"img_size={cfg['train']['img_size']} | bs={cfg['train']['batch_size']}"
    )

    # ë³€í™˜ í•¨ìˆ˜ ì„ íƒ (ê³ ê¸‰ ì¦ê°• vs ê¸°ë³¸ ì¦ê°•)
    use_advanced = cfg["train"].get("use_advanced_augmentation", False)  # ê¸°ë³¸ê°’: False
    train_transform_fn = build_advanced_train_tfms if use_advanced else build_train_tfms
    
    logger.write(f"[DATA] augmentation type: {'advanced' if use_advanced else 'basic'}")

    # í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±
    train_ds = DocClsDataset(
        trn_df,                                   # í•™ìŠµ ë°ì´í„°í”„ë ˆì„
        image_dir,                                # ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬
        cfg["data"]["image_ext"],                 # ì´ë¯¸ì§€ í™•ì¥ì
        cfg["data"]["id_col"],                    # ID ì»¬ëŸ¼ëª…
        cfg["data"]["target_col"],                # íƒ€ê¹ƒ ì»¬ëŸ¼ëª…
        train_transform_fn(cfg["train"]["img_size"])  # ì„ íƒëœ í•™ìŠµìš© ë³€í™˜ íŒŒì´í”„ë¼ì¸
    )

    # ê²€ì¦ìš© ë°ì´í„°ì…‹ ìƒì„±
    valid_ds = DocClsDataset(
        val_df,                                   # ê²€ì¦ ë°ì´í„°í”„ë ˆì„
        image_dir,                                # ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬
        cfg["data"]["image_ext"],                 # ì´ë¯¸ì§€ í™•ì¥ì
        cfg["data"]["id_col"],                    # ID ì»¬ëŸ¼ëª…
        cfg["data"]["target_col"],                # íƒ€ê¹ƒ ì»¬ëŸ¼ëª…
        build_valid_tfms(cfg["train"]["img_size"])# ê²€ì¦ìš© ë³€í™˜ íŒŒì´í”„ë¼ì¸
    )

    # ë°ì´í„°ì…‹ í¬ê¸° ë¡œê·¸ ê¸°ë¡
    logger.write(f"[DATA] dataset sizes | train={len(train_ds)} valid={len(valid_ds)}")

    # í•™ìŠµìš© DataLoader ìƒì„±
    train_ld = DataLoader(
        train_ds,                                 # í•™ìŠµ ë°ì´í„°ì…‹
        batch_size=cfg["train"]["batch_size"],    # ë°°ì¹˜ í¬ê¸°
        shuffle=True,                             # ë¬´ì‘ìœ„ ì„ê¸°
        num_workers=cfg["project"]["num_workers"],# ì›Œì»¤ ìˆ˜
        pin_memory=True,                          # GPU ë©”ëª¨ë¦¬ í•€ning
        drop_last=False                           # ë§ˆì§€ë§‰ ë°°ì¹˜ ìœ ì§€
    )

    # ê²€ì¦ìš© DataLoader ìƒì„±
    valid_ld = DataLoader(
        valid_ds,                                 # ê²€ì¦ ë°ì´í„°ì…‹
        batch_size=cfg["train"]["batch_size"],    # ë°°ì¹˜ í¬ê¸°
        shuffle=False,                            # ìˆœì„œ ìœ ì§€
        num_workers=cfg["project"]["num_workers"],# ì›Œì»¤ ìˆ˜
        pin_memory=True,                          # GPU ë©”ëª¨ë¦¬ í•€ning
        drop_last=False                           # ë§ˆì§€ë§‰ ë°°ì¹˜ ìœ ì§€
    )

    # í•™ìŠµ/ê²€ì¦ ë¡œë” ë°˜í™˜
    return train_ld, valid_ld


# ---------------------- ëª¨ë¸ ë¹Œë“œ ---------------------- #
def _build_model(cfg, device, logger):
    # ëª¨ë¸ ìƒì„±
    model = build_model(
        cfg["model"]["name"],                     # ëª¨ë¸ ì´ë¦„
        cfg["data"]["num_classes"],               # í´ë˜ìŠ¤ ê°œìˆ˜
        cfg["model"]["pretrained"],               # ì‚¬ì „í•™ìŠµ ì—¬ë¶€
        cfg["model"]["drop_rate"],                # Dropout ë¹„ìœ¨
        cfg["model"]["drop_path_rate"],           # DropPath ë¹„ìœ¨
        cfg["model"]["pooling"]                   # í’€ë§ ë°©ì‹
    ).to(device)                                  # ë””ë°”ì´ìŠ¤ì— ë¡œë“œ

    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°
    total, trainable = _count_params(model)

    # ëª¨ë¸ ê´€ë ¨ ë¡œê·¸ ì¶œë ¥
    logger.write(
        f"[MODEL] name={cfg['model']['name']} "         # ëª¨ë¸ ì´ë¦„
        f"pretrained={cfg['model']['pretrained']} "     # ì‚¬ì „í•™ìŠµ ì—¬ë¶€
        f"pooling={cfg['model']['pooling']} "           # í’€ë§ ë°©ì‹
        f"params(total/trainable)={total}/{trainable}"  # íŒŒë¼ë¯¸í„° ê°œìˆ˜
    )

    # ëª¨ë¸ ë°˜í™˜
    return model


# ---------------------- ë°ì´í„° ë¶„í•  (í´ë“œ) ---------------------- #
def _split_folds(df, cfg, logger):
    # í´ë“œ ì¤€ë¹„ ë¡œê·¸ ê¸°ë¡
    logger.write(
        f"[FOLD] preparing | folds={cfg['data']['folds']} " # í´ë“œ ìˆ˜
        f"stratify={cfg['data'].get('stratify', True)}"     # ê³„ì¸µí™” ì—¬ë¶€
    )

    # í´ë“œ ìˆ˜
    folds = cfg["data"]["folds"]

    # fold ì»¬ëŸ¼ì´ ì—†ê±°ë‚˜, ê°’ì´ ìœ íš¨í•˜ì§€ ì•Šì„ ê²½ìš° ìƒˆë¡œ ìƒì„±
    if "fold" not in df.columns or (df["fold"] < 0).any() or (df["fold"] >= folds).any():
        # stratify ëª¨ë“œì¸ ê²½ìš°
        if cfg["data"].get("stratify", True):
            # StratifiedKFold ê°ì²´ ìƒì„±
            skf = StratifiedKFold(
                n_splits=folds,                     # í´ë“œ ìˆ˜
                shuffle=True,                       # ë°ì´í„° ì„ê¸° ì—¬ë¶€
                random_state=cfg["project"]["seed"] # ëœë¤ ì‹œë“œ
            )
            
            # fold ì´ˆê¸°í™”
            df["fold"] = -1
            
            # fold ë¶„ë¦¬ ë° í• ë‹¹
            for f, (_, v_idx) in enumerate(skf.split(df, df[cfg["data"]["target_col"]])):
                # ê° ê²€ì¦ ì¸ë±ìŠ¤ì— í´ë“œ ë²ˆí˜¸ í• ë‹¹
                df.loc[df.index[v_idx], "fold"] = f

        # ë¹„-stratify ëª¨ë“œì¸ ê²½ìš°
        else:
            # ì „ì²´ ì¸ë±ìŠ¤
            idx = np.arange(len(df))

            # fold ë¶„ë¦¬ ë° í• ë‹¹
            for f in range(folds):
                # ê° í´ë“œì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ í• ë‹¹
                df.loc[idx[f::folds], "fold"] = f

    # ë¶„í¬ ê³„ì‚°
    dist = df["fold"].value_counts().sort_index().to_dict()
    # ë¡œê·¸ ì¶œë ¥
    logger.write(f"[FOLD] distribution={dist}")

    # í´ë“œê°€ í• ë‹¹ëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    return df

# =========================
# ê³µê°œ ì§„ì… í•¨ìˆ˜
# =========================

# í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
def run_training(cfg_path: str):
    # ---------------------- ì„¤ì • ë¡œë“œ ---------------------- #
    cfg = load_yaml(cfg_path)                                         # YAML ì„¤ì • ë¡œë“œ
    cfg_dir = os.path.dirname(os.path.abspath(cfg_path))              # cfg íŒŒì¼ì´ ìœ„ì¹˜í•œ ì ˆëŒ€ ê²½ë¡œ
    cfg.setdefault("train", {}).setdefault("log_interval", 50)        # ë¡œê·¸ ê°„ê²© ê¸°ë³¸ê°’ ì„¤ì •

    # ---------------------- ì‹œë“œ ë° ì‹¤í–‰ ID ---------------------- #
    set_seed(cfg["project"]["seed"])                                  # ëœë¤ ì‹œë“œ ê³ ì •
    run_id = f'{cfg["project"]["run_name"]}-{short_uid()}'            # ì‹¤í–‰ ID ìƒì„±
    logger = _make_logger(cfg, run_id)                                # ë¡œê±° ìƒì„±
    logger.write("[BOOT] training pipeline started")                  # íŒŒì´í”„ë¼ì¸ ì‹œì‘ ë¡œê·¸ ê¸°ë¡

    exit_status = "SUCCESS"                                           # ê¸°ë³¸ ì¢…ë£Œ ìƒíƒœ
    exit_code = 0                                                     # ê¸°ë³¸ ì¢…ë£Œ ì½”ë“œ

    try:
        # ---------------------- ë””ë°”ì´ìŠ¤ ì„¤ì • ---------------------- #
        device = _device(cfg)                                         # GPU/CPU ê²°ì •
        logger.write(f"[BOOT] device={device}")                       # ë””ë°”ì´ìŠ¤ ë¡œê·¸ ì¶œë ¥
        logger.write(f"[CFG] loaded config from {cfg_path}")          # ì„¤ì • ë¡œë“œ ë¡œê·¸
        logger.write(f"[CFG] data section: {cfg['data']}")            # ë°ì´í„° ì„¤ì • ì¶œë ¥
        logger.write(f"[CFG] train section: {cfg['train']}")          # í•™ìŠµ ì„¤ì • ì¶œë ¥
        logger.write(f"[CFG] model section: {cfg['model']}")          # ëª¨ë¸ ì„¤ì • ì¶œë ¥
        logger.write(f"[CFG] output section: {cfg['output']}")        # ì¶œë ¥ ì„¤ì • ì¶œë ¥

        # ---------------------- ê²½ë¡œ í™•ì¸ ---------------------- #
        train_csv = resolve_path(cfg_dir, cfg["data"]["train_csv"])   # í•™ìŠµ CSV ê²½ë¡œ í™•ì¸
        sample_csv = resolve_path(cfg_dir, cfg["data"]["sample_csv"]) # ì œì¶œ CSV ê²½ë¡œ í™•ì¸
        image_dir = resolve_path(cfg_dir,                             # ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ í™•ì¸
                                 cfg["data"].get("image_dir_train",
                                 cfg["data"].get("image_dir", "data/raw/train")))
        require_file(train_csv,  "data.train_csv í™•ì¸")                # í•™ìŠµ CSV ì¡´ì¬ í™•ì¸
        require_file(sample_csv, "data.sample_csv í™•ì¸")               # ì œì¶œ CSV ì¡´ì¬ í™•ì¸
        require_dir(image_dir,   "data.image_dir_train í™•ì¸")          # ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ ì¡´ì¬ í™•ì¸

        # ê²½ë¡œ í™•ì¸ ë¡œê·¸ ì¶œë ¥
        logger.write(f"[PATH] OK | train_csv={train_csv} | sample_csv={sample_csv} | image_dir_train={image_dir}")

        # ---------------------- ë°ì´í„° ë¡œë“œ ë° ê²€ì¦ ---------------------- #
        # í•™ìŠµ ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(train_csv)
        # í•„ìˆ˜ ì»¬ëŸ¼ ì •ì˜ (ID, target)
        need_cols = (cfg["data"]["id_col"], cfg["data"]["target_col"])
        # CSV ì»¬ëŸ¼ ë¡œê·¸ ì¶œë ¥
        logger.write(f"[DATA] columns={list(df.columns)} | required={need_cols}")
        # í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ ì‹œ ì˜ˆì™¸ ë°œìƒ
        if cfg["data"]["id_col"] not in df.columns or cfg["data"]["target_col"] not in df.columns:
            raise KeyError(f"CSV ì—´({cfg['data']['id_col']}, {cfg['data']['target_col']}) ëˆ„ë½")
        # ë°ì´í„° í”„ë ˆì„ í´ë“œ ë¶„í•  (StratifiedKFold)
        df = _split_folds(df, cfg, logger)

        # ---------------------- ì•„í‹°íŒ©íŠ¸ ë””ë ‰í† ë¦¬ ---------------------- #
        # # ì‚°ì¶œë¬¼ ê²½ë¡œ ìƒì„±
        exp_root, ckpt_dir, metrics_path, _ = _make_run_dirs(cfg, run_id, logger)

        # ---------------------- í•™ìŠµ ëª¨ë“œ í™•ì¸ ---------------------- #
        valid_fold = cfg["data"]["valid_fold"]          # valid_fold ê°’ í™•ì¸
        logger.write(f"[MODE] valid_fold={valid_fold}") # ëª¨ë“œ ë¡œê·¸ ì¶œë ¥

        # ---------------------- ë‹¨ì¼ í´ë“œ í•™ìŠµ ---------------------- #
        if isinstance(valid_fold, int):
            trn = df[df["fold"]!=valid_fold].reset_index(drop=True)                 # í•™ìŠµ ë°ì´í„°
            val = df[df["fold"]==valid_fold].reset_index(drop=True)                 # ê²€ì¦ ë°ì´í„°
            logger.write(f"[FOLD {valid_fold}] train={len(trn)} valid={len(val)}")  # ë°ì´í„° í¬ê¸° ë¡œê·¸

            train_ld, valid_ld = _build_loaders(cfg, trn, val, image_dir, logger)   # DataLoader ìƒì„±
            model = _build_model(cfg, device, logger)                               # ëª¨ë¸ ë¹Œë“œ
            criterion = nn.CrossEntropyLoss()                                       # ì†ì‹¤ í•¨ìˆ˜
            scaler = GradScaler(enabled=bool(cfg["train"]["amp"]))                  # AMP ìŠ¤ì¼€ì¼ëŸ¬
            
            # ì˜µí‹°ë§ˆì´ì €+ìŠ¤ì¼€ì¤„ëŸ¬
            optimizer, scheduler = _opt_and_sch(model.parameters(), cfg, len(train_ld), logger)

            best_f1 = -1.0                                                          # ìµœê³  F1 ì´ˆê¸°ê°’
            best_path = os.path.join(ckpt_dir, f"best_fold{valid_fold}.pth")        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ

            # ---------------------- ì—í­ ë£¨í”„ ---------------------- #
            for epoch in range(1, cfg["train"]["epochs"]+1):
                logger.write(f"[EPOCH {epoch}] ---------- start ----------")        # ì—í­ ì‹œì‘ ë¡œê·¸
                t0 = time.time()                                                    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
                
                # í•œ ì—í­ í•™ìŠµ ì‹¤í–‰
                tr_loss, tr_mem = train_one_epoch(
                    model, train_ld, criterion, optimizer, scaler,                  # ëª¨ë¸, ë°ì´í„°ë¡œë”, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¼ëŸ¬
                    device, logger, epoch,                                          # ë””ë°”ì´ìŠ¤, ë¡œê±°, ì—í­
                    cfg["train"]["grad_clip_norm"], cfg["train"]["log_interval"]    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘, ë¡œê·¸ ê°„ê²©
                )
                
                # ê²€ì¦ ì‹¤í–‰
                val_loss, val_f1, *_ = validate(
                    model, valid_ld, criterion, device, logger, epoch               # ëª¨ë¸, ë°ì´í„°ë¡œë”, ì†ì‹¤í•¨ìˆ˜, ë””ë°”ì´ìŠ¤, ë¡œê±°, ì—í­
                )
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                if scheduler: 
                    scheduler.step()

                # ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                rec = {
                    "fold": valid_fold,                         # í´ë“œ ë²ˆí˜¸
                    "epoch": epoch,                             # ì—í­ ë²ˆí˜¸
                    "train_loss": tr_loss,                      # í•™ìŠµ ì†ì‹¤
                    "valid_loss": val_loss,                     # ê²€ì¦ ì†ì‹¤
                    "macro_f1": float(val_f1),                  # ë§¤í¬ë¡œ F1 ì ìˆ˜
                    "lr": optimizer.param_groups[0]["lr"],      # í•™ìŠµë¥ 
                    "time_s": time.time() - t0,                 # ê²½ê³¼ ì‹œê°„
                    "mem_MiB": tr_mem,                          # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                }
                jsonl_append(metrics_path, rec)                 # ë©”íŠ¸ë¦­ ì €ì¥
                logger.write(f"[EPOCH {epoch}] metrics={rec}")  # ë©”íŠ¸ë¦­ ë¡œê·¸

                # ìµœê³  F1 ê°±ì‹  ì‹œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if val_f1 > best_f1:
                    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸
                    best_f1 = float(val_f1)
                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                    torch.save({"model": model.state_dict(), "cfg": cfg, "epoch": epoch, "fold": valid_fold}, best_path)
                    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ë¡œê·¸
                    logger.write(f"[EPOCH {epoch}] NEW_BEST F1={best_f1:.5f} -> {best_path}")
                    
                # ì—í­ ì¢…ë£Œ ë¡œê·¸
                logger.write(f"[EPOCH {epoch}] ----------- end -----------")

            # ë‹¨ì¼ í´ë“œ í•™ìŠµ ì¢…ë£Œ ë¡œê·¸
            logger.write(f"[DONE] single-fold training finished | best_f1={best_f1:.5f}")

        # ---------------------- ì „ì²´ í´ë“œ í•™ìŠµ ---------------------- #
        elif isinstance(valid_fold, str) and valid_fold.lower() == "all":
            folds = cfg["data"]["folds"]                            # ì „ì²´ í´ë“œ ìˆ˜
            oof_logits, oof_targets, per_fold = [], [], []          # foldë³„ ì €ì¥ ë³€ìˆ˜

            # fold ë£¨í”„
            for fold in range(folds):
                trn = df[df["fold"]!=fold].reset_index(drop=True)   # í•™ìŠµ ë°ì´í„°
                val = df[df["fold"]==fold].reset_index(drop=True)   # ê²€ì¦ ë°ì´í„°
                # í´ë“œ ì‹œì‘ ë¡œê·¸
                logger.write(f"[FOLD {fold}] >>> start | train={len(trn)} valid={len(val)}")

                # DataLoader ë¹Œë“œ
                train_ld, valid_ld = _build_loaders(cfg, trn, val, image_dir, logger)
                # ëª¨ë¸ ë¹Œë“œ
                model = _build_model(cfg, device, logger)
                # ì†ì‹¤ í•¨ìˆ˜
                criterion = nn.CrossEntropyLoss()
                # AMP ìŠ¤ì¼€ì¼ëŸ¬
                scaler = GradScaler(enabled=bool(cfg["train"]["amp"]))
                # ì˜µí‹°ë§ˆì´ì €/ìŠ¤ì¼€ì¤„ëŸ¬
                optimizer, scheduler = _opt_and_sch(model.parameters(), cfg, len(train_ld), logger)

                best_f1 = -1.0                                              # ìµœê³  f1 ì´ˆê¸°ê°’
                best_path = os.path.join(ckpt_dir, f"best_fold{fold}.pth")  # foldë³„ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
                v_best_logits, v_best_targets = None, None                  # ìµœê³  logits/targets ì €ì¥ ë³€ìˆ˜

                # ì—í­ ë°˜ë³µ
                for epoch in range(1, cfg["train"]["epochs"] + 1):
                    # ì—í­ ì‹œì‘ ë¡œê·¸
                    logger.write(f"[FOLD {fold}][EPOCH {epoch}] ---------- start ----------")
                    # ì‹œì‘ ì‹œê°„
                    t0 = time.time()
                    # í•™ìŠµ
                    tr_loss, tr_mem = train_one_epoch(
                        model, train_ld, criterion, optimizer, scaler,                  # ëª¨ë¸, í•™ìŠµ ë°ì´í„°ë¡œë”, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¼ëŸ¬
                        device, logger, epoch,                                          # ë””ë°”ì´ìŠ¤, ë¡œê±°, ì—í­
                        cfg["train"]["grad_clip_norm"], cfg["train"]["log_interval"]    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘, ë¡œê·¸ ê°„ê²©
                    )
                    
                    # ëª¨ë¸ ê²€ì¦ ì‹¤í–‰
                    # - model       : í˜„ì¬ í•™ìŠµ ì¤‘ì¸ ì‹ ê²½ë§ ëª¨ë¸
                    # - valid_ld    : ê²€ì¦ ë°ì´í„°ì…‹ DataLoader
                    # - criterion   : ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆ: CrossEntropyLoss)
                    # - device      : ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (CPU ë˜ëŠ” GPU)
                    # - logger      : í•™ìŠµ ë¡œê·¸ ê¸°ë¡ ê°ì²´
                    # - epoch       : í˜„ì¬ ì—í­ ë²ˆí˜¸ (ë¡œê·¸ ì¶œë ¥ìš©)
                    #
                    # ë°˜í™˜ê°’:
                    # - val_loss    : ê²€ì¦ ë°ì´í„°ì…‹ ì „ì²´ í‰ê·  ì†ì‹¤
                    # - val_f1      : ê²€ì¦ ë°ì´í„°ì…‹ ë§¤í¬ë¡œ F1 ì ìˆ˜
                    # - v_logits    : ëª¨ë¸ ì¶œë ¥ ë¡œì§“(logits) ì „ì²´ (torch.Tensor)
                    # - v_targets   : ê²€ì¦ ë°ì´í„°ì…‹ì˜ ì‹¤ì œ ë¼ë²¨ ì „ì²´ (torch.Tensor)
                    val_loss, val_f1, v_logits, v_targets = validate(
                        model, valid_ld, criterion, device, logger, epoch
                    )
                    
                    # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                    if scheduler:
                        scheduler.step()

                    # ë©”íŠ¸ë¦­ ê¸°ë¡
                    rec = {
                        "fold": fold,                           # í´ë“œ ë²ˆí˜¸
                        "epoch": epoch,                         # ì—í­ ë²ˆí˜¸
                        "train_loss": tr_loss,                  # í•™ìŠµ ì†ì‹¤
                        "valid_loss": val_loss,                 # ê²€ì¦ ì†ì‹¤
                        "macro_f1": float(val_f1),              # ë§¤í¬ë¡œ F1 ì ìˆ˜
                        "lr": optimizer.param_groups[0]["lr"],  # í•™ìŠµë¥ 
                        "time_s": time.time() - t0,             # ê²½ê³¼ ì‹œê°„
                        "mem_MiB": tr_mem,                      # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                    }
                    # jsonlì— ë©”íŠ¸ë¦­ ì¶”ê°€
                    jsonl_append(metrics_path, rec)
                    # ë¡œê·¸ ì¶œë ¥
                    logger.write(f"[FOLD {fold}][EPOCH {epoch}] metrics={rec}")

                                        # ìµœê³  F1 ê°±ì‹  ì‹œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                    if val_f1 > best_f1:
                        # ìµœê³  ì„±ëŠ¥ ê°±ì‹  â†’ best_f1 ì—…ë°ì´íŠ¸
                        best_f1 = float(val_f1)
                        # í˜„ì¬ foldì—ì„œì˜ ìµœê³  logits/targets ì €ì¥
                        v_best_logits, v_best_targets = v_logits.clone(), v_targets.clone()
                        # ëª¨ë¸ íŒŒë¼ë¯¸í„° + ì„¤ì • + ì—í­/í´ë“œ ì •ë³´ ì €ì¥
                        torch.save(
                            {"model": model.state_dict(), "cfg": cfg, "epoch": epoch, "fold": fold},
                            best_path
                        )
                        # ìƒˆë¡œìš´ ìµœê³  F1 ìŠ¤ì½”ì–´ ë¡œê·¸ ì¶œë ¥
                        logger.write(f"[FOLD {fold}][EPOCH {epoch}] NEW_BEST F1={best_f1:.5f} -> {best_path}")
                    # ì—í­ ì¢…ë£Œ ë¡œê·¸ ì¶œë ¥
                    logger.write(f"[FOLD {fold}][EPOCH {epoch}] ----------- end -----------")

                # foldë³„ ìµœê³  F1 ê¸°ë¡ ëˆ„ì 
                per_fold.append(best_f1)
                # foldë³„ ìµœê³  logits ì €ì¥
                if v_best_logits is not None:
                    oof_logits.append(v_best_logits)
                # foldë³„ ìµœê³  targets ì €ì¥
                if v_best_targets is not None:
                    oof_targets.append(v_best_targets)
                # fold í•™ìŠµ ì¢…ë£Œ ë¡œê·¸ ì¶œë ¥
                logger.write(f"[FOLD {fold}] <<< end | best_f1={best_f1:.5f}")

            # OOF(Out-Of-Fold) ê²°ê³¼ ê³„ì‚° ì‹œì‘
            import torch as _t
            # foldë³„ logits í•©ì¹˜ê¸° (ìˆì„ ê²½ìš°)
            oof_logits_cat = _t.cat(oof_logits, 0) if len(oof_logits) else None
            # foldë³„ targets í•©ì¹˜ê¸° (ìˆì„ ê²½ìš°)
            oof_targets_cat = _t.cat(oof_targets, 0) if len(oof_targets) else None

            # OOF macro F1 ê³„ì‚° (ë‘ í…ì„œ ëª¨ë‘ ì¡´ì¬ ì‹œ)
            if oof_logits_cat is not None and oof_targets_cat is not None:
                # macro F1 ì ìˆ˜ ê³„ì‚°
                oof_macro = macro_f1_from_logits(oof_logits_cat, oof_targets_cat)
                # ë©”íŠ¸ë¦­ íŒŒì¼ì— ê¸°ë¡
                jsonl_append(metrics_path, {"fold": "all", "epoch": -1, "oof_macro_f1": float(oof_macro)})
                # foldë³„ ì ìˆ˜ì™€ í•¨ê»˜ ë¡œê·¸ ì¶œë ¥
                logger.write(f"[OOF] macro-F1={oof_macro:.5f}; per-fold={['%.5f'%s for s in per_fold]}")
                try:
                    # OOF ê²°ê³¼ ë°°ì—´ ì €ì¥
                    oof_dir = ensure_dir(os.path.join(exp_root, "oof"))
                    # logits ì €ì¥
                    np.save(os.path.join(oof_dir, "oof_logits.npy"), oof_logits_cat.numpy())
                    # targets ì €ì¥
                    np.save(os.path.join(oof_dir, "oof_targets.npy"), oof_targets_cat.numpy())
                    # ì €ì¥ ì„±ê³µ ë¡œê·¸ ì¶œë ¥
                    logger.write(f"[OOF] saved arrays -> {oof_dir}")
                # ì €ì¥ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
                except Exception as e:
                    logger.write(f"[OOF][WARN] save failed: {e}")
            # ì „ì²´ í´ë“œ í•™ìŠµ ì¢…ë£Œ ë¡œê·¸ ì¶œë ¥
            logger.write(f"[DONE] all-fold training finished")
            
            # ---------------------- lastest-train í´ë”ì— ë³µì‚¬ ---------------------- #
            # lastest-train í´ë” ê²½ë¡œ ì„¤ì •
            lastest_train_dir = os.path.join("experiments", "train", "lastest-train")
            experiment_folder_name = cfg["project"]["run_name"]  # ì‹¤í—˜ í´ë”ëª… ì¶”ì¶œ
            lastest_train_model_path = os.path.join(lastest_train_dir, experiment_folder_name)
            
            # lastest-train ë””ë ‰í„°ë¦¬ ìƒì„±
            os.makedirs(lastest_train_dir, exist_ok=True)
            
            # ê¸°ì¡´ ëª¨ë¸ í´ë”ê°€ ìˆìœ¼ë©´ ì‚­ì œ (ë®ì–´ì“°ê¸°ë¥¼ ìœ„í•´)
            if os.path.exists(lastest_train_model_path):
                shutil.rmtree(lastest_train_model_path)
                logger.write(f"[CLEANUP] Removed existing lastest-train/{experiment_folder_name}")
            
            # í˜„ì¬ ì‹¤í—˜ ê²°ê³¼ë¥¼ lastest-trainìœ¼ë¡œ ë³µì‚¬
            shutil.copytree(exp_root, lastest_train_model_path)
            logger.write(f"[COPY] Results copied to lastest-train/{experiment_folder_name}")
            logger.write(f"ğŸ“ Latest results: {lastest_train_model_path}")


        # ---------------------- ì˜ëª»ëœ valid_fold ê°’ ---------------------- #
        else:
            raise ValueError("data.valid_fold ëŠ” ì •ìˆ˜ ë˜ëŠ” 'all' ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

        # í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì •ìƒ ì¢…ë£Œ ë¡œê·¸
        logger.write("[BOOT] training pipeline finished successfully")

    # ---------------------- ì˜ˆì™¸ ì²˜ë¦¬ ---------------------- #
    except Exception as e:
        exit_status = "ERROR"                                           # ì¢…ë£Œ ìƒíƒœ ERROR
        exit_code = 1                                                   # ì¢…ë£Œ ì½”ë“œ 1
        logger.write(f"[ERROR] {type(e).__name__}: {e}", print_error=True)  # ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥
        raise                                                           # ì˜ˆì™¸ ë‹¤ì‹œ ë°œìƒ

    # ---------------------- ì¢…ë£Œ ì²˜ë¦¬ ---------------------- #
    finally:
        logger.write(f"[EXIT] TRAINING {exit_status} code={exit_code}") # ì¢…ë£Œ ìƒíƒœ ë¡œê·¸ ê¸°ë¡
        logger.write(">> Stopping logger and restoring stdio")          # ë¡œê±° ì¢…ë£Œ ë¡œê·¸
        logger.stop_redirect()                                          # ë¦¬ë‹¤ì´ë ‰íŠ¸ í•´ì œ
        logger.close()                                                  # ë¡œê±° ë‹«ê¸°