# src/training/train_highperf.py
"""
ê³ ì„±ëŠ¥ í•™ìŠµ íŒŒì´í”„ë¼ì¸
- Mixup ì§€ì›: Mixup ë°ì´í„° ì¦ê°• ì§€ì›
- Hard Augmentation: ê°•ë ¥í•œ ë°ì´í„° ì¦ê°•
- WandB ë¡œê¹…: WandB ì‹¤í—˜ ì¶”ì 
- Swin Transformer & ConvNext ì§€ì›: ìµœì‹  ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì›
"""

# ------------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
import os, time, numpy as np, torch, torch.nn as nn, pandas as pd, psutil  # ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import shutil                                                       # íŒŒì¼/í´ë” ë³µì‚¬ ìœ í‹¸
# os       : íŒŒì¼/ë””ë ‰í„°ë¦¬ ê²½ë¡œ, ì‹œìŠ¤í…œ ìœ í‹¸
# time     : ì‹œê°„ ì¸¡ì •, ë¡œê¹…
# numpy    : ìˆ˜ì¹˜ ê³„ì‚°, ë°°ì—´ ì—°ì‚°
# torch    : PyTorch ë©”ì¸ ëª¨ë“ˆ
# torch.nn : ì‹ ê²½ë§ ê³„ì¸µ/ì†ì‹¤ í•¨ìˆ˜ ëª¨ë“ˆ
# pandas   : ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
# psutil   : ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
# shutil   : íŒŒì¼/í´ë” ë³µì‚¬, ì´ë™

# ------------------------- PyTorch ìœ í‹¸ ------------------------- #
from torch.utils.data import DataLoader                             # ë°ì´í„° ë¡œë” í´ë˜ìŠ¤
from sklearn.model_selection import StratifiedKFold                 # ê³„ì¸µì  K-í´ë“œ ë¶„í• 
from torch.cuda.amp import autocast, GradScaler                     # AMP (ìë™ í˜¼í•© ì •ë°€ë„) ì§€ì›
from torch.optim import Adam, AdamW                                 # ì˜µí‹°ë§ˆì´ì € (Adam, AdamW)
from torch.optim.lr_scheduler import CosineAnnealingLR              # ì½”ì‚¬ì¸ ê°ì‡  ìŠ¤ì¼€ì¤„ëŸ¬
from tqdm import tqdm                                               # ì§„í–‰ë¥  í‘œì‹œë°”

# ------------------------- í”„ë¡œì íŠ¸ ìœ í‹¸ ------------------------- #
from src.utils.config import set_seed                               # ëœë¤ ì‹œë“œ ê³ ì •
from src.logging.logger import Logger                               # ê¸°ë³¸ ë¡œê±° í´ë˜ìŠ¤
from src.logging.wandb_logger import WandbLogger, create_wandb_config # WandB ë¡œê±° ë° ì„¤ì • ìƒì„±
from src.utils.core.common import (                                             # í•µì‹¬ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
    load_yaml, ensure_dir, dump_yaml, short_uid, resolve_path, require_file, require_dir, create_log_path
)

# ------------------------- ì‹œê°í™” ë° ì¶œë ¥ ê´€ë¦¬ ------------------------- #
from src.utils.visualizations import visualize_training_pipeline, create_organized_output_structure
from src.utils.visualizations import ExperimentOutputManager

# ------------------------- ë°ì´í„°/ëª¨ë¸ ê´€ë ¨ ------------------------- #
from src.data.dataset import HighPerfDocClsDataset, mixup_data      # ê³ ì„±ëŠ¥ ë°ì´í„°ì…‹/ë¯¹ìŠ¤ì—… í•¨ìˆ˜
from src.models.build import build_model, get_recommended_model, build_model_for_fold, is_multi_model_config, get_model_for_fold     # ëª¨ë¸ ë¹Œë“œ/ì¶”ì²œ í•¨ìˆ˜
from src.metrics.f1 import macro_f1_from_logits                     # ë§¤í¬ë¡œ F1 ìŠ¤ì½”ì–´ ê³„ì‚°


# ---------------------- Mixup í•™ìŠµ í•¨ìˆ˜ ---------------------- #
# Mixup ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
def mixup_criterion(criterion, pred, y_a, y_b, lam):
    # ê°€ì¤‘ í‰ê·  ì†ì‹¤ ë°˜í™˜
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# Mixup í•™ìŠµ í•¨ìˆ˜ ì •ì˜
def train_one_epoch_mixup(model, loader, criterion, optimizer, scaler, device,  # ëª¨ë¸, ë°ì´í„°ë¡œë”, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¼ëŸ¬, ë””ë°”ì´ìŠ¤
                         logger, wandb_logger, epoch, max_grad_norm=None,       # ë¡œê±°/ì—í­/ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ íŒŒë¼ë¯¸í„°
                         mixup_alpha=1.0, use_mixup=True):                      # Mixup ì•ŒíŒŒê°’ê³¼ ì‚¬ìš© ì—¬ë¶€
    
    model.train()           # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
    running_loss = 0.0      # ëˆ„ì  ì†ì‹¤ ì´ˆê¸°í™”
    total_samples = 0       # ì´ ìƒ˜í”Œ ìˆ˜ ì´ˆê¸°í™”

    # ë°ì´í„°ì…‹ epoch ì—…ë°ì´íŠ¸ (Hard Augmentation ê°•ë„ ì¡°ì ˆ)
    # ë°ì´í„°ì…‹ì´ ì—í­ ì—…ë°ì´íŠ¸ ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
    if hasattr(loader.dataset, 'update_epoch'):
        # ì—í­ì— ë”°ë¥¸ ì¦ê°• ê°•ë„ ì—…ë°ì´íŠ¸
        loader.dataset.update_epoch(epoch)
    
    # í•™ìŠµ ì‹œì‘ ë¡œê·¸
    logger.write(f"[EPOCH {epoch}] >>> TRAIN start | steps={len(loader)} mixup={use_mixup}")
    
    # ë°°ì¹˜ë³„ í•™ìŠµ ì‹œì‘
    for step, (imgs, labels) in enumerate(tqdm(loader, desc=f"Train Epoch {epoch}"), 1):
        imgs, labels = imgs.to(device), labels.to(device)   # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
        optimizer.zero_grad(set_to_none=True)               # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        
        # --------------------- Mixup ì ìš© ì—¬ë¶€ --------------------- #
        # Mixup ì ìš©
        if use_mixup and np.random.random() > 0.5:          # 50% í™•ë¥ ë¡œ Mixup ì ìš©
            # Mixup ë°ì´í„° ìƒì„±
            mixed_imgs, y_a, y_b, lam = mixup_data(imgs, labels, mixup_alpha)
            
            # AMP ìë™ ìºìŠ¤íŒ… ì ìš©
            with autocast(enabled=scaler is not None):
                # ë¯¹ìŠ¤ëœ ì´ë¯¸ì§€ë¡œ ìˆœì „íŒŒ
                logits = model(mixed_imgs)
                # Mixup ì†ì‹¤ ê³„ì‚°
                loss = mixup_criterion(criterion, logits, y_a, y_b, lam)
                
        # Mixup ë¯¸ì ìš© ê²½ìš°
        else:
            with autocast(enabled=scaler is not None):      # AMP ìë™ ìºìŠ¤íŒ… ì ìš©
                logits = model(imgs)                        # ì›ë³¸ ì´ë¯¸ì§€ë¡œ ìˆœì „íŒŒ
                loss = criterion(logits, labels)            # ì¼ë°˜ ì†ì‹¤ ê³„ì‚°
        
        # --------------------- ì—­ì „íŒŒ ë° ì˜µí‹°ë§ˆì´ì € ìŠ¤í… -------------------- #
        # AMP ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš© ì‹œ
        if scaler:                                      
            scaler.scale(loss).backward()                   # ìŠ¤ì¼€ì¼ëœ ì†ì‹¤ë¡œ ì—­ì „íŒŒ
            if max_grad_norm:                               # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì ìš© ì—¬ë¶€
                scaler.unscale_(optimizer)                  # ê·¸ë˜ë””ì–¸íŠ¸ ì–¸ìŠ¤ì¼€ì¼ë§
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            scaler.step(optimizer)                          # ìŠ¤ì¼€ì¼ëœ ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
            scaler.update()                                 # ìŠ¤ì¼€ì¼ëŸ¬ ì—…ë°ì´íŠ¸
            
        # ì¼ë°˜ ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ
        else:
            loss.backward()                                 # ì¼ë°˜ ì—­ì „íŒŒ
            if max_grad_norm:                               # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì ìš© ì—¬ë¶€
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            optimizer.step()                                # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…

        # ë©”íŠ¸ë¦­ ëˆ„ì 
        running_loss += loss.item() * imgs.size(0)          # ë°°ì¹˜ í¬ê¸°ë¡œ ê°€ì¤‘ëœ ì†ì‹¤ ëˆ„ì 
        total_samples += imgs.size(0)                       # ì´ ìƒ˜í”Œ ìˆ˜ ëˆ„ì 
        
        # -------------------- WandB ë¡œê¹… -------------------- #
        if wandb_logger and step % 10 == 0:                 # 10 ìŠ¤í…ë§ˆë‹¤ WandB ë¡œê¹…
            wandb_logger.log_metrics({                      # ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ë¡œê¹…
                "train/batch_loss": loss.item(),            # ë°°ì¹˜ ì†ì‹¤ê°’
                "train/learning_rate": optimizer.param_groups[0]["lr"],  # í˜„ì¬ í•™ìŠµë¥ 
                "train/epoch": epoch,                       # í˜„ì¬ ì—í­ ë²ˆí˜¸
                "train/step": step                          # í˜„ì¬ ìŠ¤í… ë²ˆí˜¸
            })                                              # ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ì¢…ë£Œ
        
        # -------------------- ë¡œê·¸ ì¶œë ¥ ---------------------- #
        # 50ìŠ¤í…ë§ˆë‹¤ ë˜ëŠ” ì²«/ë§ˆì§€ë§‰ ìŠ¤í…
        if step % 50 == 0 or step == 1 or step == len(loader):
            lr = optimizer.param_groups[0]["lr"]            # í˜„ì¬ í•™ìŠµë¥  ì¶”ì¶œ
            
            # ë¡œê·¸ ë©”ì‹œì§€ ì‘ì„±
            logger.write(
                f"[EPOCH {epoch}][TRAIN step {step}/{len(loader)}] "        # ì—í­/ìŠ¤í… ì •ë³´
                f"loss={loss.item():.5f} lr={lr:.6f} bs={imgs.size(0)}"     # ì†ì‹¤/í•™ìŠµë¥ /ë°°ì¹˜í¬ê¸°
            )
    
    # ì—í­ í‰ê·  ì†ì‹¤ ê³„ì‚°
    epoch_loss = running_loss / total_samples
    
    # í•™ìŠµ ì¢…ë£Œ ë¡œê·¸
    logger.write(f"[EPOCH {epoch}] <<< TRAIN end | loss={epoch_loss:.5f}")
    
    # ì—í­ ì†ì‹¤ ë°˜í™˜
    return epoch_loss


# ---------------------- ê³ ì„±ëŠ¥ ê²€ì¦ í•¨ìˆ˜ ---------------------- #
@torch.no_grad()            # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” ë°ì½”ë ˆì´í„°
# ê³ ì„±ëŠ¥ ê²€ì¦ í•¨ìˆ˜ ì •ì˜
def validate_highperf(model, loader, criterion, device, logger, wandb_logger, epoch=None):
    phase = f"EPOCH {epoch}" if epoch is not None else "EVAL"           # ì—í­ ì •ë³´ ë˜ëŠ” EVAL ì„¤ì •
    logger.write(f"[{phase}] >>> VALID start | steps={len(loader)}")    # ê²€ì¦ ì‹œì‘ ë¡œê·¸
    
    model.eval()            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    running_loss = 0.0      # ëˆ„ì  ì†ì‹¤ ì´ˆê¸°í™”
    total_samples = 0       # ì´ ìƒ˜í”Œ ìˆ˜ ì´ˆê¸°í™”
    all_logits = []         # ëª¨ë“  ë¡œì§“ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    all_targets = []        # ëª¨ë“  íƒ€ê²Ÿ ì €ì¥ ë¦¬ìŠ¤íŠ¸

    # ---------------------- ë°°ì¹˜ë³„ ê²€ì¦ ì‹œì‘ --------------------- #
    for step, (imgs, labels) in enumerate(tqdm(loader, desc=f"Valid Epoch {epoch}"), 1):
        imgs, labels = imgs.to(device), labels.to(device)   # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
        
        logits = model(imgs)                                # ëª¨ë¸ ìˆœì „íŒŒ
        loss = criterion(logits, labels)                    # ì†ì‹¤ ê³„ì‚°
        
        running_loss += loss.item() * imgs.size(0)          # ë°°ì¹˜ í¬ê¸°ë¡œ ê°€ì¤‘ëœ ì†ì‹¤ ëˆ„ì 
        total_samples += imgs.size(0)                       # ì´ ìƒ˜í”Œ ìˆ˜ ëˆ„ì 
        all_logits.append(logits.cpu())                     # ë¡œì§“ì„ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
        all_targets.append(labels.cpu())                    # ë¼ë²¨ì„ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥

    # ---------------------- ì—í­ ë©”íŠ¸ë¦­ ê³„ì‚° --------------------- #
    logits = torch.cat(all_logits, dim=0)               # ëª¨ë“  ë¡œì§“ ì—°ê²°
    targets = torch.cat(all_targets, dim=0)             # ëª¨ë“  íƒ€ê²Ÿ ì—°ê²°
    f1 = macro_f1_from_logits(logits, targets)          # ë§¤í¬ë¡œ F1 ìŠ¤ì½”ì–´ ê³„ì‚°
    epoch_loss = running_loss / total_samples           # ì—í­ í‰ê·  ì†ì‹¤ ê³„ì‚°

    # ------------------------ WandB ë¡œê¹… ----------------------- #
    # WandB ë¡œê±°ê°€ ìˆëŠ” ê²½ìš°
    if wandb_logger:
        wandb_logger.log_metrics({                      # ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ë¡œê¹…
            "val/loss": epoch_loss,                     # ê²€ì¦ ì†ì‹¤ê°’
            "val/f1": f1,                               # ê²€ì¦ F1 ìŠ¤ì½”ì–´
            "val/epoch": epoch                          # ì—í­ ë²ˆí˜¸
        })

    # ê²€ì¦ ì¢…ë£Œ ë¡œê·¸
    logger.write(f"[{phase}] <<< VALID end | loss={epoch_loss:.5f} macro_f1={f1:.5f}")
    
    # ì†ì‹¤, F1, ë¡œì§“, íƒ€ê²Ÿ ë°˜í™˜
    return epoch_loss, f1, logits, targets


# ---------------------- ê³ ì„±ëŠ¥ ë°ì´í„°ë¡œë” ë¹Œë“œ í•¨ìˆ˜ ---------------------- #
# ê³ ì„±ëŠ¥ ë°ì´í„°ë¡œë” ë¹Œë“œ í•¨ìˆ˜ ì •ì˜
def build_highperf_loaders(cfg, trn_df, val_df, image_dir, logger, epoch=0):
    img_size = cfg["train"]["img_size"]             # ì„¤ì •ì—ì„œ ì´ë¯¸ì§€ í¬ê¸° ì¶”ì¶œ
    batch_size = cfg["train"]["batch_size"]         # ì„¤ì •ì—ì„œ ë°°ì¹˜ í¬ê¸° ì¶”ì¶œ
    total_epochs = cfg["train"]["epochs"]           # ì„¤ì •ì—ì„œ ì´ ì—í­ ìˆ˜ ì¶”ì¶œ

    # ë°ì´í„°ë¡œë” ë¹Œë“œ ë¡œê·¸
    logger.write(f"[DATA] build highperf loaders | img_size={img_size} bs={batch_size}")
    logger.write(f"[DATA] augmentation: baseline-advanced (normal + hard augmentation)")
    
    # ê³ ì„±ëŠ¥ ë°ì´í„°ì…‹ ìƒì„±
    train_ds = HighPerfDocClsDataset(
        trn_df,                                     # í•™ìŠµ ë°ì´í„°í”„ë ˆì„
        image_dir,                                  # ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
        img_size=img_size,                          # ì´ë¯¸ì§€ í¬ê¸°
        epoch=epoch,                                # í˜„ì¬ ì—í­
        total_epochs=total_epochs,                  # ì´ ì—í­ ìˆ˜
        is_train=True,                              # í•™ìŠµ ëª¨ë“œ í”Œë˜ê·¸
        id_col=cfg["data"]["id_col"],               # ID ì»¬ëŸ¼ëª…
        target_col=cfg["data"]["target_col"],       # íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
        logger=logger                               # ë¡œê±° ê°ì²´
    )
    
    # ê²€ì¦ìš© ê³ ì„±ëŠ¥ ë°ì´í„°ì…‹ ìƒì„±
    valid_ds = HighPerfDocClsDataset(
        val_df,                                     # ê²€ì¦ ë°ì´í„°í”„ë ˆì„
        image_dir,                                  # ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
        img_size=img_size,                          # ì´ë¯¸ì§€ í¬ê¸°
        epoch=epoch,                                # í˜„ì¬ ì—í­
        total_epochs=total_epochs,                  # ì´ ì—í­ ìˆ˜
        is_train=False,                             # í‰ê°€ ëª¨ë“œ í”Œë˜ê·¸
        id_col=cfg["data"]["id_col"],               # ID ì»¬ëŸ¼ëª…
        target_col=cfg["data"]["target_col"],       # íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
        logger=logger                               # ë¡œê±° ê°ì²´
    )
    
    # ë°ì´í„°ì…‹ í¬ê¸° ë¡œê·¸
    logger.write(f"[DATA] dataset sizes | train={len(train_ds)} valid={len(valid_ds)}")
    
    # í•™ìŠµìš© ë°ì´í„°ë¡œë” ìƒì„±
    train_ld = DataLoader(
        train_ds,                                   # í•™ìŠµ ë°ì´í„°ì…‹
        batch_size=batch_size,                      # ë°°ì¹˜ í¬ê¸°
        shuffle=True,                               # ë°ì´í„° ì…”í”Œ í™œì„±í™”
        num_workers=cfg["project"]["num_workers"],  # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        pin_memory=True,                            # ë©”ëª¨ë¦¬ ê³ ì • í™œì„±í™”
        drop_last=False                             # ë§ˆì§€ë§‰ ë°°ì¹˜ ìœ ì§€
    )
    
    # ê²€ì¦ìš© ë°ì´í„°ë¡œë” ìƒì„±
    valid_ld = DataLoader(
        valid_ds,                                   # ê²€ì¦ ë°ì´í„°ì…‹
        batch_size=batch_size,                      # ë°°ì¹˜ í¬ê¸°
        shuffle=False,                              # ì…”í”Œ ë¹„í™œì„±í™” (ê²€ì¦ìš©)
        num_workers=cfg["project"]["num_workers"],  # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        pin_memory=True,                            # ë©”ëª¨ë¦¬ ê³ ì • í™œì„±í™”
        drop_last=False                             # ë§ˆì§€ë§‰ ë°°ì¹˜ ìœ ì§€
    )
    
    # í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¡œë” ë°˜í™˜
    return train_ld, valid_ld


# ---------------------- ê³ ì„±ëŠ¥ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---------------------- #
# ê³ ì„±ëŠ¥ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
def run_highperf_training(cfg_path: str):
    #--------------------------- ì„¤ì • ë° ë¡œê±° ì´ˆê¸°í™” ------------------------- #
    cfg = load_yaml(cfg_path)                               # YAML ì„¤ì • íŒŒì¼ ë¡œë“œ
    cfg_dir = os.path.dirname(os.path.abspath(cfg_path))    # ì„¤ì • íŒŒì¼ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
    
    #------------------------ ëœë¤ ì‹œë“œ ë° ì‹¤í–‰ ID ì„¤ì • ---------------------- #
    set_seed(cfg["project"]["seed"])                        # ëœë¤ ì‹œë“œ ê³ ì •
    run_id = f'{cfg["project"]["run_name"]}'  # ì‹¤í–‰ ID ìƒì„±
    
    #------------------------- ì‹¤í—˜ ë””ë ‰í„°ë¦¬ ë° ë¡œê±° ì„¤ì • ---------------------- #
    day = time.strftime(cfg["project"]["date_format"])      # í˜„ì¬ ë‚ ì§œ ë¬¸ìì—´
    time_str = time.strftime(cfg["project"]["time_format"]) # í˜„ì¬ ì‹œê°„ ë¬¸ìì—´
    # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ëœ í´ë”ëª… ìƒì„± (ì˜ˆ: swin-highperf_20250907_1530)
    folder_name = f"{day}_{time_str}_{cfg['project']['run_name']}"
    exp_root = ensure_dir(os.path.join(cfg["output"]["exp_dir"], day, folder_name))  # ì‹¤í—˜ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬
    ckpt_dir = ensure_dir(os.path.join(exp_root, "ckpt"))   # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í„°ë¦¬
    
    # lastest-train í´ë”ì—ë„ ë™ì¼í•œ êµ¬ì¡°ë¡œ ìƒì„±
    lastest_exp_root = ensure_dir(os.path.join(cfg["output"]["exp_dir"], "lastest-train", folder_name))
    lastest_ckpt_dir = ensure_dir(os.path.join(lastest_exp_root, "ckpt"))
    
    #------------------------- ì„¤ì • íŒŒì¼ ë°±ì—… ------------------------- #
    # ì¦ê°• íƒ€ì…ì— ë”°ë¥¸ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
    aug_type = "advanced_augmentation" if cfg["train"].get("use_advanced_augmentation", False) else "basic_augmentation"
    log_filename = f'{cfg["project"]["log_prefix"]}_{day}-{time.strftime("%H%M")}_{run_id}_{aug_type}.log'  # ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
    log_path = create_log_path("train", log_filename)       # ë‚ ì§œë³„ ë¡œê·¸ íŒŒì¼ ì „ì²´ ê²½ë¡œ
    
    # ë¡œê±° ê°ì²´ ìƒì„±
    logger = Logger(
        log_path=log_path,                                  # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        print_also=cfg["project"]["verbose"]                # ì½˜ì†” ì¶œë ¥ ì—¬ë¶€
    )
    
    # íŒŒì´í”„ë¼ì¸ ì‹œì‘ ë¡œê·¸
    logger.write("[BOOT] high-performance training pipeline started")
    
    try:
        #--------------------------- ë””ë°”ì´ìŠ¤ ë° ê²½ë¡œ ì„¤ì • ------------------------- #
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")                   # GPU/CPU ë””ë°”ì´ìŠ¤ ì„¤ì •
        logger.write(f"[BOOT] device={device}")                                                 # ë””ë°”ì´ìŠ¤ ì •ë³´ ë¡œê·¸

        # ê²½ë¡œ í™•ì¸
        train_csv = resolve_path(cfg_dir, cfg["data"]["train_csv"])                             # í•™ìŠµ CSV ê²½ë¡œ í•´ê²°
        image_dir = resolve_path(cfg_dir, cfg["data"].get("image_dir_train", "data/raw/train")) # ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ ê²½ë¡œ í•´ê²°
        require_file(train_csv, "train_csv í™•ì¸")                                                # í•™ìŠµ CSV íŒŒì¼ ì¡´ì¬ì„± ê²€ì¦
        require_dir(image_dir, "image_dir í™•ì¸")                                                 # ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ ì¡´ì¬ì„± ê²€ì¦

        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(train_csv)                                                             # í•™ìŠµ ë°ì´í„° CSV ë¡œë“œ
        logger.write(f"[DATA] loaded train data | shape={df.shape}")                            # ë°ì´í„° ë¡œë“œ ë¡œê·¸

        #--------------------------- í´ë“œ ë¶„í•  --------------------------- #
        folds = cfg["data"]["folds"]    # í´ë“œ ìˆ˜ ì„¤ì •
        
        # í´ë“œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìƒì„±
        if "fold" not in df.columns:
            # ê³„ì¸µì  K-í´ë“œ ê°ì²´ ìƒì„±
            skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=cfg["project"]["seed"])
            
            # í´ë“œ ì»¬ëŸ¼ ì´ˆê¸°í™”
            df["fold"] = -1
            
            # í´ë“œë³„ ë¶„í• 
            for f, (_, v_idx) in enumerate(skf.split(df, df[cfg["data"]["target_col"]])):
                # ê²€ì¦ ì¸ë±ìŠ¤ì— í´ë“œ ë²ˆí˜¸ í• ë‹¹
                df.loc[df.index[v_idx], "fold"] = f
        
        #--------------------------- WandB ì„¤ì • --------------------------- #
        # ë‹¤ì¤‘ ëª¨ë¸ ì—¬ë¶€ì— ë”°ë¥¸ ëª¨ë¸ëª… ê²°ì •
        if is_multi_model_config(cfg):
            # ë‹¤ì¤‘ ëª¨ë¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ í´ë“œì˜ ëª¨ë¸ëª… ì‚¬ìš© (WandB í‘œì‹œìš©)
            wandb_model_name, _ = get_model_for_fold(cfg, 0)
        else:
            # ë‹¨ì¼ ëª¨ë¸ì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹
            wandb_model_name = cfg["model"]["name"]
            
        wandb_config = create_wandb_config(                     # WandB ì„¤ì • ìƒì„±
            model_name=wandb_model_name,                        # ëª¨ë¸ëª…
            img_size=cfg["train"]["img_size"],                  # ì´ë¯¸ì§€ í¬ê¸°
            batch_size=cfg["train"]["batch_size"],              # ë°°ì¹˜ í¬ê¸°
            learning_rate=cfg["train"]["lr"],                   # í•™ìŠµë¥ 
            epochs=cfg["train"]["epochs"],                      # ì—í­ ìˆ˜
            mixup_alpha=cfg["train"].get("mixup_alpha", 1.0),  # Mixup ì•ŒíŒŒê°’
            hard_augmentation=True,                             # Hard Augmentation í”Œë˜ê·¸
            optimizer=cfg["train"]["optimizer"],                # ì˜µí‹°ë§ˆì´ì € íƒ€ì…
            scheduler=cfg["train"]["scheduler"]                 # ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…
        )
        
        #--------------------------- í´ë“œë³„ í•™ìŠµ --------------------------- #
        # í´ë“œ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        fold_results = []
        
        # ê° í´ë“œë³„ ë°˜ë³µ
        for fold in range(folds):
            #--------------------------- í´ë“œë³„ ë¡œê±° ì„¤ì • -------------------------- #
            logger.write(f"\n{'='*50}")                             # í´ë“œ êµ¬ë¶„ì„ 
            logger.write(f"FOLD {fold+1}/{folds} START")            # í´ë“œ ì‹œì‘ ë¡œê·¸
            logger.write(f"{'='*50}")                               # í´ë“œ êµ¬ë¶„ì„ 
            
            # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
            trn_df = df[df["fold"] != fold].reset_index(drop=True)  # í•™ìŠµ ë°ì´í„°í”„ë ˆì„
            val_df = df[df["fold"] == fold].reset_index(drop=True)  # ê²€ì¦ ë°ì´í„°í”„ë ˆì„
            
            # í´ë“œ ë°ì´í„° í¬ê¸° ë¡œê·¸
            logger.write(f"[FOLD {fold}] train={len(trn_df)} valid={len(val_df)}")
            
            #-------------------------- WandB ë¡œê±° ì´ˆê¸°í™” -------------------------- #
            # ë™ì  ì‹¤í–‰ ì´ë¦„ ìƒì„± (submissionsì™€ ë™ì¼í•œ í˜•ì‹)
            current_date = pd.Timestamp.now().strftime('%Y%m%d')
            current_time = pd.Timestamp.now().strftime('%H%M')
            # ë‹¤ì¤‘ ëª¨ë¸ì¸ ê²½ìš° ì´ë¯¸ ì •ì˜ëœ wandb_model_name ì‚¬ìš©
            if is_multi_model_config(cfg):
                model_name_for_exp = wandb_model_name  # ì´ë¯¸ WandB ì„¤ì • ì‹œ ì •ì˜ëœ ë³€ìˆ˜ ì¬ì‚¬ìš©
            else:
                model_name_for_exp = cfg["model"]["name"]
            dynamic_experiment_name = f"{current_date}_{current_time}_{model_name_for_exp}_ensemble_tta"
            
            # WandB ì´ˆê¸°í™”
            wandb_logger = WandbLogger(
                experiment_name=dynamic_experiment_name,        # ë™ì  ì‹¤í—˜ëª…
                config=wandb_config,                            # ì„¤ì • ë”•ì…”ë„ˆë¦¬
                tags=["high-performance", "mixup", "hard-aug"]  # íƒœê·¸ ë¦¬ìŠ¤íŠ¸
            )
            
            # WandB ì‹¤í–‰ ì´ˆê¸°í™”
            wandb_logger.init_run(fold=fold)
            
            #-------------------------- ëª¨ë¸ ìƒì„± -------------------------- #
            # ë‹¤ì¤‘ ëª¨ë¸ ì„¤ì • í™•ì¸ ë° ëª¨ë¸ ìƒì„±
            if is_multi_model_config(cfg):
                # ë‹¤ì¤‘ ëª¨ë¸: í´ë“œë³„ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
                model = build_model_for_fold(cfg, fold, cfg["data"]["num_classes"]).to(device)
                model_name, _ = get_model_for_fold(cfg, fold)
                logger.write(f"[MULTI-MODEL] Fold {fold} using model: {model_name}")
            else:
                # ë‹¨ì¼ ëª¨ë¸: ëª¨ë“  í´ë“œì— ê°™ì€ ëª¨ë¸ ì‚¬ìš©
                model_name = get_recommended_model(cfg["model"]["name"])
                model = build_model(
                    model_name,                         # ëª¨ë¸ëª…
                    cfg["data"]["num_classes"],         # í´ë˜ìŠ¤ ìˆ˜
                    cfg["model"]["pretrained"],         # ì‚¬ì „í›ˆë ¨ ì—¬ë¶€
                    cfg["model"]["drop_rate"],          # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
                    cfg["model"]["drop_path_rate"],     # ë“œë¡­íŒ¨ìŠ¤ ë¹„ìœ¨
                    cfg["model"]["pooling"]             # í’€ë§ íƒ€ì…
                ).to(device)                            # GPUë¡œ ëª¨ë¸ ì´ë™
                logger.write(f"[SINGLE-MODEL] All folds using model: {model_name}")
            
            #-------------------------- ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ------------------------- #
            # AdamW ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ
            if cfg["train"]["optimizer"] == "adamw":
                # AdamW ì˜µí‹°ë§ˆì´ì € ìƒì„±
                optimizer = AdamW(model.parameters(), lr=cfg["train"]["lr"], weight_decay=cfg["train"]["weight_decay"])
            # ê¸°ë³¸ Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ì‹œ
            else:
                # Adam ì˜µí‹°ë§ˆì´ì € ìƒì„±
                optimizer = Adam(model.parameters(), lr=cfg["train"]["lr"], weight_decay=cfg["train"]["weight_decay"])
            
            # ì½”ì‚¬ì¸ ê°ì‡  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
            scheduler = CosineAnnealingLR(optimizer, T_max=cfg["train"]["epochs"])
            
            #-------------------------- ì†ì‹¤ í•¨ìˆ˜ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì„¤ì • ------------------------- #
            # êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜
            criterion = nn.CrossEntropyLoss()
            # AMP ìŠ¤ì¼€ì¼ëŸ¬ (ì¡°ê±´ë¶€)
            scaler = GradScaler() if cfg["train"].get("mixed_precision", True) else None
            
            #-------------------------- í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ -------------------------- #
            # ìµœê³  F1 ì ìˆ˜ ì¶”ì 
            best_f1 = 0.0   # ìµœê³  F1 ì ìˆ˜ ì´ˆê¸°í™”
            
            # ìµœê³  ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            best_model_path = os.path.join(ckpt_dir, f"best_model_fold_{fold+1}.pth")
            lastest_best_model_path = os.path.join(lastest_ckpt_dir, f"best_model_fold_{fold+1}.pth")
            
            #-------------------------- í´ë“œë³„ í•™ìŠµ -------------------------- #
            # ì—í¬í¬ë³„ í•™ìŠµ
            for epoch in range(1, cfg["train"]["epochs"] + 1):
                # ë°ì´í„°ë¡œë” ìƒì„± (ì—í¬í¬ë³„ Hard Aug ê°•ë„ ì¡°ì ˆ)  # ë°ì´í„°ë¡œë” ìƒì„± ì£¼ì„
                train_ld, valid_ld = build_highperf_loaders(cfg, trn_df, val_df, image_dir, logger, epoch-1)
                
                # í•™ìŠµ
                train_loss = train_one_epoch_mixup(                         # Mixup í•™ìŠµ í•¨ìˆ˜ í˜¸ì¶œ
                    model, train_ld, criterion, optimizer, scaler, device,  # ëª¨ë¸/ë°ì´í„°/ì†ì‹¤/ì˜µí‹°ë§ˆì´ì €/ìŠ¤ì¼€ì¼ëŸ¬/ë””ë°”ì´ìŠ¤
                    logger, wandb_logger, epoch,                            # ë¡œê±°/WandBë¡œê±°/ì—í­
                    max_grad_norm=cfg["train"].get("max_grad_norm"),        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ë…¸ë¦„
                    mixup_alpha=cfg["train"].get("mixup_alpha", 1.0),       # Mixup ì•ŒíŒŒê°’
                    use_mixup=cfg["train"].get("use_mixup", True)           # Mixup ì‚¬ìš© ì—¬ë¶€
                )

                # ê²€ì¦
                val_loss, val_f1, _, _ = validate_highperf(                 # ê³ ì„±ëŠ¥ ê²€ì¦ í•¨ìˆ˜ í˜¸ì¶œ
                    model, valid_ld, criterion, device, logger, wandb_logger, epoch  # ëª¨ë¸/ë°ì´í„°/ì†ì‹¤/ë””ë°”ì´ìŠ¤/ë¡œê±°/ì—í­
                ) 
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ìˆëŠ” ê²½ìš° ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                if scheduler:
                    scheduler.step()    # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í… ì‹¤í–‰
                
                # í˜„ì¬ F1ì´ ìµœê³  ê¸°ë¡ë³´ë‹¤ ë†’ì€ ê²½ìš° ìµœê³  ëª¨ë¸ ì €ì¥
                if val_f1 > best_f1:
                    best_f1 = val_f1                                    # ìµœê³  F1 ì ìˆ˜ ì—…ë°ì´íŠ¸
                    model_state = {                                     # ëª¨ë¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
                        'epoch': epoch,                                 # ì—í­ ë²ˆí˜¸
                        'model_state_dict': model.state_dict(),         # ëª¨ë¸ ê°€ì¤‘ì¹˜
                        'optimizer_state_dict': optimizer.state_dict(), # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ
                        'f1': val_f1,                                   # F1 ì ìˆ˜
                        'loss': val_loss,                               # ì†ì‹¤ê°’
                    }
                    torch.save(model_state, best_model_path)            # ë‚ ì§œ í´ë”ì— ëª¨ë¸ ì €ì¥
                    torch.save(model_state, lastest_best_model_path)     # lastest í´ë”ì—ë„ ëª¨ë¸ ì €ì¥
                    
                    # ìƒˆë¡œìš´ ìµœê³  ê¸°ë¡ ë¡œê·¸
                    logger.write(f"[FOLD {fold}] NEW BEST F1: {best_f1:.5f} (epoch {epoch})")
                
                # WandB ë¡œê¹…
                wandb_logger.log_metrics({          # ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ë¡œê¹…
                    "fold": fold,                   # í´ë“œ ë²ˆí˜¸
                    "epoch": epoch,                 # ì—í­ ë²ˆí˜¸
                    "train/loss": train_loss,       # í•™ìŠµ ì†ì‹¤
                    "val/loss": val_loss,           # ê²€ì¦ ì†ì‹¤
                    "val/f1": val_f1,               # ê²€ì¦ F1
                    "best_f1": best_f1              # ìµœê³  F1
                })
            
            # í´ë“œ ê²°ê³¼ ì €ì¥
            fold_results.append({                   # í´ë“œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                "fold": fold,                       # í´ë“œ ë²ˆí˜¸
                "best_f1": best_f1,                 # ìµœê³  F1 ì ìˆ˜
                "model_path": best_model_path       # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            })
            
            # í´ë“œ ì™„ë£Œ ë¡œê·¸
            logger.write(f"[FOLD {fold}] COMPLETED | Best F1: {best_f1:.5f}")
            # WandB ì¢…ë£Œ
            wandb_logger.finish()
        
        #------ --------------------- ì „ì²´ í´ë“œ ì™„ë£Œ í›„ ê²°ê³¼ ìš”ì•½ ---------------------- #
        avg_f1 = float(np.mean([r["best_f1"] for r in fold_results]))   # í‰ê·  F1 ê³„ì‚° (numpy floatë¥¼ Python floatë¡œ ë³€í™˜)
        logger.write(f"\n{'='*50}")                                     # ê²°ê³¼ êµ¬ë¶„ì„ 
        logger.write(f"ALL FOLDS COMPLETED")                            # ì „ì²´ í´ë“œ ì™„ë£Œ ë¡œê·¸
        logger.write(f"Average F1: {avg_f1:.5f}")                       # í‰ê·  F1 ë¡œê·¸
        for r in fold_results:                                          # ê° í´ë“œ ê²°ê³¼ ì¶œë ¥
            logger.write(f"Fold {r['fold']}: {r['best_f1']:.5f}")       # í´ë“œë³„ F1 ë¡œê·¸
        logger.write(f"{'='*50}")                                       # ê²°ê³¼ êµ¬ë¶„ì„ 
        
        # ---------------------- ê²°ê³¼ ì €ì¥ ---------------------- #
        results_path = os.path.join(exp_root, "fold_results.yaml")      # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        
        # ê²°ê³¼ë¥¼ YAMLë¡œ ì €ì¥
        dump_yaml({"fold_results": fold_results, "average_f1": avg_f1}, results_path)
        
        # lastest-train í´ë”ì—ë„ ê²°ê³¼ ì €ì¥
        lastest_results_path = os.path.join(lastest_exp_root, "fold_results.yaml")
        dump_yaml({"fold_results": fold_results, "average_f1": avg_f1}, lastest_results_path)
        
        # ---------------------- ì‹œê°í™” ìƒì„± ---------------------- #
        try:
            # ì‹œê°í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬ ë°ì´í„° ì¤€ë¹„
            # WandB ë¡œê·¸ì—ì„œ ê¸°ë¡ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ê¸°ë³¸ êµ¬ì¡° ìƒì„±
            history_data = {
                'train_loss': [],
                'val_loss': [],
                'val_f1': [],
                'epochs': list(range(1, cfg["train"]["epochs"] + 1))
            }
            
            # ì‹œê°í™” ìƒì„± - ë‹¤ì¤‘ ëª¨ë¸ì„ ê³ ë ¤í•œ ëª¨ë¸ëª… ì‚¬ìš©
            if is_multi_model_config(cfg):
                # ë‹¤ì¤‘ ëª¨ë¸ì¸ ê²½ìš° "multi-model-ensemble" ì´ë¦„ ì‚¬ìš©
                model_name_viz = "multi-model-ensemble"
            else:
                model_name_viz = cfg["model"]["name"]
            
            # fold_resultsë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            fold_results_dict = {
                'fold_results': fold_results,
                'average_f1': avg_f1,
                'total_folds': cfg["train"]["n_folds"]
            }
            
            visualize_training_pipeline(
                fold_results=fold_results_dict,
                model_name=model_name_viz,
                output_dir=exp_root,
                history_data=history_data
            )
            logger.write(f"[VIZ] Training visualizations created in {exp_root}")
            
        except Exception as viz_error:
            logger.write(f"[WARNING] Visualization failed: {str(viz_error)}")
        
        # ---------------------- lastest-train í´ë”ì— ë³µì‚¬ ---------------------- #
        # lastest-train í´ë” ê²½ë¡œ ì„¤ì •
        lastest_train_dir = os.path.join("experiments", "train", "lastest-train")
        experiment_folder_name = cfg["project"]["run_name"]     # ì‹¤í—˜ í´ë”ëª…
        lastest_train_model_path = os.path.join(lastest_train_dir, experiment_folder_name)
        
        # lastest-train ë””ë ‰í„°ë¦¬ ìƒì„±
        os.makedirs(lastest_train_dir, exist_ok=True)
        
        # ê¸°ì¡´ ëª¨ë¸ í´ë”ê°€ ìˆìœ¼ë©´ ì‚­ì œ (ë®ì–´ì“°ê¸°ë¥¼ ìœ„í•´)
        if os.path.exists(lastest_train_model_path):
            shutil.rmtree(lastest_train_model_path)
            logger.write(f"[CLEANUP] Removed existing lastest-train/{experiment_folder_name}")
        
        # í˜„ì¬ ì‹¤í—˜ ê²°ê³¼ë¥¼ lastest-trainìœ¼ë¡œ ë³µì‚¬
        shutil.copytree(exp_root, lastest_train_model_path)
        logger.write(f"[COPY] Results copied to lastest-train/{experiment_folder_name}")
        logger.write(f"ğŸ“ Latest results: {lastest_train_model_path}")
        
        # í•™ìŠµ ì„±ê³µ ë¡œê·¸
        logger.write(f"[SUCCESS] Training completed | avg_f1={avg_f1:.5f}")
    
    # ì˜ˆì™¸ ë°œìƒ ì‹œ ì²˜ë¦¬
    except Exception as e:
        logger.write(f"[ERROR] Training failed: {str(e)}")              # ì—ëŸ¬ ë¡œê·¸
        raise                                                           # ì˜ˆì™¸ ì¬ë°œìƒ
    # ì¢…ë£Œ ì²˜ë¦¬
    finally:
        logger.write("[SHUTDOWN] Training pipeline ended")              # íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ë¡œê·¸


# ---------------------- ë©”ì¸ ì‹¤í–‰ë¶€ ---------------------- #
if __name__ == "__main__":
    import sys      # sys ëª¨ë“ˆ import
    
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì ê°œìˆ˜ í™•ì¸
    if len(sys.argv) != 2:
        print("Usage: python train_highperf.py <config_path>")  # ì‚¬ìš©ë²• ì¶œë ¥
        sys.exit(1)                                             # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    
    run_highperf_training(sys.argv[1])                          # ê³ ì„±ëŠ¥ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
