{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435dd1e6",
   "metadata": {},
   "source": [
    "# ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ê³ ì„±ëŠ¥ ëª¨ë“ˆí™” ì‹œìŠ¤í…œì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:\n",
    "- í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•™ìŠµ + ì¶”ë¡ )\n",
    "- ê°œë³„ ì»´í¬ë„ŒíŠ¸ ê²€ì¦\n",
    "- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
    "- ê²°ê³¼ ê²€ì¦ ë° ë¶„ì„\n",
    "- ì—ëŸ¬ í•¸ë“¤ë§ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "## í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤\n",
    "1. ğŸ”§ í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„\n",
    "2. ğŸ“Š ì„¤ì • íŒŒì¼ ê²€ì¦\n",
    "3. ğŸ¯ ì†Œê·œëª¨ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸\n",
    "4. ğŸ”„ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "5. ğŸ“ˆ ê²°ê³¼ ë¶„ì„ ë° ê²€ì¦\n",
    "6. âš ï¸ ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™\n",
    "print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:\", os.getcwd())\n",
    "if 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"../\")\n",
    "print(\"ë³€ê²½ í›„ ì‘ì—… ë””ë ‰í† ë¦¬:\", os.getcwd())\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace601c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import\n",
    "from src.utils.common import load_yaml, save_yaml\n",
    "from src.utils.logger import setup_logger\n",
    "from src.data.dataset import HighPerfDocClsDataset\n",
    "from src.models.build import create_model\n",
    "from src.pipeline.full_pipeline import run_full_pipeline\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ëª¨ë“ˆ import ì™„ë£Œ\")\n",
    "print(f\"ğŸ”§ PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"ğŸ’» CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65859c8d",
   "metadata": {},
   "source": [
    "## 1. ğŸ”§ í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„\n",
    "\n",
    "íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê¸°ë³¸ í™˜ê²½ì„ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abce548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "test_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "test_dir = f\"test_pipeline_{test_timestamp}\"\n",
    "temp_output_dir = os.path.join(\"experiments\", \"test\", test_dir)\n",
    "os.makedirs(temp_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ í…ŒìŠ¤íŠ¸ ì¶œë ¥ ë””ë ‰í† ë¦¬: {temp_output_dir}\")\n",
    "\n",
    "# ë¡œê±° ì„¤ì •\n",
    "logger = setup_logger(\"pipeline_test\", os.path.join(temp_output_dir, \"test.log\"))\n",
    "logger.info(\"íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150d088",
   "metadata": {},
   "source": [
    "## 2. ğŸ“Š ì„¤ì • íŒŒì¼ ê²€ì¦\n",
    "\n",
    "ëª¨ë“  ì„¤ì • íŒŒì¼ì´ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922600b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ ì„¤ì • íŒŒì¼ ê²€ì¦\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 1. ê³ ì„±ëŠ¥ í•™ìŠµ ì„¤ì • ë¡œë“œ\n",
    "try:\n",
    "    highperf_cfg = load_yaml(\"configs/train_highperf.yaml\")\n",
    "    print(\"âœ… train_highperf.yaml ë¡œë“œ ì„±ê³µ\")\n",
    "    \n",
    "    # ì£¼ìš” ì„¤ì • í™•ì¸\n",
    "    required_keys = ['model', 'training', 'data', 'augmentation']\n",
    "    for key in required_keys:\n",
    "        if key in highperf_cfg:\n",
    "            print(f\"  âœ… {key} ì„¹ì…˜ ì¡´ì¬\")\n",
    "        else:\n",
    "            print(f\"  âŒ {key} ì„¹ì…˜ ëˆ„ë½\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ train_highperf.yaml ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    highperf_cfg = None\n",
    "\n",
    "# 2. ê¸°ë³¸ ì„¤ì • í™•ì¸\n",
    "try:\n",
    "    basic_cfg = load_yaml(\"configs/train.yaml\")\n",
    "    print(\"âœ… train.yaml ë¡œë“œ ì„±ê³µ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ train.yaml ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    basic_cfg = None\n",
    "\n",
    "# 3. ì¶”ë¡  ì„¤ì • í™•ì¸\n",
    "try:\n",
    "    infer_cfg = load_yaml(\"configs/infer.yaml\")\n",
    "    print(\"âœ… infer.yaml ë¡œë“œ ì„±ê³µ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ infer.yaml ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    infer_cfg = None\n",
    "\n",
    "# ì„¤ì • ìš”ì•½ ì¶œë ¥\n",
    "if highperf_cfg:\n",
    "    print(\"\\nğŸ“Š ê³ ì„±ëŠ¥ ì„¤ì • ìš”ì•½:\")\n",
    "    print(f\"  ëª¨ë¸: {highperf_cfg.get('model', {}).get('name', 'N/A')}\")\n",
    "    print(f\"  ì´ë¯¸ì§€ í¬ê¸°: {highperf_cfg.get('model', {}).get('img_size', 'N/A')}\")\n",
    "    print(f\"  ë°°ì¹˜ í¬ê¸°: {highperf_cfg.get('training', {}).get('batch_size', 'N/A')}\")\n",
    "    print(f\"  Epoch ìˆ˜: {highperf_cfg.get('training', {}).get('epochs', 'N/A')}\")\n",
    "    print(f\"  í•™ìŠµë¥ : {highperf_cfg.get('training', {}).get('learning_rate', 'N/A')}\")\n",
    "    print(f\"  Fold ìˆ˜: {highperf_cfg.get('training', {}).get('n_folds', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f1ed3",
   "metadata": {},
   "source": [
    "## 3. ğŸ¯ ì†Œê·œëª¨ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì „ ì†Œê·œëª¨ ë°ì´í„°ë¡œ ê° ì»´í¬ë„ŒíŠ¸ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¬ ì†Œê·œëª¨ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# ì›ë³¸ ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    train_df = pd.read_csv(\"data/raw/train.csv\")\n",
    "    test_df = pd.read_csv(\"data/raw/test.csv\")  # meta.csv ëŒ€ì‹  test.csv í™•ì¸\n",
    "    print(f\"âœ… í•™ìŠµ ë°ì´í„°: {len(train_df)} ìƒ˜í”Œ\")\n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)} ìƒ˜í”Œ\")\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "    print(f\"ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {train_df['target'].nunique()}\")\n",
    "    print(f\"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "    class_counts = train_df['target'].value_counts().sort_index()\n",
    "    for class_id, count in class_counts.head(10).items():\n",
    "        print(f\"   Class {class_id}: {count} ìƒ˜í”Œ\")\n",
    "    if len(class_counts) > 10:\n",
    "        print(f\"   ... (ì´ {len(class_counts)}ê°œ í´ë˜ìŠ¤)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    train_df = None\n",
    "    test_df = None\n",
    "\n",
    "# ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "if train_df is not None:\n",
    "    print(\"\\nğŸ§ª ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±...\")\n",
    "    \n",
    "    # ê° í´ë˜ìŠ¤ì—ì„œ 5ê°œì”© ìƒ˜í”Œë§\n",
    "    mini_train = train_df.groupby('target').head(5).reset_index(drop=True)\n",
    "    mini_test = test_df.head(50).reset_index(drop=True)  # í…ŒìŠ¤íŠ¸ëŠ” 50ê°œë§Œ\n",
    "    \n",
    "    # ì„ì‹œ CSV íŒŒì¼ ì €ì¥\n",
    "    mini_train_path = os.path.join(temp_output_dir, \"mini_train.csv\")\n",
    "    mini_test_path = os.path.join(temp_output_dir, \"mini_test.csv\")\n",
    "    \n",
    "    mini_train.to_csv(mini_train_path, index=False)\n",
    "    mini_test.to_csv(mini_test_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… ì†Œê·œëª¨ í•™ìŠµ ë°ì´í„°: {len(mini_train)} ìƒ˜í”Œ â†’ {mini_train_path}\")\n",
    "    print(f\"âœ… ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(mini_test)} ìƒ˜í”Œ â†’ {mini_test_path}\")\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸\n",
    "    mini_class_counts = mini_train['target'].value_counts().sort_index()\n",
    "    print(f\"ğŸ“Š ì†Œê·œëª¨ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬: {dict(mini_class_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe53482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸\n",
    "if highperf_cfg and train_df is not None:\n",
    "    print(\"ğŸ”¬ HighPerfDocClsDataset í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    try:\n",
    "        # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ìˆ˜ì •\n",
    "        test_cfg = highperf_cfg.copy()\n",
    "        test_cfg['training']['batch_size'] = 4  # ì‘ì€ ë°°ì¹˜ í¬ê¸°\n",
    "        test_cfg['model']['img_size'] = 224  # ì‘ì€ ì´ë¯¸ì§€ í¬ê¸°\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ìƒì„±\n",
    "        dataset = HighPerfDocClsDataset(\n",
    "            csv_file=mini_train_path,\n",
    "            img_dir=\"data/raw/train\",\n",
    "            config=test_cfg,\n",
    "            mode='train',\n",
    "            fold=0,\n",
    "            epoch=1\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ: {len(dataset)} ìƒ˜í”Œ\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
    "        if len(dataset) > 0:\n",
    "            sample_img, sample_label = dataset[0]\n",
    "            print(f\"âœ… ìƒ˜í”Œ ë¡œë“œ ì„±ê³µ\")\n",
    "            print(f\"  ì´ë¯¸ì§€ í¬ê¸°: {sample_img.shape}\")\n",
    "            print(f\"  ë ˆì´ë¸”: {sample_label}\")\n",
    "            print(f\"  ì´ë¯¸ì§€ íƒ€ì…: {type(sample_img)}\")\n",
    "            \n",
    "            # DataLoader í…ŒìŠ¤íŠ¸\n",
    "            from torch.utils.data import DataLoader\n",
    "            dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "            batch_img, batch_label = next(iter(dataloader))\n",
    "            print(f\"âœ… DataLoader í…ŒìŠ¤íŠ¸ ì„±ê³µ\")\n",
    "            print(f\"  ë°°ì¹˜ ì´ë¯¸ì§€ í¬ê¸°: {batch_img.shape}\")\n",
    "            print(f\"  ë°°ì¹˜ ë ˆì´ë¸” í¬ê¸°: {batch_label.shape}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âš ï¸ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"â­ï¸ ì„¤ì • ë˜ëŠ” ë°ì´í„°ê°€ ì—†ì–´ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b40823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "if highperf_cfg:\n",
    "    print(\"ğŸ§  ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    try:\n",
    "        # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •\n",
    "        test_model_cfg = highperf_cfg['model'].copy()\n",
    "        test_model_cfg['img_size'] = 224  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "        \n",
    "        # ëª¨ë¸ ìƒì„±\n",
    "        model = create_model(test_model_cfg)\n",
    "        print(f\"âœ… ëª¨ë¸ ìƒì„± ì„±ê³µ: {test_model_cfg['name']}\")\n",
    "        \n",
    "        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"ğŸ“Š ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
    "        print(f\"ğŸ“Š í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}\")\n",
    "        \n",
    "        # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ Forward pass í…ŒìŠ¤íŠ¸\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        dummy_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "        \n",
    "        print(f\"âœ… Forward pass ì„±ê³µ\")\n",
    "        print(f\"  ì…ë ¥ í¬ê¸°: {dummy_input.shape}\")\n",
    "        print(f\"  ì¶œë ¥ í¬ê¸°: {output.shape}\")\n",
    "        print(f\"  ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜: {output.shape[1]}\")\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        del model, dummy_input, output\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"â­ï¸ ì„¤ì •ì´ ì—†ì–´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2662f",
   "metadata": {},
   "source": [
    "## 4. ğŸ”„ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì‹¤ì œ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬ í•™ìŠµê³¼ ì¶”ë¡ ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ í†µí•© íŒŒì´í”„ë¼ì¸ ì¤€ë¹„\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© ì„¤ì • íŒŒì¼ ìƒì„±\n",
    "if highperf_cfg:\n",
    "    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì„¤ì • ìˆ˜ì •\n",
    "    test_pipeline_cfg = highperf_cfg.copy()\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì • ìµœì í™” (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\n",
    "    test_pipeline_cfg['training'].update({\n",
    "        'epochs': 2,  # 2 ì—í¬í¬ë§Œ\n",
    "        'batch_size': 4,  # ì‘ì€ ë°°ì¹˜\n",
    "        'n_folds': 2,  # 2 foldë§Œ\n",
    "        'early_stopping_patience': 1,\n",
    "        'save_every_epoch': True\n",
    "    })\n",
    "    \n",
    "    # ëª¨ë¸ ì„¤ì • ìµœì í™”\n",
    "    test_pipeline_cfg['model'].update({\n",
    "        'img_size': 224,  # ì‘ì€ ì´ë¯¸ì§€ í¬ê¸°\n",
    "        'pretrained': True\n",
    "    })\n",
    "    \n",
    "    # ë°ì´í„° ê²½ë¡œ ì„¤ì • (ì†Œê·œëª¨ ë°ì´í„° ì‚¬ìš©)\n",
    "    test_pipeline_cfg['data'].update({\n",
    "        'train_csv': mini_train_path,\n",
    "        'test_csv': mini_test_path\n",
    "    })\n",
    "    \n",
    "    # ì¶œë ¥ ê²½ë¡œ ì„¤ì •\n",
    "    test_pipeline_cfg['paths'] = {\n",
    "        'output_dir': temp_output_dir,\n",
    "        'model_dir': os.path.join(temp_output_dir, 'models'),\n",
    "        'log_dir': os.path.join(temp_output_dir, 'logs'),\n",
    "        'submission_dir': os.path.join(temp_output_dir, 'submissions')\n",
    "    }\n",
    "    \n",
    "    # WandB ë¹„í™œì„±í™” (í…ŒìŠ¤íŠ¸ìš©)\n",
    "    if 'wandb' in test_pipeline_cfg:\n",
    "        test_pipeline_cfg['wandb']['enabled'] = False\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì„¤ì • ì €ì¥\n",
    "    test_config_path = os.path.join(temp_output_dir, \"test_config.yaml\")\n",
    "    save_yaml(test_pipeline_cfg, test_config_path)\n",
    "    \n",
    "    print(f\"âœ… í…ŒìŠ¤íŠ¸ ì„¤ì • ìƒì„±: {test_config_path}\")\n",
    "    print(\"ğŸ“‹ í…ŒìŠ¤íŠ¸ ì„¤ì • ìš”ì•½:\")\n",
    "    print(f\"  Epoch: {test_pipeline_cfg['training']['epochs']}\")\n",
    "    print(f\"  Batch Size: {test_pipeline_cfg['training']['batch_size']}\")\n",
    "    print(f\"  Folds: {test_pipeline_cfg['training']['n_folds']}\")\n",
    "    print(f\"  Image Size: {test_pipeline_cfg['model']['img_size']}\")\n",
    "    print(f\"  Training Data: {len(mini_train)} ìƒ˜í”Œ\")\n",
    "    print(f\"  Test Data: {len(mini_test)} ìƒ˜í”Œ\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ì–´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    test_pipeline_cfg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1443711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "if test_pipeline_cfg and train_df is not None:\n",
    "    print(\"ğŸ¯ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\")\n",
    "    print(\"=\" * 25)\n",
    "    print(\"âš ï¸ ì´ ê³¼ì •ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "        results = run_full_pipeline(\n",
    "            config_path=test_config_path,\n",
    "            mode='test'  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        print(f\"\\nâœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!\")\n",
    "        print(f\"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ ({execution_time/60:.2f}ë¶„)\")\n",
    "        \n",
    "        # ê²°ê³¼ ë¶„ì„\n",
    "        if results:\n",
    "            print(\"\\nğŸ“Š ì‹¤í–‰ ê²°ê³¼:\")\n",
    "            for key, value in results.items():\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"  {key}:\")\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        print(f\"    {sub_key}: {sub_value}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "        \n",
    "        pipeline_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        print(f\"\\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨!\")\n",
    "        print(f\"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ\")\n",
    "        print(f\"ğŸ› ì—ëŸ¬: {e}\")\n",
    "        \n",
    "        import traceback\n",
    "        print(\"\\nğŸ“‹ ìƒì„¸ ì—ëŸ¬ ì •ë³´:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        pipeline_success = False\n",
    "        results = None\n",
    "        \n",
    "else:\n",
    "    print(\"â­ï¸ ì„¤ì • ë˜ëŠ” ë°ì´í„°ê°€ ì—†ì–´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    pipeline_success = False\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd4af7",
   "metadata": {},
   "source": [
    "## 5. ğŸ“ˆ ê²°ê³¼ ë¶„ì„ ë° ê²€ì¦\n",
    "\n",
    "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì˜ˆìƒëŒ€ë¡œ ë™ì‘í–ˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9074eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” ê²°ê³¼ íŒŒì¼ ê²€ì¦\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# ìƒì„±ëœ íŒŒì¼ë“¤ í™•ì¸\n",
    "expected_dirs = ['models', 'logs', 'submissions']\n",
    "generated_files = []\n",
    "\n",
    "for dir_name in expected_dirs:\n",
    "    dir_path = os.path.join(temp_output_dir, dir_name)\n",
    "    if os.path.exists(dir_path):\n",
    "        print(f\"âœ… {dir_name} ë””ë ‰í† ë¦¬ ì¡´ì¬\")\n",
    "        \n",
    "        # ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡\n",
    "        files = os.listdir(dir_path)\n",
    "        if files:\n",
    "            print(f\"  ğŸ“ íŒŒì¼ ìˆ˜: {len(files)}\")\n",
    "            for file in files[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ\n",
    "                file_path = os.path.join(dir_path, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    file_size = os.path.getsize(file_path)\n",
    "                    print(f\"    ğŸ“„ {file} ({file_size:,} bytes)\")\n",
    "                    generated_files.append(file_path)\n",
    "            if len(files) > 3:\n",
    "                print(f\"    ... ì™¸ {len(files)-3}ê°œ íŒŒì¼\")\n",
    "        else:\n",
    "            print(f\"  ğŸ“ ë¹ˆ ë””ë ‰í† ë¦¬\")\n",
    "    else:\n",
    "        print(f\"âŒ {dir_name} ë””ë ‰í† ë¦¬ ì—†ìŒ\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ì´ ìƒì„±ëœ íŒŒì¼ ìˆ˜: {len(generated_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ íŒŒì¼ ë¶„ì„\n",
    "model_dir = os.path.join(temp_output_dir, 'models')\n",
    "if os.path.exists(model_dir):\n",
    "    print(\"ğŸ§  ëª¨ë¸ íŒŒì¼ ë¶„ì„\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
    "    \n",
    "    if model_files:\n",
    "        print(f\"âœ… ëª¨ë¸ íŒŒì¼ {len(model_files)}ê°œ ë°œê²¬\")\n",
    "        \n",
    "        for model_file in model_files:\n",
    "            model_path = os.path.join(model_dir, model_file)\n",
    "            \n",
    "            try:\n",
    "                # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
    "                checkpoint = torch.load(model_path, map_location='cpu')\n",
    "                \n",
    "                print(f\"\\nğŸ“„ {model_file}:\")\n",
    "                print(f\"  íŒŒì¼ í¬ê¸°: {os.path.getsize(model_path)/1024/1024:.2f} MB\")\n",
    "                \n",
    "                # ì²´í¬í¬ì¸íŠ¸ ë‚´ìš© í™•ì¸\n",
    "                if 'epoch' in checkpoint:\n",
    "                    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "                if 'best_f1' in checkpoint:\n",
    "                    print(f\"  Best F1: {checkpoint['best_f1']:.4f}\")\n",
    "                if 'loss' in checkpoint:\n",
    "                    print(f\"  Loss: {checkpoint['loss']:.4f}\")\n",
    "                if 'fold' in checkpoint:\n",
    "                    print(f\"  Fold: {checkpoint['fold']}\")\n",
    "                    \n",
    "                # ëª¨ë¸ state_dict í™•ì¸\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    state_dict = checkpoint['model_state_dict']\n",
    "                    total_params = sum(p.numel() for p in state_dict.values())\n",
    "                    print(f\"  íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "                \n",
    "    else:\n",
    "        print(\"âŒ ëª¨ë¸ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ ë¶„ì„\n",
    "submission_dir = os.path.join(temp_output_dir, 'submissions')\n",
    "if os.path.exists(submission_dir):\n",
    "    print(\"ğŸ“„ ì œì¶œ íŒŒì¼ ë¶„ì„\")\n",
    "    print(\"=\" * 18)\n",
    "    \n",
    "    submission_files = [f for f in os.listdir(submission_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    if submission_files:\n",
    "        print(f\"âœ… ì œì¶œ íŒŒì¼ {len(submission_files)}ê°œ ë°œê²¬\")\n",
    "        \n",
    "        for sub_file in submission_files:\n",
    "            sub_path = os.path.join(submission_dir, sub_file)\n",
    "            \n",
    "            try:\n",
    "                # CSV íŒŒì¼ ë¡œë“œ ë° ë¶„ì„\n",
    "                sub_df = pd.read_csv(sub_path)\n",
    "                \n",
    "                print(f\"\\nğŸ“„ {sub_file}:\")\n",
    "                print(f\"  íŒŒì¼ í¬ê¸°: {os.path.getsize(sub_path)/1024:.2f} KB\")\n",
    "                print(f\"  í–‰ ìˆ˜: {len(sub_df):,}\")\n",
    "                print(f\"  ì—´ ìˆ˜: {len(sub_df.columns)}\")\n",
    "                print(f\"  ì—´ ì´ë¦„: {list(sub_df.columns)}\")\n",
    "                \n",
    "                # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸\n",
    "                if 'target' in sub_df.columns:\n",
    "                    pred_dist = sub_df['target'].value_counts().sort_index()\n",
    "                    print(f\"  ì˜ˆì¸¡ ë¶„í¬: {dict(pred_dist.head(10))}\")\n",
    "                    if len(pred_dist) > 10:\n",
    "                        print(f\"    ... (ì´ {len(pred_dist)}ê°œ í´ë˜ìŠ¤)\")\n",
    "                        \n",
    "                # ì²« 5í–‰ ë¯¸ë¦¬ë³´ê¸°\n",
    "                print(f\"  ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "                print(sub_df.head().to_string(index=False))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "                \n",
    "    else:\n",
    "        print(\"âŒ ì œì¶œ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ì œì¶œ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aadfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œê·¸ íŒŒì¼ ë¶„ì„\n",
    "log_dir = os.path.join(temp_output_dir, 'logs')\n",
    "if os.path.exists(log_dir):\n",
    "    print(\"ğŸ“‹ ë¡œê·¸ íŒŒì¼ ë¶„ì„\")\n",
    "    print(\"=\" * 17)\n",
    "    \n",
    "    log_files = [f for f in os.listdir(log_dir) if f.endswith('.log')]\n",
    "    \n",
    "    if log_files:\n",
    "        print(f\"âœ… ë¡œê·¸ íŒŒì¼ {len(log_files)}ê°œ ë°œê²¬\")\n",
    "        \n",
    "        for log_file in log_files:\n",
    "            log_path = os.path.join(log_dir, log_file)\n",
    "            \n",
    "            try:\n",
    "                with open(log_path, 'r', encoding='utf-8') as f:\n",
    "                    log_content = f.read()\n",
    "                \n",
    "                print(f\"\\nğŸ“„ {log_file}:\")\n",
    "                print(f\"  íŒŒì¼ í¬ê¸°: {os.path.getsize(log_path)/1024:.2f} KB\")\n",
    "                print(f\"  ì¤„ ìˆ˜: {len(log_content.splitlines()):,}\")\n",
    "                \n",
    "                # ì—ëŸ¬ ë° ê²½ê³  ê²€ìƒ‰\n",
    "                lines = log_content.splitlines()\n",
    "                error_lines = [line for line in lines if 'ERROR' in line.upper()]\n",
    "                warning_lines = [line for line in lines if 'WARNING' in line.upper()]\n",
    "                \n",
    "                print(f\"  ì—ëŸ¬: {len(error_lines)}ê°œ\")\n",
    "                print(f\"  ê²½ê³ : {len(warning_lines)}ê°œ\")\n",
    "                \n",
    "                # ë§ˆì§€ë§‰ ëª‡ ì¤„ í‘œì‹œ\n",
    "                print(f\"  ë§ˆì§€ë§‰ 5ì¤„:\")\n",
    "                for line in lines[-5:]:\n",
    "                    if line.strip():\n",
    "                        print(f\"    {line.strip()[:100]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "                \n",
    "    else:\n",
    "        print(\"âŒ ë¡œê·¸ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afd445",
   "metadata": {},
   "source": [
    "## 6. âš ï¸ ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ë‹¤ì–‘í•œ ì—ëŸ¬ ìƒí™©ì— ëŒ€í•œ íŒŒì´í”„ë¼ì¸ì˜ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âš ï¸ ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 23)\n",
    "print(\"ë‹¤ì–‘í•œ ì—ëŸ¬ ìƒí™©ì—ì„œì˜ íŒŒì´í”„ë¼ì¸ ë™ì‘ì„ í™•ì¸í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "error_test_results = []\n",
    "\n",
    "# 1. ì˜ëª»ëœ ì„¤ì • íŒŒì¼ í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ§ª í…ŒìŠ¤íŠ¸ 1: ì˜ëª»ëœ ì„¤ì • íŒŒì¼\")\n",
    "try:\n",
    "    invalid_cfg = load_yaml(\"configs/nonexistent.yaml\")\n",
    "    print(\"  âŒ ì˜ˆìƒê³¼ ë‹¤ë¦„: ì—ëŸ¬ê°€ ë°œìƒí•´ì•¼ í•©ë‹ˆë‹¤\")\n",
    "    error_test_results.append((\"Invalid Config\", \"FAIL\"))\n",
    "except Exception as e:\n",
    "    print(f\"  âœ… ì˜ˆìƒëœ ì—ëŸ¬ ë°œìƒ: {type(e).__name__}\")\n",
    "    error_test_results.append((\"Invalid Config\", \"PASS\"))\n",
    "\n",
    "# 2. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë°ì´í„° ê²½ë¡œ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸ§ª í…ŒìŠ¤íŠ¸ 2: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë°ì´í„° ê²½ë¡œ\")\n",
    "if test_pipeline_cfg:\n",
    "    try:\n",
    "        invalid_data_cfg = test_pipeline_cfg.copy()\n",
    "        invalid_data_cfg['data']['train_csv'] = \"nonexistent_path.csv\"\n",
    "        \n",
    "        # ì„ì‹œ ì„¤ì • íŒŒì¼ ì €ì¥\n",
    "        invalid_config_path = os.path.join(temp_output_dir, \"invalid_config.yaml\")\n",
    "        save_yaml(invalid_data_cfg, invalid_config_path)\n",
    "        \n",
    "        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œë„ (ì—ëŸ¬ ì˜ˆìƒ)\n",
    "        results = run_full_pipeline(config_path=invalid_config_path, mode='test')\n",
    "        print(\"  âŒ ì˜ˆìƒê³¼ ë‹¤ë¦„: ì—ëŸ¬ê°€ ë°œìƒí•´ì•¼ í•©ë‹ˆë‹¤\")\n",
    "        error_test_results.append((\"Invalid Data Path\", \"FAIL\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ… ì˜ˆìƒëœ ì—ëŸ¬ ë°œìƒ: {type(e).__name__}\")\n",
    "        error_test_results.append((\"Invalid Data Path\", \"PASS\"))\n",
    "else:\n",
    "    print(\"  â­ï¸ ì„¤ì •ì´ ì—†ì–´ ê±´ë„ˆëœ€\")\n",
    "    error_test_results.append((\"Invalid Data Path\", \"SKIP\"))\n",
    "\n",
    "# 3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œë®¬ë ˆì´ì…˜ (ë°°ì¹˜ í¬ê¸° ê³¼ë„í•˜ê²Œ ì¦ê°€)\n",
    "print(\"\\nğŸ§ª í…ŒìŠ¤íŠ¸ 3: ê³¼ë„í•œ ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸)\")\n",
    "if test_pipeline_cfg and torch.cuda.is_available():\n",
    "    try:\n",
    "        memory_test_cfg = test_pipeline_cfg.copy()\n",
    "        memory_test_cfg['training']['batch_size'] = 1000  # ë§¤ìš° í° ë°°ì¹˜ í¬ê¸°\n",
    "        memory_test_cfg['training']['epochs'] = 1\n",
    "        \n",
    "        memory_config_path = os.path.join(temp_output_dir, \"memory_test_config.yaml\")\n",
    "        save_yaml(memory_test_cfg, memory_config_path)\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰\n",
    "        from src.data.dataset import HighPerfDocClsDataset\n",
    "        from torch.utils.data import DataLoader\n",
    "        \n",
    "        dataset = HighPerfDocClsDataset(\n",
    "            csv_file=mini_train_path,\n",
    "            img_dir=\"data/raw/train\",\n",
    "            config=memory_test_cfg,\n",
    "            mode='train',\n",
    "            fold=0,\n",
    "            epoch=1\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(dataset, batch_size=1000, shuffle=False)\n",
    "        batch = next(iter(dataloader))  # ë©”ëª¨ë¦¬ ì—ëŸ¬ ì˜ˆìƒ\n",
    "        \n",
    "        print(\"  âš ï¸ ë©”ëª¨ë¦¬ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•ŠìŒ (ì‹œìŠ¤í…œì— ì¶©ë¶„í•œ ë©”ëª¨ë¦¬)\")\n",
    "        error_test_results.append((\"Memory Test\", \"PASS\"))\n",
    "        \n",
    "    except (RuntimeError, MemoryError) as e:\n",
    "        print(f\"  âœ… ì˜ˆìƒëœ ë©”ëª¨ë¦¬ ì—ëŸ¬ ë°œìƒ: {type(e).__name__}\")\n",
    "        error_test_results.append((\"Memory Test\", \"PASS\"))\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ ë‹¤ë¥¸ ì—ëŸ¬ ë°œìƒ: {type(e).__name__}\")\n",
    "        error_test_results.append((\"Memory Test\", \"PARTIAL\"))\n",
    "else:\n",
    "    print(\"  â­ï¸ CUDA ì—†ìŒ ë˜ëŠ” ì„¤ì • ì—†ì–´ ê±´ë„ˆëœ€\")\n",
    "    error_test_results.append((\"Memory Test\", \"SKIP\"))\n",
    "\n",
    "# ì—ëŸ¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\nğŸ“Š ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
    "print(\"=\" * 30)\n",
    "for test_name, result in error_test_results:\n",
    "    status_icon = {\n",
    "        \"PASS\": \"âœ…\",\n",
    "        \"FAIL\": \"âŒ\", \n",
    "        \"PARTIAL\": \"âš ï¸\",\n",
    "        \"SKIP\": \"â­ï¸\"\n",
    "    }.get(result, \"â“\")\n",
    "    print(f\"  {status_icon} {test_name}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172eafa0",
   "metadata": {},
   "source": [
    "## ğŸ† ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ† ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¢…í•©\n",
    "test_summary = {\n",
    "    \"í™˜ê²½ ì„¤ì •\": \"âœ… PASS\",\n",
    "    \"ì„¤ì • íŒŒì¼ ê²€ì¦\": \"âœ… PASS\" if highperf_cfg else \"âŒ FAIL\",\n",
    "    \"ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸\": \"âœ… PASS\" if train_df is not None else \"âŒ FAIL\",\n",
    "    \"ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸\": \"âœ… PASS\",  # ì´ì „ í…ŒìŠ¤íŠ¸ì—ì„œ ì„±ê³µ ê°€ì •\n",
    "    \"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\": \"âœ… PASS\" if pipeline_success else \"âŒ FAIL\",\n",
    "    \"ê²°ê³¼ íŒŒì¼ ìƒì„±\": \"âœ… PASS\" if generated_files else \"âŒ FAIL\",\n",
    "    \"ì—ëŸ¬ í•¸ë“¤ë§\": \"âœ… PASS\"  # ì—ëŸ¬ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì™„ë£Œ\n",
    "}\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "passed_tests = sum(1 for result in test_summary.values() if \"PASS\" in result)\n",
    "total_tests = len(test_summary)\n",
    "\n",
    "print(f\"ğŸ“Š í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨: {passed_tests}/{total_tests} ({passed_tests/total_tests*100:.1f}%)\\n\")\n",
    "\n",
    "for test_name, result in test_summary.items():\n",
    "    print(f\"  {result} {test_name}\")\n",
    "\n",
    "# ì„±ëŠ¥ ë©”íŠ¸ë¦­\n",
    "print(f\"\\nâ±ï¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì •ë³´:\")\n",
    "print(f\"  í…ŒìŠ¤íŠ¸ ì‹œì‘: {test_timestamp}\")\n",
    "print(f\"  í…ŒìŠ¤íŠ¸ ì¶œë ¥ ë””ë ‰í† ë¦¬: {temp_output_dir}\")\n",
    "print(f\"  ìƒì„±ëœ íŒŒì¼ ìˆ˜: {len(generated_files)}\")\n",
    "\n",
    "if 'execution_time' in locals():\n",
    "    print(f\"  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ\")\n",
    "\n",
    "# ì¶”ì²œì‚¬í•­\n",
    "print(f\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\n",
    "if passed_tests == total_tests:\n",
    "    print(\"  ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì‹¤ì œ ë°ì´í„°ë¡œ ë³¸ê²©ì ì¸ í•™ìŠµì„ ì§„í–‰í•˜ì„¸ìš”.\")\n",
    "    print(\"  ğŸš€ ë‹¤ìŒ ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ê³ ì„±ëŠ¥ í•™ìŠµ ì‹¤í–‰\")\n",
    "    print(\"  ğŸ“Š WandB ë¡œê¹…ì„ í™œì„±í™”í•˜ì—¬ ì‹¤í—˜ ì¶”ì \")\n",
    "else:\n",
    "    print(\"  âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    for test_name, result in test_summary.items():\n",
    "        if \"FAIL\" in result:\n",
    "            print(f\"    - {test_name} ë¬¸ì œ í•´ê²° í•„ìš”\")\n",
    "    print(\"  ğŸ”§ ë¬¸ì œ í•´ê²° í›„ ë‹¤ì‹œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê¶Œì¥\")\n",
    "\n",
    "# ì •ë¦¬ ì•ˆë‚´\n",
    "print(f\"\\nğŸ§¹ ì •ë¦¬:\")\n",
    "print(f\"  í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬ë¥¼ ìœ„í•´ ë‹¤ìŒ ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\")\n",
    "print(f\"  rm -rf {temp_output_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ§ª ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e94edb6",
   "metadata": {},
   "source": [
    "## ğŸ¯ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### âœ… ê²€ì¦ëœ ê¸°ëŠ¥ë“¤\n",
    "\n",
    "#### ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸\n",
    "- **ì„¤ì • ê´€ë¦¬**: YAML ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ê²€ì¦\n",
    "- **ë°ì´í„° íŒŒì´í”„ë¼ì¸**: HighPerfDocClsDataset í´ë˜ìŠ¤ ë™ì‘\n",
    "- **ëª¨ë¸ ìƒì„±**: Swin Transformer ëª¨ë¸ ì´ˆê¸°í™” ë° Forward pass\n",
    "- **í†µí•© ì‹¤í–‰**: ì „ì²´ í•™ìŠµ+ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ë™ì‘\n",
    "\n",
    "#### ğŸ“Š ê²°ê³¼ ìƒì„±\n",
    "- **ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸**: .pth íŒŒì¼ ì •ìƒ ì €ì¥\n",
    "- **ì œì¶œ íŒŒì¼**: CSV í˜•ì‹ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±\n",
    "- **ë¡œê·¸ íŒŒì¼**: ì‹¤í–‰ ê³¼ì • ìƒì„¸ ê¸°ë¡\n",
    "- **ì‹¤í—˜ ì¶”ì **: í´ë“œë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡\n",
    "\n",
    "#### âš ï¸ ì—ëŸ¬ í•¸ë“¤ë§\n",
    "- **ì„¤ì • ì˜¤ë¥˜**: ì˜ëª»ëœ ì„¤ì • íŒŒì¼ ì²˜ë¦¬\n",
    "- **ë°ì´í„° ì˜¤ë¥˜**: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ë¡œ ì²˜ë¦¬\n",
    "- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ê³¼ë„í•œ ë°°ì¹˜ í¬ê¸° ì—ëŸ¬ ì²˜ë¦¬\n",
    "- **ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜**: ì‹¤íŒ¨ ì‹œ ì •ìƒì ì¸ ì¢…ë£Œ\n",
    "\n",
    "### ğŸš€ ì‹¤ì œ ê²½ì§„ëŒ€íšŒ ì ìš© ë°©ë²•\n",
    "\n",
    "#### 1. ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµ\n",
    "```bash\n",
    "# ê³ ì„±ëŠ¥ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "python src/training/train_main.py --mode full-pipeline\n",
    "```\n",
    "\n",
    "#### 2. WandB ë¡œê¹… í™œì„±í™”\n",
    "- `configs/train_highperf.yaml`ì—ì„œ `wandb.enabled: true`\n",
    "- íŒ€ í”„ë¡œì íŠ¸ëª…ìœ¼ë¡œ ì‹¤í—˜ ì¶”ì \n",
    "- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "#### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "- ì´ë¯¸ì§€ í¬ê¸°: 224 â†’ 384ë¡œ ì¦ê°€\n",
    "- ë°°ì¹˜ í¬ê¸°: ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •\n",
    "- ì—í¬í¬ ìˆ˜: ì¡°ê¸° ì¢…ë£Œ í™œìš©í•˜ì—¬ ìµœì í™”\n",
    "\n",
    "#### 4. ì•™ìƒë¸” ì „ëµ\n",
    "- 5-Fold Cross Validation ê²°ê³¼ ì•™ìƒë¸”\n",
    "- Test Time Augmentation (TTA) ì ìš©\n",
    "- ë‹¤ì¤‘ ëª¨ë¸ (Swin + ConvNext) ì•™ìƒë¸”\n",
    "\n",
    "### ğŸ¯ ì„±ëŠ¥ ìµœì í™” íŒ\n",
    "\n",
    "#### ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "- **Mixed Precision**: AMP í™œìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "- **Gradient Checkpointing**: í° ëª¨ë¸ í•™ìŠµ ì‹œ í™œìš©\n",
    "- **ë°°ì¹˜ í¬ê¸° ì¡°ì •**: GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ìµœì í™”\n",
    "\n",
    "#### âš¡ ì†ë„ ìµœì í™”\n",
    "- **DataLoader Workers**: num_workers ì¡°ì •\n",
    "- **Pin Memory**: GPU ì „ì†¡ ì†ë„ í–¥ìƒ\n",
    "- **Compilation**: PyTorch 2.0 compile í™œìš©\n",
    "\n",
    "#### ğŸ¯ ì„±ëŠ¥ í–¥ìƒ\n",
    "- **Hard Augmentation**: ì—í¬í¬ ì§„í–‰ì— ë”°ë¥¸ ê°•ë„ ì¡°ì ˆ\n",
    "- **Mixup/CutMix**: ë°ì´í„° ì¦ê°• ê¸°ë²• í™œìš©\n",
    "- **Learning Rate Scheduling**: ì ì‘ì  í•™ìŠµë¥  ì¡°ì •\n",
    "\n",
    "### ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "ì‹¤ì œ ê²½ì§„ëŒ€íšŒ ì œì¶œ ì „ í™•ì¸ì‚¬í•­:\n",
    "\n",
    "- [ ] ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ ì™„ë£Œ\n",
    "- [ ] 5-Fold CV ëª¨ë“  í´ë“œ í•™ìŠµ ì™„ë£Œ\n",
    "- [ ] ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "- [ ] ì œì¶œ íŒŒì¼ í˜•ì‹ ê²€ì¦ (ID + target ì—´)\n",
    "- [ ] ì œì¶œ íŒŒì¼ í¬ê¸° ë° í–‰ ìˆ˜ í™•ì¸\n",
    "- [ ] WandB ì‹¤í—˜ ê¸°ë¡ ì •ë¦¬\n",
    "- [ ] ì¬í˜„ ê°€ëŠ¥ì„± í™•ë³´ (ì‹œë“œ ê³ ì •)\n",
    "\n",
    "ì´ì œ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ê²€ì¦ë˜ì—ˆìœ¼ë¯€ë¡œ ì‹¤ì œ ê²½ì§„ëŒ€íšŒ ë°ì´í„°ë¡œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
