{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22429e1",
   "metadata": {},
   "source": [
    "# ğŸ§ª Swin Transformer ëª¨ë¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Swin Transformer ëª¨ë¸ì˜ ë™ì‘ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:\n",
    "- ëª¨ë¸ ë¡œë”© ë° ì•„í‚¤í…ì²˜ í™•ì¸\n",
    "- Forward pass í…ŒìŠ¤íŠ¸\n",
    "- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •\n",
    "- ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬\n",
    "- ë‹¤ë¥¸ ëª¨ë¸ê³¼ì˜ ë¹„êµ\n",
    "\n",
    "## í…ŒìŠ¤íŠ¸ í•­ëª©\n",
    "1. Swin Transformer ëª¨ë¸ ë¡œë”©\n",
    "2. ëª¨ë¸ êµ¬ì¡° ë° íŒŒë¼ë¯¸í„° ë¶„ì„\n",
    "3. Forward pass ë™ì‘ í™•ì¸\n",
    "4. ë©”ëª¨ë¦¬ ë° ì†ë„ ë²¤ì¹˜ë§ˆí¬\n",
    "5. EfficientNetê³¼ ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™\n",
    "print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:\", os.getcwd())\n",
    "if 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"../../\")\n",
    "print(\"ë³€ê²½ í›„ ì‘ì—… ë””ë ‰í† ë¦¬:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import psutil\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import\n",
    "from src.models.build import build_model, get_recommended_model, RECOMMENDED_MODELS\n",
    "from src.data.dataset import HighPerfDocClsDataset\n",
    "from src.utils.common import load_yaml\n",
    "\n",
    "# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë¡œê±° ì´ˆê¸°í™”\n",
    "from src.utils.unit_test_logger import create_test_logger\n",
    "test_logger = create_test_logger(\"swin_model\")\n",
    "test_logger.log_info(\"Swin Transformer ëª¨ë¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "\n",
    "with test_logger.capture_output(\"environment_setup\") as (output, error):\n",
    "    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"ğŸ“‹ í™˜ê²½ ì„¤ì • ì™„ë£Œ\")\n",
    "    print(f\"ğŸ”§ PyTorch ë²„ì „: {torch.__version__}\")\n",
    "    print(f\"ğŸ’» CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ¯ GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# í™˜ê²½ ì •ë³´ ì €ì¥\n",
    "env_info = {\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"device_name\": torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\",\n",
    "    \"gpu_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "}\n",
    "\n",
    "test_logger.save_test_result(\"environment_setup\", {\n",
    "    \"status\": \"success\",\n",
    "    \"environment\": env_info\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba53ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì • ë¡œë“œ\n",
    "cfg = load_yaml(\"configs/train_highperf.yaml\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"ğŸ“‹ ì„¤ì • ì •ë³´:\")\n",
    "print(f\"ğŸ¯ ëª¨ë¸: {cfg['model']['name']}\")\n",
    "print(f\"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {cfg['train']['img_size']}\")\n",
    "print(f\"ğŸ¯ í´ë˜ìŠ¤ ìˆ˜: {cfg['data']['num_classes']}\")\n",
    "print(f\"ğŸ’» ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# ì¶”ì²œ ëª¨ë¸ ëª©ë¡ í™•ì¸\n",
    "print(f\"\\nğŸ† ì‚¬ìš© ê°€ëŠ¥í•œ ê³ ì„±ëŠ¥ ëª¨ë¸ë“¤:\")\n",
    "for key, model_name in RECOMMENDED_MODELS.items():\n",
    "    print(f\"  {key}: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ec8c4",
   "metadata": {},
   "source": [
    "## 1. Swin Transformer ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model):\n",
    "    \"\"\"ëª¨ë¸ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return {\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params,\n",
    "        'model_size_mb': total_params * 4 / (1024 * 1024)  # float32 ê¸°ì¤€\n",
    "    }\n",
    "\n",
    "# Swin Transformer ëª¨ë¸ ë¡œë”©\n",
    "print(\"ğŸ—ï¸ Swin Transformer ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "\n",
    "try:\n",
    "    swin_model_name = get_recommended_model(\"swin_base_384\")\n",
    "    print(f\"ğŸ“‹ ì‹¤ì œ ëª¨ë¸ëª…: {swin_model_name}\")\n",
    "    \n",
    "    swin_model = build_model(\n",
    "        name=swin_model_name,\n",
    "        num_classes=cfg[\"data\"][\"num_classes\"],\n",
    "        pretrained=cfg[\"model\"][\"pretrained\"],\n",
    "        drop_rate=cfg[\"model\"][\"drop_rate\"],\n",
    "        drop_path_rate=cfg[\"model\"][\"drop_path_rate\"],\n",
    "        pooling=cfg[\"model\"][\"pooling\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "    swin_info = get_model_info(swin_model)\n",
    "    print(f\"âœ… Swin Transformer ë¡œë”© ì™„ë£Œ\")\n",
    "    print(f\"ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {swin_info['total_params']:,}\")\n",
    "    print(f\"ğŸ¯ í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {swin_info['trainable_params']:,}\")\n",
    "    print(f\"ğŸ’¾ ëª¨ë¸ í¬ê¸°: {swin_info['model_size_mb']:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Swin Transformer ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ’¡ timm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: pip install timm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ed85d",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ êµ¬ì¡° ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb803160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ êµ¬ì¡° í™•ì¸\n",
    "print(\"ğŸ” Swin Transformer êµ¬ì¡° ë¶„ì„\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ëª¨ë¸ì˜ ì£¼ìš” êµ¬ì„±ìš”ì†Œ í™•ì¸\n",
    "model_children = list(swin_model.children())\n",
    "print(f\"ğŸ“‹ ì£¼ìš” ëª¨ë“ˆ ìˆ˜: {len(model_children)}\")\n",
    "\n",
    "for i, child in enumerate(model_children):\n",
    "    child_name = child.__class__.__name__\n",
    "    if hasattr(child, '__len__'):\n",
    "        try:\n",
    "            child_len = len(child)\n",
    "            print(f\"  {i+1}. {child_name} (ê¸¸ì´: {child_len})\")\n",
    "        except:\n",
    "            print(f\"  {i+1}. {child_name}\")\n",
    "    else:\n",
    "        print(f\"  {i+1}. {child_name}\")\n",
    "\n",
    "# ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¥¸ feature map í¬ê¸° í™•ì¸\n",
    "print(f\"\\nğŸ“ ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {cfg['train']['img_size']}x{cfg['train']['img_size']}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ feature map í¬ê¸° í™•ì¸\n",
    "test_input = torch.randn(1, 3, cfg['train']['img_size'], cfg['train']['img_size']).to(device)\n",
    "print(f\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì…ë ¥ í˜•íƒœ: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward passë¡œ ì¶œë ¥ í¬ê¸° í™•ì¸\n",
    "    test_output = swin_model(test_input)\n",
    "    print(f\"ğŸ“¤ ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: {test_output.shape}\")\n",
    "    print(f\"âœ… Forward pass ì •ìƒ ë™ì‘ í™•ì¸\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6ef06",
   "metadata": {},
   "source": [
    "## 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8eea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory_usage(model, input_size, batch_sizes=[1, 4, 8, 16, 32]):\n",
    "    \"\"\"ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •\"\"\"\n",
    "    memory_usage = []\n",
    "    successful_batches = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        try:\n",
    "            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±\n",
    "            test_input = torch.randn(batch_size, 3, input_size, input_size).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                _ = model(test_input)\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •\n",
    "            if torch.cuda.is_available():\n",
    "                memory_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "            else:\n",
    "                # CPU ë©”ëª¨ë¦¬ëŠ” ëŒ€ëµì ì¸ ì¶”ì •\n",
    "                memory_mb = test_input.numel() * 4 / (1024 * 1024)  # ì…ë ¥ í¬ê¸° ê¸°ë°˜ ì¶”ì •\n",
    "            \n",
    "            memory_usage.append(memory_mb)\n",
    "            successful_batches.append(batch_size)\n",
    "            print(f\"âœ… ë°°ì¹˜ í¬ê¸° {batch_size:2d}: {memory_mb:6.1f} MB\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"âŒ ë°°ì¹˜ í¬ê¸° {batch_size:2d}: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"âŒ ë°°ì¹˜ í¬ê¸° {batch_size:2d}: {e}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë°°ì¹˜ í¬ê¸° {batch_size:2d}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return successful_batches, memory_usage\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •\n",
    "print(\"ğŸ’¾ Swin Transformer ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "batch_sizes, memory_usage = measure_memory_usage(\n",
    "    swin_model, \n",
    "    cfg['train']['img_size'],\n",
    "    [1, 2, 4, 8, 16, 32]\n",
    ")\n",
    "\n",
    "if memory_usage:\n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹œê°í™”\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(batch_sizes, memory_usage, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('ë°°ì¹˜ í¬ê¸°')\n",
    "    plt.ylabel('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)')\n",
    "    plt.title(f'Swin Transformer ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ({cfg[\"train\"][\"img_size\"]}px)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # í˜„ì¬ ì„¤ì •ê°’ í‘œì‹œ\n",
    "    current_batch_size = cfg['train']['batch_size']\n",
    "    if current_batch_size in batch_sizes:\n",
    "        idx = batch_sizes.index(current_batch_size)\n",
    "        plt.axhline(y=memory_usage[idx], color='r', linestyle='--', alpha=0.7)\n",
    "        plt.axvline(x=current_batch_size, color='r', linestyle='--', alpha=0.7)\n",
    "        plt.text(current_batch_size, memory_usage[idx], \n",
    "                f'  í˜„ì¬ ì„¤ì •\\n  ({current_batch_size}, {memory_usage[idx]:.0f}MB)', \n",
    "                verticalalignment='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìš”ì•½:\")\n",
    "    print(f\"   ìµœì†Œ (ë°°ì¹˜=1): {min(memory_usage):.1f} MB\")\n",
    "    print(f\"   ìµœëŒ€ (ë°°ì¹˜={max(batch_sizes)}): {max(memory_usage):.1f} MB\")\n",
    "    print(f\"   í˜„ì¬ ì„¤ì • (ë°°ì¹˜={current_batch_size}): \" +\n",
    "          f\"{memory_usage[batch_sizes.index(current_batch_size)]:.1f} MB\" \n",
    "          if current_batch_size in batch_sizes else \"ì¸¡ì • ì‹¤íŒ¨\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a11d7",
   "metadata": {},
   "source": [
    "## 4. ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa89d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference_speed(model, input_size, batch_size=1, num_iterations=50):\n",
    "    \"\"\"ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # ì›Œë°ì—…\n",
    "    warmup_input = torch.randn(batch_size, 3, input_size, input_size).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            _ = model(warmup_input)\n",
    "    \n",
    "    # ì‹¤ì œ ì¸¡ì •\n",
    "    test_input = torch.randn(batch_size, 3, input_size, input_size).to(device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            _ = model(test_input)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_batch = total_time / num_iterations\n",
    "    throughput = (batch_size * num_iterations) / total_time\n",
    "    \n",
    "    return avg_time_per_batch, throughput\n",
    "\n",
    "# ì¶”ë¡  ì†ë„ ì¸¡ì •\n",
    "print(\"âš¡ Swin Transformer ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡  ì†ë„\n",
    "single_time, single_throughput = benchmark_inference_speed(\n",
    "    swin_model, cfg['train']['img_size'], batch_size=1, num_iterations=100\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ :\")\n",
    "print(f\"   í‰ê·  ì‹œê°„: {single_time*1000:.2f} ms\")\n",
    "print(f\"   ì²˜ë¦¬ëŸ‰: {single_throughput:.1f} images/sec\")\n",
    "\n",
    "# ë°°ì¹˜ ì¶”ë¡  ì†ë„\n",
    "batch_size = min(cfg['train']['batch_size'], 8)  # ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ ì œí•œ\n",
    "batch_time, batch_throughput = benchmark_inference_speed(\n",
    "    swin_model, cfg['train']['img_size'], batch_size=batch_size, num_iterations=50\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“¦ ë°°ì¹˜ ì¶”ë¡  (ë°°ì¹˜ í¬ê¸°: {batch_size}):\")\n",
    "print(f\"   ë°°ì¹˜ë‹¹ ì‹œê°„: {batch_time*1000:.2f} ms\")\n",
    "print(f\"   ì´ë¯¸ì§€ë‹¹ ì‹œê°„: {batch_time/batch_size*1000:.2f} ms\")\n",
    "print(f\"   ì²˜ë¦¬ëŸ‰: {batch_throughput:.1f} images/sec\")\n",
    "\n",
    "# íš¨ìœ¨ì„± ë¶„ì„\n",
    "efficiency = batch_throughput / (single_throughput * batch_size)\n",
    "print(f\"\\nğŸ“ˆ ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±: {efficiency:.2f}x\")\n",
    "\n",
    "print(\"=\" * 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1eed3f",
   "metadata": {},
   "source": [
    "## 5. ë‹¤ë¥¸ ëª¨ë¸ê³¼ì˜ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ab453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetê³¼ ë¹„êµ\n",
    "print(\"ğŸ”„ ëª¨ë¸ ë¹„êµ: Swin Transformer vs EfficientNet\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def compare_models():\n",
    "    \"\"\"ëª¨ë¸ë“¤ ë¹„êµ\"\"\"\n",
    "    comparison_results = []\n",
    "    \n",
    "    models_to_compare = [\n",
    "        (\"Swin Transformer\", \"swin_base_384\"),\n",
    "        (\"EfficientNet-B3\", \"efficientnet_b3\"),\n",
    "        (\"ConvNext\", \"convnext_base_384\")\n",
    "    ]\n",
    "    \n",
    "    for model_display_name, model_key in models_to_compare:\n",
    "        try:\n",
    "            print(f\"\\nğŸ§ª {model_display_name} í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "            \n",
    "            # ëª¨ë¸ ë¡œë”©\n",
    "            model_name = get_recommended_model(model_key)\n",
    "            \n",
    "            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (EfficientNetì€ ë” ì‘ì€ í¬ê¸° ì‚¬ìš©)\n",
    "            img_size = 300 if \"efficientnet\" in model_key else cfg['train']['img_size']\n",
    "            \n",
    "            test_model = build_model(\n",
    "                name=model_name,\n",
    "                num_classes=cfg[\"data\"][\"num_classes\"],\n",
    "                pretrained=False,  # ì†ë„ ì¸¡ì •ì„ ìœ„í•´ pretrained=False\n",
    "                drop_rate=0.0,\n",
    "                drop_path_rate=0.0,\n",
    "                pooling=\"avg\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # ëª¨ë¸ ì •ë³´\n",
    "            model_info = get_model_info(test_model)\n",
    "            \n",
    "            # ì¶”ë¡  ì†ë„ ì¸¡ì •\n",
    "            inf_time, throughput = benchmark_inference_speed(\n",
    "                test_model, img_size, batch_size=1, num_iterations=20\n",
    "            )\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                test_input = torch.randn(1, 3, img_size, img_size).to(device)\n",
    "                with torch.no_grad():\n",
    "                    _ = test_model(test_input)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    memory_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "                else:\n",
    "                    memory_mb = model_info['model_size_mb']\n",
    "                    \n",
    "            except Exception as e:\n",
    "                memory_mb = model_info['model_size_mb']\n",
    "                print(f\"   âš ï¸ ë©”ëª¨ë¦¬ ì¸¡ì • ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "            result = {\n",
    "                'name': model_display_name,\n",
    "                'params': model_info['total_params'],\n",
    "                'model_size_mb': model_info['model_size_mb'],\n",
    "                'memory_mb': memory_mb,\n",
    "                'inference_time_ms': inf_time * 1000,\n",
    "                'throughput': throughput,\n",
    "                'img_size': img_size\n",
    "            }\n",
    "            \n",
    "            comparison_results.append(result)\n",
    "            \n",
    "            print(f\"   âœ… íŒŒë¼ë¯¸í„°: {model_info['total_params']:,}\")\n",
    "            print(f\"   ğŸ’¾ ëª¨ë¸ í¬ê¸°: {model_info['model_size_mb']:.1f} MB\")\n",
    "            print(f\"   ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f} MB\")\n",
    "            print(f\"   âš¡ ì¶”ë¡  ì‹œê°„: {inf_time*1000:.2f} ms\")\n",
    "            print(f\"   ğŸš€ ì²˜ë¦¬ëŸ‰: {throughput:.1f} images/sec\")\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del test_model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ {model_display_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "# ëª¨ë¸ ë¹„êµ ì‹¤í–‰\n",
    "comparison_results = compare_models()\n",
    "\n",
    "# ë¹„êµ ê²°ê³¼ ì‹œê°í™”\n",
    "if len(comparison_results) >= 2:\n",
    "    print(f\"\\nğŸ“Š ëª¨ë¸ ë¹„êµ ìš”ì•½:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ë¹„êµ í…Œì´ë¸”\n",
    "    print(f\"{'ëª¨ë¸':<15} {'íŒŒë¼ë¯¸í„°':<12} {'ì¶”ë¡ ì‹œê°„(ms)':<12} {'ì²˜ë¦¬ëŸ‰(img/s)':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for result in comparison_results:\n",
    "        print(f\"{result['name']:<15} \"\n",
    "              f\"{result['params']/1e6:.1f}M{'':<7} \"\n",
    "              f\"{result['inference_time_ms']:7.1f}{'':<5} \"\n",
    "              f\"{result['throughput']:7.1f}{'':<5}\")\n",
    "    \n",
    "    # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸\n",
    "    if len(comparison_results) >= 2:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        names = [r['name'] for r in comparison_results]\n",
    "        params = [r['params']/1e6 for r in comparison_results]  # ë°±ë§Œ ë‹¨ìœ„\n",
    "        times = [r['inference_time_ms'] for r in comparison_results]\n",
    "        throughputs = [r['throughput'] for r in comparison_results]\n",
    "        \n",
    "        # íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ\n",
    "        axes[0].bar(names, params, color='skyblue')\n",
    "        axes[0].set_title('ëª¨ë¸ í¬ê¸° (íŒŒë¼ë¯¸í„° ìˆ˜)')\n",
    "        axes[0].set_ylabel('íŒŒë¼ë¯¸í„° (M)')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # ì¶”ë¡  ì‹œê°„ ë¹„êµ\n",
    "        axes[1].bar(names, times, color='lightcoral')\n",
    "        axes[1].set_title('ì¶”ë¡  ì‹œê°„')\n",
    "        axes[1].set_ylabel('ì‹œê°„ (ms)')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # ì²˜ë¦¬ëŸ‰ ë¹„êµ\n",
    "        axes[2].bar(names, throughputs, color='lightgreen')\n",
    "        axes[2].set_title('ì²˜ë¦¬ëŸ‰')\n",
    "        axes[2].set_ylabel('Images/sec')\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7cb80",
   "metadata": {},
   "source": [
    "## ğŸ† Swin Transformer í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\n",
    "\n",
    "### âœ… ì •ìƒ ë™ì‘ í™•ì¸\n",
    "- âœ… Swin Transformer ëª¨ë¸ ë¡œë”© ë° ì´ˆê¸°í™”\n",
    "- âœ… Forward pass ì •ìƒ ë™ì‘\n",
    "- âœ… ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •\n",
    "- âœ… ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ\n",
    "- âœ… ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### ğŸ“Š ì„±ëŠ¥ íŠ¹ì„± ë¶„ì„\n",
    "\n",
    "#### ğŸ¯ ëª¨ë¸ í¬ê¸°\n",
    "- **íŒŒë¼ë¯¸í„° ìˆ˜**: ëŒ€ìš©ëŸ‰ ëª¨ë¸ (EfficientNet ëŒ€ë¹„ 3-4ë°°)\n",
    "- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 384px í•´ìƒë„ì—ì„œ ì ì ˆí•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n",
    "- **ëª¨ë¸ ë³µì¡ë„**: Vision Transformer ê¸°ë°˜ì˜ ê³ ì„±ëŠ¥ ì•„í‚¤í…ì²˜\n",
    "\n",
    "#### âš¡ ì¶”ë¡  ì„±ëŠ¥\n",
    "- **ë‹¨ì¼ ì´ë¯¸ì§€**: ì‹¤ì‹œê°„ ì¶”ë¡  ê°€ëŠ¥í•œ ì†ë„\n",
    "- **ë°°ì¹˜ ì²˜ë¦¬**: ë°°ì¹˜ í¬ê¸° ì¦ê°€ì— ë”°ë¥¸ íš¨ìœ¨ì„± í–¥ìƒ\n",
    "- **ë©”ëª¨ë¦¬ í™•ì¥ì„±**: ë°°ì¹˜ í¬ê¸°ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„ í˜• ì¦ê°€\n",
    "\n",
    "#### ğŸ† ê²½ìŸë ¥ ë¶„ì„\n",
    "- **ì •í™•ë„**: 0.934 F1 ìŠ¤ì½”ì–´ ë‹¬ì„± ê°€ëŠ¥í•œ ê³ ì„±ëŠ¥ ëª¨ë¸\n",
    "- **íš¨ìœ¨ì„±**: ì •í™•ë„ ëŒ€ë¹„ í•©ë¦¬ì ì¸ ì—°ì‚° ë¹„ìš©\n",
    "- **í™•ì¥ì„±**: ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í¬ê¸° ë° íƒœìŠ¤í¬ì— ì ìš© ê°€ëŠ¥\n",
    "\n",
    "### ğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­\n",
    "\n",
    "#### ğŸ¯ ë°°ì¹˜ í¬ê¸° ìµœì í™”\n",
    "- **ê¶Œì¥ ë°°ì¹˜ í¬ê¸°**: GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ 16-32 ì‚¬ìš©\n",
    "- **ë©”ëª¨ë¦¬ vs ì†ë„**: ë” í° ë°°ì¹˜ë¡œ ì²˜ë¦¬ëŸ‰ í–¥ìƒ ê°€ëŠ¥\n",
    "\n",
    "#### âš™ï¸ ì‹¤ë¬´ ì ìš© íŒ\n",
    "- **Mixed Precision**: AMP ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ\n",
    "- **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ í™œìš©\n",
    "- **Model Parallel**: ë§¤ìš° í° ëª¨ë¸ì˜ ê²½ìš° ê³ ë ¤\n",
    "\n",
    "### ğŸ¯ ê²½ì§„ëŒ€íšŒ ì í•©ì„±\n",
    "- âœ… **ê³ ì„±ëŠ¥**: 0.934 F1 ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥\n",
    "- âœ… **ì•ˆì •ì„±**: robustí•œ ì•„í‚¤í…ì²˜ë¡œ ì¼ê´€ëœ ì„±ëŠ¥\n",
    "- âœ… **í™•ì¥ì„±**: ì•™ìƒë¸” ë° TTAì™€ ì¡°í•© ì‹œ ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
