{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2a01e9",
   "metadata": {},
   "source": [
    "# ğŸ—‚ï¸ ê³ ì„±ëŠ¥ ë°ì´í„°ì…‹ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (ë¡œê¹… í†µí•© ë²„ì „)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë¡œê¹… ì‹œìŠ¤í…œì´ í†µí•©ëœ ê³ ì„±ëŠ¥ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
    "ëª¨ë“  ì¶œë ¥, ì‹œê°í™”, ê²°ê³¼ê°€ ì²´ê³„ì ìœ¼ë¡œ `logs/unit_test/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“ ë¡œê·¸ ì €ì¥ êµ¬ì¡°\n",
    "```\n",
    "logs/unit_test/highperf_dataset/20250905_143052/\n",
    "â”œâ”€â”€ logs/           # í…ìŠ¤íŠ¸ ë¡œê·¸ ë° ì¶œë ¥\n",
    "â”œâ”€â”€ images/         # ì‹œê°í™” ê²°ê³¼\n",
    "â”œâ”€â”€ data/           # ì²˜ë¦¬ëœ ë°ì´í„°\n",
    "â”œâ”€â”€ results/        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ JSON\n",
    "â””â”€â”€ test_summary.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f70080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™\n",
    "print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:\", os.getcwd())\n",
    "if 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"../\")\n",
    "print(\"ë³€ê²½ í›„ ì‘ì—… ë””ë ‰í† ë¦¬:\", os.getcwd())\n",
    "\n",
    "# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë¡œê±° ì´ˆê¸°í™”\n",
    "from src.utils.unit_test_logger import create_test_logger\n",
    "test_logger = create_test_logger(\"highperf_dataset\")\n",
    "test_logger.log_info(\"ê³ ì„±ëŠ¥ ë°ì´í„°ì…‹ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹œì‘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import\n",
    "try:\n",
    "    from src.data.dataset import HighPerfDocClsDataset\n",
    "    from src.utils.common import load_yaml\n",
    "    test_logger.log_success(\"ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ\")\n",
    "except Exception as e:\n",
    "    test_logger.log_error(\"ëª¨ë“ˆ import ì‹¤íŒ¨\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb8381",
   "metadata": {},
   "source": [
    "## 1. ğŸ“Š ê¸°ë³¸ ë°ì´í„° ë¶„ì„\n",
    "\n",
    "ì›ë³¸ ë°ì´í„°ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ ìº¡ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  printë¬¸ì„ ë¡œê·¸ íŒŒì¼ì— ì €ì¥\n",
    "with test_logger.capture_output(\"basic_data_analysis\") as (output, error):\n",
    "    print(\"=== ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ì‹œì‘ ===\")\n",
    "    \n",
    "    try:\n",
    "        # ë°ì´í„° ë¡œë“œ\n",
    "        train_df = pd.read_csv(\"data/raw/train.csv\")\n",
    "        test_df = pd.read_csv(\"data/raw/meta.csv\")\n",
    "        \n",
    "        print(f\"âœ… í•™ìŠµ ë°ì´í„°: {len(train_df):,} ìƒ˜í”Œ\")\n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,} ìƒ˜í”Œ\")\n",
    "        print(f\"ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {train_df['target'].nunique()}\")\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\n",
    "        class_dist = train_df['target'].value_counts().sort_index()\n",
    "        print(f\"\\nğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "        for class_id, count in class_dist.head(10).items():\n",
    "            print(f\"   Class {class_id}: {count:,} ìƒ˜í”Œ ({count/len(train_df)*100:.1f}%)\")\n",
    "        \n",
    "        if len(class_dist) > 10:\n",
    "            print(f\"   ... ì™¸ {len(class_dist)-10}ê°œ í´ë˜ìŠ¤\")\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„ ì €ì¥\n",
    "        basic_stats = {\n",
    "            \"train_samples\": len(train_df),\n",
    "            \"test_samples\": len(test_df),\n",
    "            \"num_classes\": train_df['target'].nunique(),\n",
    "            \"class_distribution\": dict(class_dist),\n",
    "            \"data_balance\": {\n",
    "                \"min_samples\": int(class_dist.min()),\n",
    "                \"max_samples\": int(class_dist.max()),\n",
    "                \"std_samples\": float(class_dist.std())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        test_logger.save_test_result(\"basic_data_analysis\", {\n",
    "            \"status\": \"success\",\n",
    "            \"stats\": basic_stats\n",
    "        })\n",
    "        \n",
    "        print(\"\\nâœ… ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ì™„ë£Œ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        test_logger.save_test_result(\"basic_data_analysis\", {\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        raise\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ì„ íŒŒì¼ë¡œ ì €ì¥\n",
    "test_logger.save_dataframe(train_df.head(100), \"sample_train_data\", \"í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ (100ê°œ)\")\n",
    "test_logger.save_dataframe(class_dist.to_frame('count'), \"class_distribution\", \"í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ ë°” ì°¨íŠ¸\n",
    "class_dist.plot(kind='bar', ax=axes[0], color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ë¶„í¬', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('í´ë˜ìŠ¤ ID')\n",
    "axes[0].set_ylabel('ìƒ˜í”Œ ìˆ˜')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶ˆê· í˜• ì •ë„ ì‹œê°í™”\n",
    "class_percentages = (class_dist / len(train_df) * 100).sort_values(ascending=False)\n",
    "axes[1].pie(class_percentages.head(8), labels=[f'Class {i}' for i in class_percentages.head(8).index], \n",
    "           autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('ìƒìœ„ 8ê°œ í´ë˜ìŠ¤ ë¹„ìœ¨', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ê·¸ë¦¼ì„ ë¡œê·¸ ë””ë ‰í† ë¦¬ì— ì €ì¥\n",
    "test_logger.save_figure(fig, \"class_distribution_analysis\", \"í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca1ac43",
   "metadata": {},
   "source": [
    "## 2. ğŸ§ª ë°ì´í„°ì…‹ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "HighPerfDocClsDataset í´ë˜ìŠ¤ì˜ ê¸°ë³¸ ë™ì‘ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_logger.capture_output(\"dataset_class_test\") as (output, error):\n",
    "    print(\"=== ë°ì´í„°ì…‹ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===\")\n",
    "    \n",
    "    try:\n",
    "        # ì„¤ì • ë¡œë“œ\n",
    "        cfg = load_yaml(\"configs/train_highperf.yaml\")\n",
    "        print(f\"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ\")\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ìš© ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±\n",
    "        mini_train = train_df.groupby('target').head(3).reset_index(drop=True)\n",
    "        mini_train_path = \"temp_mini_train.csv\"\n",
    "        mini_train.to_csv(mini_train_path, index=False)\n",
    "        \n",
    "        print(f\"ğŸ“ ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±: {len(mini_train)} ìƒ˜í”Œ\")\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ìˆ˜ì •\n",
    "        test_cfg = cfg.copy()\n",
    "        test_cfg['model']['img_size'] = 224\n",
    "        test_cfg['training']['batch_size'] = 4\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "        dataset = HighPerfDocClsDataset(\n",
    "            csv_file=mini_train_path,\n",
    "            img_dir=\"data/raw/train\",\n",
    "            config=test_cfg,\n",
    "            mode='train',\n",
    "            fold=0,\n",
    "            epoch=1\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ: {len(dataset)} ìƒ˜í”Œ\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸\n",
    "        if len(dataset) > 0:\n",
    "            sample_img, sample_label = dataset[0]\n",
    "            print(f\"âœ… ìƒ˜í”Œ ë°ì´í„° ë¡œë”© ì„±ê³µ\")\n",
    "            print(f\"   ì´ë¯¸ì§€ í¬ê¸°: {sample_img.shape}\")\n",
    "            print(f\"   ì´ë¯¸ì§€ íƒ€ì…: {type(sample_img)}\")\n",
    "            print(f\"   ë ˆì´ë¸”: {sample_label} (íƒ€ì…: {type(sample_label)})\")\n",
    "            \n",
    "            # ì´ë¯¸ì§€ í…ì„œë¥¼ numpyë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥\n",
    "            if isinstance(sample_img, torch.Tensor):\n",
    "                img_np = sample_img.permute(1, 2, 0).numpy()\n",
    "                # ì •ê·œí™” í•´ì œ (0-1 ë²”ìœ„ë¡œ)\n",
    "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "                test_logger.save_numpy_array(img_np, \"sample_image\", \"ë°ì´í„°ì…‹ì—ì„œ ë¡œë“œëœ ìƒ˜í”Œ ì´ë¯¸ì§€\")\n",
    "        \n",
    "        # DataLoader í…ŒìŠ¤íŠ¸\n",
    "        dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "        batch_img, batch_label = next(iter(dataloader))\n",
    "        \n",
    "        print(f\"âœ… DataLoader í…ŒìŠ¤íŠ¸ ì„±ê³µ\")\n",
    "        print(f\"   ë°°ì¹˜ ì´ë¯¸ì§€ í¬ê¸°: {batch_img.shape}\")\n",
    "        print(f\"   ë°°ì¹˜ ë ˆì´ë¸” í¬ê¸°: {batch_label.shape}\")\n",
    "        print(f\"   ë°°ì¹˜ ë ˆì´ë¸”: {batch_label.tolist()}\")\n",
    "        \n",
    "        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¸¡ì •\n",
    "        start_time = time.time()\n",
    "        for i, (img, label) in enumerate(dataloader):\n",
    "            if i >= 5:  # 5ê°œ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸\n",
    "                break\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time_per_batch = (end_time - start_time) / 5\n",
    "        print(f\"ğŸ“Š í‰ê·  ë°°ì¹˜ ë¡œë”© ì‹œê°„: {avg_time_per_batch:.4f}ì´ˆ\")\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥\n",
    "        dataset_test_result = {\n",
    "            \"status\": \"success\",\n",
    "            \"dataset_size\": len(dataset),\n",
    "            \"sample_image_shape\": list(sample_img.shape) if 'sample_img' in locals() else None,\n",
    "            \"batch_loading_time_sec\": avg_time_per_batch,\n",
    "            \"config_used\": {\n",
    "                \"img_size\": test_cfg['model']['img_size'],\n",
    "                \"batch_size\": test_cfg['training']['batch_size']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        test_logger.save_test_result(\"dataset_class_test\", dataset_test_result)\n",
    "        print(\"\\nâœ… ë°ì´í„°ì…‹ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n",
    "        \n",
    "        # ì„ì‹œ íŒŒì¼ ì •ë¦¬\n",
    "        os.remove(mini_train_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        test_logger.save_test_result(\"dataset_class_test\", {\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        # ì •ë¦¬\n",
    "        if os.path.exists(mini_train_path):\n",
    "            os.remove(mini_train_path)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95771f4d",
   "metadata": {},
   "source": [
    "## 3. ğŸ¨ Hard Augmentation íš¨ê³¼ ë¶„ì„\n",
    "\n",
    "ì—í¬í¬ë³„ Hard Augmentation ê°•ë„ ë³€í™”ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d067f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_logger.capture_output(\"hard_augmentation_analysis\") as (output, error):\n",
    "    print(\"=== Hard Augmentation ë¶„ì„ ì‹œì‘ ===\")\n",
    "    \n",
    "    try:\n",
    "        # ì—í¬í¬ë³„ ì¦ê°• í™•ë¥  ê³„ì‚°\n",
    "        total_epochs = 30\n",
    "        epochs = list(range(1, total_epochs + 1))\n",
    "        \n",
    "        # ì„¤ì •ì—ì„œ ì¦ê°• íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°\n",
    "        aug_config = cfg.get('augmentation', {}).get('hard_augmentation', {})\n",
    "        initial_prob = aug_config.get('initial_prob', 0.1)\n",
    "        final_prob = aug_config.get('final_prob', 0.8)\n",
    "        \n",
    "        print(f\"ğŸ“Š Hard Augmentation ì„¤ì •:\")\n",
    "        print(f\"   ì´ˆê¸° í™•ë¥ : {initial_prob}\")\n",
    "        print(f\"   ìµœì¢… í™•ë¥ : {final_prob}\")\n",
    "        print(f\"   ì´ ì—í¬í¬: {total_epochs}\")\n",
    "        \n",
    "        # ì—í¬í¬ë³„ í™•ë¥  ê³„ì‚° (ì„ í˜• ì¦ê°€)\n",
    "        hard_aug_probs = []\n",
    "        for epoch in epochs:\n",
    "            progress = (epoch - 1) / (total_epochs - 1)\n",
    "            prob = initial_prob + (final_prob - initial_prob) * progress\n",
    "            hard_aug_probs.append(prob)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ì£¼ìš” ì—í¬í¬ë³„ ì¦ê°• í™•ë¥ :\")\n",
    "        for epoch in [1, 5, 10, 15, 20, 25, 30]:\n",
    "            if epoch <= total_epochs:\n",
    "                prob = hard_aug_probs[epoch-1]\n",
    "                print(f\"   Epoch {epoch:2d}: {prob:.3f} ({prob*100:.1f}%)\")\n",
    "        \n",
    "        # ì¦ê°• í™•ë¥  ë°ì´í„° ì €ì¥\n",
    "        aug_prob_df = pd.DataFrame({\n",
    "            'epoch': epochs,\n",
    "            'hard_aug_probability': hard_aug_probs\n",
    "        })\n",
    "        \n",
    "        test_logger.save_dataframe(aug_prob_df, \"hard_augmentation_schedule\", \n",
    "                                   \"ì—í¬í¬ë³„ Hard Augmentation í™•ë¥  ìŠ¤ì¼€ì¤„\")\n",
    "        \n",
    "        # ì¦ê°• ê°•ë„ íš¨ê³¼ ë¶„ì„\n",
    "        intensity_analysis = {\n",
    "            \"initial_intensity\": initial_prob,\n",
    "            \"final_intensity\": final_prob,\n",
    "            \"intensity_increase\": final_prob - initial_prob,\n",
    "            \"midpoint_epoch\": total_epochs // 2,\n",
    "            \"midpoint_intensity\": hard_aug_probs[total_epochs // 2 - 1]\n",
    "        }\n",
    "        \n",
    "        test_logger.save_test_result(\"hard_augmentation_analysis\", {\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": intensity_analysis,\n",
    "            \"schedule_type\": \"linear_progression\"\n",
    "        })\n",
    "        \n",
    "        print(\"\\nâœ… Hard Augmentation ë¶„ì„ ì™„ë£Œ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Hard Augmentation ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        test_logger.save_test_result(\"hard_augmentation_analysis\", {\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Augmentation ìŠ¤ì¼€ì¤„ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. ì—í¬í¬ë³„ ì¦ê°• í™•ë¥  ë³€í™”\n",
    "axes[0, 0].plot(epochs, hard_aug_probs, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "axes[0, 0].set_title('ì—í¬í¬ë³„ Hard Augmentation í™•ë¥  ë³€í™”', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Augmentation Probability')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='50% ê¸°ì¤€ì„ ')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. ì¦ê°• ê°•ë„ êµ¬ê°„ë³„ ë¶„í¬\n",
    "intensity_ranges = ['Low (0-0.3)', 'Medium (0.3-0.6)', 'High (0.6-1.0)']\n",
    "range_counts = [\n",
    "    sum(1 for p in hard_aug_probs if p < 0.3),\n",
    "    sum(1 for p in hard_aug_probs if 0.3 <= p < 0.6),\n",
    "    sum(1 for p in hard_aug_probs if p >= 0.6)\n",
    "]\n",
    "\n",
    "axes[0, 1].bar(intensity_ranges, range_counts, color=['lightgreen', 'orange', 'red'], alpha=0.7)\n",
    "axes[0, 1].set_title('ì¦ê°• ê°•ë„ë³„ ì—í¬í¬ ë¶„í¬', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('ì—í¬í¬ ìˆ˜')\n",
    "for i, count in enumerate(range_counts):\n",
    "    axes[0, 1].text(i, count + 0.5, str(count), ha='center', fontweight='bold')\n",
    "\n",
    "# 3. ëˆ„ì  ì¦ê°• íš¨ê³¼\n",
    "cumulative_effect = np.cumsum(hard_aug_probs)\n",
    "axes[1, 0].fill_between(epochs, cumulative_effect, alpha=0.5, color='purple')\n",
    "axes[1, 0].plot(epochs, cumulative_effect, 'purple', linewidth=2)\n",
    "axes[1, 0].set_title('ëˆ„ì  ì¦ê°• íš¨ê³¼', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Cumulative Augmentation Effect')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ì¦ê°• ê°•ë„ íˆíŠ¸ë§µ\n",
    "intensity_matrix = np.array(hard_aug_probs).reshape(6, 5)  # 30 ì—í¬í¬ë¥¼ 6x5 ê²©ìë¡œ\n",
    "im = axes[1, 1].imshow(intensity_matrix, cmap='viridis', aspect='auto')\n",
    "axes[1, 1].set_title('ì¦ê°• ê°•ë„ íˆíŠ¸ë§µ (6ì£¼ x 5ì¼)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('ì¼ (Day)')\n",
    "axes[1, 1].set_ylabel('ì£¼ (Week)')\n",
    "\n",
    "# ì»¬ëŸ¬ë°” ì¶”ê°€\n",
    "cbar = plt.colorbar(im, ax=axes[1, 1], shrink=0.8)\n",
    "cbar.set_label('Augmentation Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ì‹œê°í™” ê²°ê³¼ ì €ì¥\n",
    "test_logger.save_figure(fig, \"hard_augmentation_schedule_visualization\", \n",
    "                       \"Hard Augmentation ìŠ¤ì¼€ì¤„ ì¢…í•© ë¶„ì„\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aaec0b",
   "metadata": {},
   "source": [
    "## 4. ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ìµœì¢… ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef07f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n",
    "performance_metrics = {\n",
    "    \"data_loading\": {\n",
    "        \"dataset_size\": len(dataset) if 'dataset' in locals() else 0,\n",
    "        \"avg_batch_time_sec\": avg_time_per_batch if 'avg_time_per_batch' in locals() else 0,\n",
    "        \"estimated_epoch_time_min\": (avg_time_per_batch * len(train_df) / 4 / 60) if 'avg_time_per_batch' in locals() else 0\n",
    "    },\n",
    "    \"augmentation\": {\n",
    "        \"hard_aug_enabled\": aug_config.get('enabled', False),\n",
    "        \"initial_prob\": initial_prob if 'initial_prob' in locals() else 0,\n",
    "        \"final_prob\": final_prob if 'final_prob' in locals() else 0,\n",
    "        \"avg_intensity\": np.mean(hard_aug_probs) if 'hard_aug_probs' in locals() else 0\n",
    "    },\n",
    "    \"data_quality\": {\n",
    "        \"class_balance_std\": float(class_dist.std()) if 'class_dist' in locals() else 0,\n",
    "        \"min_class_samples\": int(class_dist.min()) if 'class_dist' in locals() else 0,\n",
    "        \"max_class_samples\": int(class_dist.max()) if 'class_dist' in locals() else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "test_logger.save_performance_metrics(performance_metrics, \"dataset_performance\")\n",
    "\n",
    "with test_logger.capture_output(\"final_summary\") as (output, error):\n",
    "    print(\"=== ìµœì¢… í…ŒìŠ¤íŠ¸ ìš”ì•½ ===\")\n",
    "    print(f\"âœ… ê¸°ë³¸ ë°ì´í„° ë¶„ì„: ì™„ë£Œ\")\n",
    "    print(f\"âœ… ë°ì´í„°ì…‹ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸: ì™„ë£Œ\")\n",
    "    print(f\"âœ… Hard Augmentation ë¶„ì„: ì™„ë£Œ\")\n",
    "    print(f\"âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘: ì™„ë£Œ\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì£¼ìš” ê²°ê³¼:\")\n",
    "    print(f\"   ì´ í•™ìŠµ ìƒ˜í”Œ: {len(train_df):,}ê°œ\")\n",
    "    print(f\"   í´ë˜ìŠ¤ ìˆ˜: {train_df['target'].nunique()}ê°œ\")\n",
    "    print(f\"   í‰ê·  ë°°ì¹˜ ë¡œë”© ì‹œê°„: {performance_metrics['data_loading']['avg_batch_time_sec']:.4f}ì´ˆ\")\n",
    "    print(f\"   Hard Augmentation í‰ê·  ê°•ë„: {performance_metrics['augmentation']['avg_intensity']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\n",
    "    if performance_metrics['data_quality']['class_balance_std'] > 500:\n",
    "        print(f\"   âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ í½ë‹ˆë‹¤. ê°€ì¤‘ ìƒ˜í”Œë§ ê³ ë ¤\")\n",
    "    if performance_metrics['data_loading']['avg_batch_time_sec'] > 0.1:\n",
    "        print(f\"   âš ï¸ ë°ì´í„° ë¡œë”©ì´ ëŠë¦½ë‹ˆë‹¤. num_workers ì¦ê°€ ê³ ë ¤\")\n",
    "    if performance_metrics['augmentation']['avg_intensity'] < 0.3:\n",
    "        print(f\"   ğŸ’¡ ì¦ê°• ê°•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë” ê°•í•œ ì¦ê°• ê³ ë ¤\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(f\"   1. test_mixup_augmentation.ipynb - Mixup íŒŒë¼ë¯¸í„° ìµœì í™”\")\n",
    "    print(f\"   2. test_swin_model.ipynb - ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\")\n",
    "    print(f\"   3. ì‹¤ì œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n",
    "final_summary = test_logger.finalize_test()\n",
    "\n",
    "print(f\"\\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ ìƒì„¸ ê²°ê³¼: {test_logger.base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a34ffc",
   "metadata": {},
   "source": [
    "## ğŸ† í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\n",
    "\n",
    "### âœ… ì™„ë£Œëœ í…ŒìŠ¤íŠ¸\n",
    "1. **ê¸°ë³¸ ë°ì´í„° ë¶„ì„**: í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°, í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "2. **ë°ì´í„°ì…‹ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸**: HighPerfDocClsDataset ë™ì‘ ê²€ì¦\n",
    "3. **Hard Augmentation ë¶„ì„**: ì—í¬í¬ë³„ ì¦ê°• ê°•ë„ ìŠ¤ì¼€ì¤„ ë¶„ì„\n",
    "4. **ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘**: ë°ì´í„° ë¡œë”© ì†ë„, ì¦ê°• íš¨ê³¼ ì¸¡ì •\n",
    "\n",
    "### ğŸ“ ì €ì¥ëœ ê²°ê³¼\n",
    "- **ë¡œê·¸ íŒŒì¼**: ëª¨ë“  ì¶œë ¥ê³¼ ì—ëŸ¬ ë©”ì‹œì§€\n",
    "- **ì‹œê°í™”**: í´ë˜ìŠ¤ ë¶„í¬, ì¦ê°• ìŠ¤ì¼€ì¤„ ì°¨íŠ¸\n",
    "- **ë°ì´í„°**: ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ê³¼ NumPy ë°°ì—´\n",
    "- **ë©”íŠ¸ë¦­**: JSON í˜•íƒœì˜ ì„±ëŠ¥ ì§€í‘œ\n",
    "\n",
    "### ğŸ”— ë¡œê·¸ ë””ë ‰í† ë¦¬ ì ‘ê·¼\n",
    "```bash\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "ls -la logs/unit_test/highperf_dataset/[timestamp]/\n",
    "\n",
    "# ì´ë¯¸ì§€ í™•ì¸\n",
    "ls logs/unit_test/highperf_dataset/[timestamp]/images/\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ìš”ì•½ í™•ì¸\n",
    "cat logs/unit_test/highperf_dataset/[timestamp]/test_summary.json\n",
    "```\n",
    "\n",
    "ì´ì œ ëª¨ë“  í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì²´ê³„ì ìœ¼ë¡œ ì €ì¥ë˜ì–´ ì¶”í›„ ë¶„ì„ê³¼ ë¹„êµê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤! ğŸ¯"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
