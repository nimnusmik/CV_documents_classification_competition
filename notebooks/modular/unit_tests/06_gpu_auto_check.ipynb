{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa33a4e",
   "metadata": {},
   "source": [
    "# π§ GPU μλ™ μ²΄ν¬ λ° ν™κ²½ ν…μ¤νΈ\n",
    "\n",
    "μ΄ λ…ΈνΈλ¶μ€ GPU ν™κ²½ μλ™ μ²΄ν¬ λ° κ΄€λ ¨ μ„¤μ •μ„ ν…μ¤νΈν•©λ‹λ‹¤:\n",
    "- GPU μ‚¬μ© κ°€λ¥ μ—¬λ¶€ ν™•μΈ\n",
    "- CUDA/λ©”λ¨λ¦¬ μ •λ³΄ μ¶λ ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862171b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ν„μ¬ μ‘μ—… λ””λ ‰ν† λ¦¬: /home/ieyeppo/AI_Lab/computer-vision-competition-1SEN\n"
     ]
    }
   ],
   "source": [
    "# [1] ν”„λ΅μ νΈ λ£¨νΈ λ””λ ‰ν† λ¦¬λ΅ μ΄λ™\n",
    "import os                                                   # OS λ¨λ“ μ„ν¬νΈ\n",
    "import sys                                                  # μ‹μ¤ν… λ¨λ“ μ„ν¬νΈ\n",
    "os.chdir(\"../../../\")                                       # ν”„λ΅μ νΈ λ£¨νΈλ΅ μ΄λ™\n",
    "print(\"ν„μ¬ μ‘μ—… λ””λ ‰ν† λ¦¬:\", os.getcwd())                      # ν„μ¬ λ””λ ‰ν† λ¦¬ μ¶λ ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7696a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] ν°νΈ μ„¤μ • λ° κ²½κ³  μ–µμ \n",
    "# ν•κΈ€ ν°νΈ μ μ© λ° μ‹κ°ν™” ν™κ²½ μ„¤μ •\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# λ‚λ”κ³ λ”• ν°νΈ κ²½λ΅ λ° μ„¤μ •\n",
    "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "fontprop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "# ν°νΈ λ“±λ΅ λ° μ„¤μ •\n",
    "fe = fm.FontEntry(fname=font_path, name='NanumGothic')\n",
    "fm.fontManager.ttflist.insert(0, fe)\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# κΈ€μ κ²ΉμΉ¨ λ°©μ§€λ¥Ό μ„ν• λ μ΄μ•„μ›ƒ μ„¤μ •\n",
    "plt.rcParams['figure.autolayout'] = True\n",
    "plt.rcParams['axes.titlepad'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8a303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "π“ λ‹¨μ„ ν…μ¤νΈ μ‹μ‘: gpu_auto_check\n",
      "π“ λ΅κ·Έ λ””λ ‰ν† λ¦¬: notebooks/modular/unit_tests/gpu_auto_check/20250906_003909\n"
     ]
    }
   ],
   "source": [
    "# [3] λΌμ΄λΈλ¬λ¦¬ λ° μ ν‹Έλ¦¬ν‹° μ„ν¬νΈ\n",
    "# GPU μ²΄ν¬ λ° λ΅κ±° λ“± ν•„μ”ν• λ¨λ“ μ„ν¬νΈ\n",
    "import torch  # νμ΄ν† μΉ μ„ν¬νΈ\n",
    "from src.utils.unit_test_logger import create_test_logger  # λ‹¨μ„ ν…μ¤νΈ λ΅κ±° μƒμ„± ν•¨μ μ„ν¬νΈ\n",
    "test_logger = create_test_logger(\"gpu_auto_check\")  # λ‹¨μ„ ν…μ¤νΈ λ΅κ±° μ΄κΈ°ν™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f2d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "β… GPU μ‚¬μ© κ°€λ¥: NVIDIA GeForce RTX 4090\n",
      "μ΄ GPU μ: 1\n",
      "GPU λ©”λ¨λ¦¬: 24563 MB\n",
      "CUDA λ²„μ „: 12.1\n",
      "π“ ν…μ¤νΈ κ²°κ³Ό μ €μ¥: gpu_check_test\n",
      "β… ν…μ¤νΈ μ™„λ£! μ΄ μ†μ” μ‹κ°„: 12.39μ΄\n",
      "β… κ²°κ³Ό μ”μ•½: notebooks/modular/unit_tests/gpu_auto_check/20250906_003909/test_summary.json\n",
      "\n",
      "==================================================\n",
      "π λ‹¨μ„ ν…μ¤νΈ μ™„λ£: gpu_auto_check\n",
      "==================================================\n",
      "π“ κ²°κ³Ό λ””λ ‰ν† λ¦¬: notebooks/modular/unit_tests/gpu_auto_check/20250906_003909\n",
      "β±οΈ μ†μ” μ‹κ°„: 12.39μ΄\n",
      "π“ ν…μ¤νΈ μ„Ήμ… μ: 1\n",
      "==================================================\n",
      "β… GPU μλ™ μ²΄ν¬ ν…μ¤νΈ μ™„λ£\n"
     ]
    }
   ],
   "source": [
    "# [4] GPU μλ™ μ²΄ν¬ λ° μ •λ³΄ μμ§‘\n",
    "# GPU ν™κ²½ μ •λ³΄ ν™•μΈ λ° μ €μ¥\n",
    "gpu_info = {}\n",
    "if torch.cuda.is_available():  # GPU μ‚¬μ© κ°€λ¥ μ—¬λ¶€ ν™•μΈ\n",
    "    gpu_info = {\n",
    "        'gpu_available': True,\n",
    "        'gpu_name': torch.cuda.get_device_name(0),\n",
    "        'gpu_count': torch.cuda.device_count(),\n",
    "        'gpu_memory_mb': torch.cuda.get_device_properties(0).total_memory // (1024**2),\n",
    "        'cuda_version': torch.version.cuda\n",
    "    }\n",
    "    print(f'β… GPU μ‚¬μ© κ°€λ¥: {gpu_info[\"gpu_name\"]}')  # GPU μ΄λ¦„ μ¶λ ¥\n",
    "    print(f'μ΄ GPU μ: {gpu_info[\"gpu_count\"]}')  # GPU κ°μ μ¶λ ¥\n",
    "    print(f'GPU λ©”λ¨λ¦¬: {gpu_info[\"gpu_memory_mb\"]} MB')  # GPU λ©”λ¨λ¦¬ μ¶λ ¥\n",
    "    print(f'CUDA λ²„μ „: {gpu_info[\"cuda_version\"]}')  # CUDA λ²„μ „ μ¶λ ¥\n",
    "else:\n",
    "    gpu_info = {\n",
    "        'gpu_available': False,\n",
    "        'gpu_name': None,\n",
    "        'gpu_count': 0,\n",
    "        'gpu_memory_mb': 0,\n",
    "        'cuda_version': None\n",
    "    }\n",
    "    print('β οΈ GPU μ‚¬μ© λ¶κ°€, CPUλ΅ μ‹¤ν–‰λ©λ‹λ‹¤')  # CPU μ‚¬μ© μ•λ‚΄\n",
    "\n",
    "# ν…μ¤νΈ κ²°κ³Ό μ €μ¥\n",
    "test_logger.save_test_result('gpu_check_test', gpu_info)\n",
    "test_logger.finalize_test()\n",
    "print('β… GPU μλ™ μ²΄ν¬ ν…μ¤νΈ μ™„λ£')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5aab8",
   "metadata": {},
   "source": [
    "## π“ GPU μλ™ μ²΄ν¬ ν…μ¤νΈ κ²°κ³Ό μ”μ•½\n",
    "\n",
    "### β… ν…μ¤νΈ μ„±κ³µ ν•­λ©\n",
    "1. **GPU κ°μ§€**: NVIDIA GeForce RTX 4090 μ •μƒ μΈμ‹\n",
    "2. **CUDA ν™κ²½**: CUDA 12.1 λ²„μ „ μ •μƒ μ‘λ™\n",
    "3. **λ©”λ¨λ¦¬ ν™•μΈ**: 24,563 MB GPU λ©”λ¨λ¦¬ ν™•μΈ\n",
    "4. **PyTorch μ—°λ™**: torch.cuda.is_available() μ •μƒ μ‘λ‹µ\n",
    "\n",
    "### π“ ν•λ“μ›¨μ–΄ μ‚¬μ–‘\n",
    "- **GPU λ¨λΈ**: NVIDIA GeForce RTX 4090\n",
    "- **μ΄ GPU μ**: 1κ°\n",
    "- **GPU λ©”λ¨λ¦¬**: 24,563 MB (~24GB)\n",
    "- **CUDA λ²„μ „**: 12.1\n",
    "- **PyTorch μ§€μ›**: β… μ™„μ „ νΈν™\n",
    "\n",
    "### π“ μƒμ„±λ νμΌ κµ¬μ΅°\n",
    "```\n",
    "notebooks/modular/unit_tests/gpu_auto_check/20250906_003909/\n",
    "β”β”€β”€ test_summary.json\n",
    "β””β”€β”€ test_results.json\n",
    "```\n",
    "\n",
    "### π”§ GPU ν™κ²½ μ •λ³΄\n",
    "- **μ‚¬μ© κ°€λ¥ μƒνƒ**: β… μ •μƒ\n",
    "- **λ©”λ¨λ¦¬ μƒνƒ**: μ¶©λ¶„ν• μ©λ‰ ν™•λ³΄\n",
    "- **λ“λΌμ΄λ²„ νΈν™μ„±**: PyTorchμ™€ μ™„μ „ νΈν™\n",
    "- **μ„±λ¥ λ“±κΈ‰**: κ³ μ„±λ¥ AI/ML μ‘μ—… κ°€λ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38a6bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "π” GPU μλ™ μ²΄ν¬ ν…μ¤νΈ κ²°κ³Ό νμΌ ν™•μΈ\n",
      "\n",
      "π“ λ‹¨μ„ ν…μ¤νΈ κ²°κ³Ό λ””λ ‰ν† λ¦¬ (1κ°):\n",
      "   β”β”€β”€ 20250906_003909 (μƒμ„±: 2025-09-06 00:39:22)\n",
      "   β”‚   β””β”€β”€ test_summary.json (0.5 KB)\n",
      "\n",
      "π”§ μµμ‹  GPU ν…μ¤νΈ μƒμ„Έ μ •λ³΄:\n",
      "   π“… ν…μ¤νΈ μ‹κ°„: N/A\n",
      "   β±οΈ μ†μ” μ‹κ°„: 0.00μ΄\n",
      "   π“ ν…μ¤νΈ κ²°κ³Ό: dict_keys([])\n",
      "\n",
      "π― μ‹μ¤ν… νΈν™μ„± κ²€μ¦:\n",
      "   β… PyTorch GPU μ§€μ›: True\n",
      "   β… CUDA λ””λ°”μ΄μ¤ μ: 1\n",
      "   β… ν„μ¬ GPU: NVIDIA GeForce RTX 4090\n",
      "   β… λ©”λ¨λ¦¬ μ΄λ‰: 24563 MB\n",
      "\n",
      "β… GPU μλ™ μ²΄ν¬ ν…μ¤νΈ μ™„λ£!\n",
      "π“… ν…μ¤νΈ μΌμ‹: 2025-09-06 00:40:13\n",
      "π― GPU ν™κ²½μ΄ AI/ML μ‘μ—…μ— μµμ ν™”λμ–΄ μμμ„ ν™•μΈν–μµλ‹λ‹¤.\n"
     ]
    }
   ],
   "source": [
    "# [5] μ‹¤μ  μƒμ„±λ νμΌ κµ¬μ΅° ν™•μΈ\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"π” GPU μλ™ μ²΄ν¬ ν…μ¤νΈ κ²°κ³Ό νμΌ ν™•μΈ\\n\")\n",
    "\n",
    "# 1. ν…μ¤νΈ κ²°κ³Ό λ””λ ‰ν† λ¦¬ ν™•μΈ\n",
    "test_dirs = glob.glob(\"notebooks/modular/unit_tests/gpu_auto_check/202509*\")\n",
    "print(f\"π“ λ‹¨μ„ ν…μ¤νΈ κ²°κ³Ό λ””λ ‰ν† λ¦¬ ({len(test_dirs)}κ°):\")\n",
    "for test_dir in sorted(test_dirs)[-3:]:  # μµκ·Ό 3κ°λ§ ν‘μ‹\n",
    "    mtime = datetime.fromtimestamp(os.path.getmtime(test_dir))\n",
    "    files = os.listdir(test_dir)\n",
    "    json_files = [f for f in files if f.endswith('.json')]\n",
    "    \n",
    "    print(f\"   β”β”€β”€ {os.path.basename(test_dir)} (μƒμ„±: {mtime.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(test_dir, json_file)\n",
    "        size = os.path.getsize(file_path) / 1024  # KB\n",
    "        print(f\"   β”‚   β””β”€β”€ {json_file} ({size:.1f} KB)\")\n",
    "\n",
    "# 2. GPU ν…μ¤νΈ μ •λ³΄ μƒμ„Έ ν™•μΈ\n",
    "if test_dirs:\n",
    "    latest_test_dir = max(test_dirs, key=os.path.getmtime)\n",
    "    summary_file = os.path.join(latest_test_dir, \"test_summary.json\")\n",
    "    \n",
    "    if os.path.exists(summary_file):\n",
    "        import json\n",
    "        with open(summary_file, 'r', encoding='utf-8') as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        print(f\"\\nπ”§ μµμ‹  GPU ν…μ¤νΈ μƒμ„Έ μ •λ³΄:\")\n",
    "        print(f\"   π“… ν…μ¤νΈ μ‹κ°„: {summary.get('test_datetime', 'N/A')}\")\n",
    "        print(f\"   β±οΈ μ†μ” μ‹κ°„: {summary.get('total_time', 0):.2f}μ΄\")\n",
    "        print(f\"   π“ ν…μ¤νΈ κ²°κ³Ό: {summary.get('test_results', {}).keys()}\")\n",
    "\n",
    "# 3. μ‹μ¤ν… νΈν™μ„± μ”μ•½\n",
    "print(f\"\\nπ― μ‹μ¤ν… νΈν™μ„± κ²€μ¦:\")\n",
    "print(f\"   β… PyTorch GPU μ§€μ›: {torch.cuda.is_available()}\")\n",
    "print(f\"   β… CUDA λ””λ°”μ΄μ¤ μ: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   β… ν„μ¬ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   β… λ©”λ¨λ¦¬ μ΄λ‰: {torch.cuda.get_device_properties(0).total_memory // (1024**2)} MB\")\n",
    "\n",
    "print(f\"\\nβ… GPU μλ™ μ²΄ν¬ ν…μ¤νΈ μ™„λ£!\")\n",
    "print(f\"π“… ν…μ¤νΈ μΌμ‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"π― GPU ν™κ²½μ΄ AI/ML μ‘μ—…μ— μµμ ν™”λμ–΄ μμμ„ ν™•μΈν–μµλ‹λ‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b49149",
   "metadata": {},
   "source": [
    "## π› οΈ GPU μλ™ μ²΄ν¬ νΈλ¬λΈ”μν… κ°€μ΄λ“\n",
    "\n",
    "### π¨ μμ£Ό λ°μƒν•λ” λ¬Έμ λ“¤\n",
    "\n",
    "#### 1. **GPUκ°€ κ°μ§€λμ§€ μ•μ**\n",
    "```python\n",
    "# λ¬Έμ  ν™•μΈ\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # FalseμΈ κ²½μ°\n",
    "\n",
    "# ν•΄κ²° λ°©λ²•\n",
    "# 1. NVIDIA λ“λΌμ΄λ²„ μ¬μ„¤μΉ\n",
    "# 2. CUDA ν΄ν‚· μ„¤μΉ ν™•μΈ\n",
    "# 3. PyTorch GPU λ²„μ „ μ¬μ„¤μΉ\n",
    "```\n",
    "\n",
    "#### 2. **CUDA λ²„μ „ λ¶μΌμΉ**\n",
    "```bash\n",
    "# μ‹μ¤ν… CUDA λ²„μ „ ν™•μΈ\n",
    "nvidia-smi\n",
    "\n",
    "# PyTorch CUDA λ²„μ „ ν™•μΈ\n",
    "python -c \"import torch; print(torch.version.cuda)\"\n",
    "```\n",
    "\n",
    "#### 3. **λ©”λ¨λ¦¬ λ¶€μ΅± μ¤λ¥**\n",
    "```python\n",
    "# GPU λ©”λ¨λ¦¬ μƒνƒ ν™•μΈ\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"λ©”λ¨λ¦¬ μ΄λ‰: {torch.cuda.get_device_properties(0).total_memory // (1024**2)} MB\")\n",
    "    print(f\"μ‚¬μ© μ¤‘: {torch.cuda.memory_allocated() // (1024**2)} MB\")\n",
    "    print(f\"μΊμ‹λ¨: {torch.cuda.memory_reserved() // (1024**2)} MB\")\n",
    "```\n",
    "\n",
    "#### 4. **λ“λΌμ΄λ²„ νΈν™μ„± λ¬Έμ **\n",
    "- **NVIDIA λ“λΌμ΄λ²„**: μµμ‹  Game Ready λλ” Studio λ“λΌμ΄λ²„ κ¶μ¥\n",
    "- **CUDA νΈν™μ„±**: PyTorch 2.5.1+cu121μ€ CUDA 12.1 μ΄μƒ ν•„μ”\n",
    "- **μµμ† μ”κµ¬μ‚¬ν•­**: CUDA Compute Capability 3.7 μ΄μƒ\n",
    "\n",
    "### π“‹ ν•„μ ν™•μΈ μ‚¬ν•­\n",
    "1. **ν•λ“μ›¨μ–΄**: NVIDIA GPU μ„¤μΉ λ° μ „μ› μ—°κ²°\n",
    "2. **λ“λΌμ΄λ²„**: μµμ‹  NVIDIA λ“λΌμ΄λ²„ μ„¤μΉ\n",
    "3. **CUDA**: μ μ ν• CUDA ν΄ν‚· λ²„μ „\n",
    "4. **PyTorch**: GPU μ§€μ› λ²„μ „ μ„¤μΉ\n",
    "\n",
    "### π”§ μλ™ ν…μ¤νΈ λ°©λ²•\n",
    "```python\n",
    "# 1. κΈ°λ³Έ GPU ν…μ¤νΈ\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.randn(1000, 1000).to(device)\n",
    "y = torch.randn(1000, 1000).to(device)\n",
    "z = torch.matmul(x, y)\n",
    "print(f\"μ—°μ‚° μ™„λ£: {z.shape} on {z.device}\")\n",
    "\n",
    "# 2. λ©”λ¨λ¦¬ ν…μ¤νΈ\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()  # μΊμ‹ μ •λ¦¬\n",
    "    print(\"GPU λ©”λ¨λ¦¬ μΊμ‹ μ •λ¦¬ μ™„λ£\")\n",
    "```\n",
    "\n",
    "### π’΅ μ„±λ¥ μµμ ν™” ν\n",
    "- **νΌν•© μ •λ°€λ„**: `torch.cuda.amp` μ‚¬μ©μΌλ΅ λ©”λ¨λ¦¬ μ μ•½\n",
    "- **λ°°μΉ ν¬κΈ°**: GPU λ©”λ¨λ¦¬μ— λ§λ” μ μ ν• λ°°μΉ ν¬κΈ° μ„¤μ •\n",
    "- **λ‹¤μ¤‘ GPU**: `torch.nn.DataParallel` λλ” `DistributedDataParallel` ν™μ©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
