{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **ğŸ“„ Document Classification - Data-Optimized Version**\n",
    "# ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì— ìµœì í™”ëœ ì„¤ì • (1,570 train / 3,140 test / 17 classes)\n",
    "\n",
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Mixed Precision Training\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "## 2. ë°ì´í„°ì…‹ ë° ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "class DocumentDataset(Dataset):\n",
    "    \"\"\"ğŸ“„ ë¬¸ì„œ ë¶„ë¥˜ íŠ¹í™” ë°ì´í„°ì…‹\"\"\"\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, str):\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        else:\n",
    "            self.df = csv.values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"ğŸ¯ Focal Loss - ì†Œê·œëª¨ ë°ì´í„°ì˜ ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘\"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"ğŸ¯ Label Smoothing - ê³¼ì í•© ë°©ì§€\"\"\"\n",
    "    def __init__(self, epsilon=0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        n_classes = preds.size(-1)\n",
    "        log_preds = F.log_softmax(preds, dim=-1)\n",
    "        \n",
    "        targets_smooth = torch.zeros_like(log_preds).scatter_(1, targets.unsqueeze(1), 1)\n",
    "        targets_smooth = targets_smooth * (1 - self.epsilon) + self.epsilon / n_classes\n",
    "        \n",
    "        if self.weight is not None:\n",
    "            weights = self.weight[targets]\n",
    "            loss = -(targets_smooth * log_preds).sum(dim=-1) * weights\n",
    "        else:\n",
    "            loss = -(targets_smooth * log_preds).sum(dim=-1)\n",
    "            \n",
    "        return loss.mean()\n",
    "\n",
    "def calculate_class_weights(csv_path):\n",
    "    \"\"\"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ê²½ë¯¸í•œ ë¶ˆê· í˜•ìš©)\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    class_counts = df['target'].value_counts().sort_index()\n",
    "    total_samples = len(df)\n",
    "    n_classes = len(class_counts)\n",
    "    \n",
    "    # ê²½ë¯¸í•œ ë¶ˆê· í˜•ì´ë¯€ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë„ˆë¬´ ê°•í•˜ê²Œ ì£¼ì§€ ì•ŠìŒ\n",
    "    weights = []\n",
    "    for count in class_counts:\n",
    "        weight = np.sqrt(total_samples / (n_classes * count))  # sqrtë¡œ ì™„í™”\n",
    "        weights.append(weight)\n",
    "    \n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "## 3. í›ˆë ¨ ë° ê²€ì¦ í•¨ìˆ˜\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, scheduler=None, use_amp=True):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"ğŸ“š Document Training\")\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_amp:\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'LR': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "def validate_one_epoch(loader, model, loss_fn, device, use_amp=True):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"ğŸ” Validation\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    preds = model(image)\n",
    "                    loss = loss_fn(preds, targets)\n",
    "            else:\n",
    "                preds = model(image)\n",
    "                loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "## 4. ë°ì´í„° íŠ¹ì„±ì— ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = 'datasets_fin/'\n",
    "model_name = 'convnext_base'\n",
    "\n",
    "# ğŸ¯ ì†Œê·œëª¨ ë°ì´í„°(1,570ê°œ)ì— ìµœì í™”ëœ ì„¤ì •\n",
    "img_size = 224              # ë‹¨ì¼ í•´ìƒë„ (Multi-Scale ì œê±°)\n",
    "LR = 3e-4                   # ì ë‹¹í•œ í•™ìŠµë¥ \n",
    "EPOCHS = 12                 # ê³¼ì í•© ë°©ì§€ (20â†’12)\n",
    "BATCH_SIZE = 16             # GPU íš¨ìœ¨ì„± ê³ ë ¤ (6â†’16)\n",
    "num_workers = 4             # ë°ì´í„° ê·œëª¨ì— ë§ì¶¤ (8â†’4)\n",
    "\n",
    "# ê³ ê¸‰ ì„¤ì • ìµœì í™”\n",
    "USE_AMP = True\n",
    "LABEL_SMOOTHING = 0.1\n",
    "N_FOLDS = 3                 # ì†Œê·œëª¨ ë°ì´í„°ë¼ 3-foldê°€ ì í•© (5â†’3)\n",
    "PATIENCE = 5                # ì¡°ê¸ˆ ë” ê¸´ ì¸ë‚´ì‹¬\n",
    "WARMUP_EPOCHS = 2\n",
    "MIN_LR = 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "# ğŸš« ì œê±°ëœ ê³¼ë„í•œ ê¸°ë²•ë“¤\n",
    "USE_KNOWLEDGE_DISTILLATION = False  # ì†Œê·œëª¨ ë°ì´í„°ì—ëŠ” ë¶€ì í•©\n",
    "USE_PSEUDO_LABELING = False         # íš¨ê³¼ ì œí•œì \n",
    "COSINE_RESTARTS = False            # ë‹¨ìˆœí•œ Cosine Annealing ì‚¬ìš©\n",
    "\n",
    "print(f\"ğŸ“Š ë°ì´í„° ìµœì í™”ëœ ì„¤ì •:\")\n",
    "print(f\"  í›ˆë ¨ ë°ì´í„°: 1,570ê°œ â†’ 3-fold CV\")\n",
    "print(f\"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: 3,140ê°œ\")\n",
    "print(f\"  í´ë˜ìŠ¤ ìˆ˜: 17ê°œ (ì˜ë£Œ/ì‹ ë¶„ì¦/ì°¨ëŸ‰/ê¸ˆìœµ/ê¸°íƒ€)\")\n",
    "print(f\"  ì´ë¯¸ì§€ í¬ê¸°: {img_size}x{img_size} (ë‹¨ì¼ í•´ìƒë„)\")\n",
    "print(f\"  ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE} (GPU íš¨ìœ¨ ìµœì í™”)\")\n",
    "print(f\"  ì—í¬í¬: {EPOCHS} (ê³¼ì í•© ë°©ì§€)\")\n",
    "\n",
    "## 5. ë¬¸ì„œ íŠ¹í™” Augmentation\n",
    "\n",
    "def create_document_transforms(img_size):\n",
    "    \"\"\"ğŸ“„ ë¬¸ì„œ ë¶„ë¥˜ íŠ¹í™” Augmentation - ì ë‹¹í•œ ìˆ˜ì¤€\"\"\"\n",
    "    \n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        \n",
    "        # ğŸ“„ ë¬¸ì„œ íšŒì „ (ìŠ¤ìº” ì˜¤ì°¨)\n",
    "        A.OneOf([\n",
    "            A.Rotate(limit=15, p=1.0),          # ì ë‹¹í•œ íšŒì „ (45â†’15)\n",
    "            A.SafeRotate(limit=20, p=0.8),      # ì•ˆì „í•œ íšŒì „ (75â†’20)\n",
    "        ], p=0.6),                             # í™•ë¥  ê°ì†Œ (0.7â†’0.6)\n",
    "        \n",
    "        # ğŸ”€ ë’¤ì§‘ê¸° (ì ë‹¹í•œ í™•ë¥ )\n",
    "        A.HorizontalFlip(p=0.3),               # í™•ë¥  ê°ì†Œ (0.5â†’0.3)\n",
    "        A.VerticalFlip(p=0.1),                 # í™•ë¥  ê°ì†Œ (0.3â†’0.1)\n",
    "        \n",
    "        # ğŸ“ ê¸°í•˜í•™ì  ë³€í˜• (ì™„í™”)\n",
    "        A.OneOf([\n",
    "            A.Perspective(scale=(0.05, 0.15), p=1.0),      # ë²”ìœ„ ì™„í™”\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=1.0),\n",
    "            A.GridDistortion(num_steps=3, distort_limit=0.2, p=1.0),  # ê°•ë„ ì™„í™”\n",
    "        ], p=0.4),                             # í™•ë¥  ê°ì†Œ (0.6â†’0.4)\n",
    "        \n",
    "        # ğŸ” í’ˆì§ˆ ì €í•˜ (ì™„í™”)\n",
    "        A.OneOf([\n",
    "            A.ImageCompression(quality_lower=30, quality_upper=80, p=1.0),  # ë²”ìœ„ ì™„í™”\n",
    "            A.GaussianBlur(blur_limit=5, p=1.0),           # ê°•ë„ ì™„í™” (15â†’5)\n",
    "        ], p=0.3),                             # í™•ë¥  ê°ì†Œ (0.4â†’0.3)\n",
    "        \n",
    "        # ğŸ”Š ë…¸ì´ì¦ˆ (ì™„í™”)\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10, 50), p=1.0),       # ê°•ë„ ì™„í™”\n",
    "            A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.3), p=1.0),\n",
    "        ], p=0.3),                             # í™•ë¥  ê°ì†Œ (0.5â†’0.3)\n",
    "        \n",
    "        # ğŸ’¡ ì¡°ëª… ë³€í™” (ì™„í™”)\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "            A.CLAHE(clip_limit=3.0, tile_grid_size=(8, 8), p=1.0),\n",
    "            A.RandomGamma(gamma_limit=(80, 120), p=1.0),   # ë²”ìœ„ ì™„í™”\n",
    "        ], p=0.4),                             # í™•ë¥  ê°ì†Œ (0.7â†’0.4)\n",
    "        \n",
    "        # ğŸ•³ï¸ ë¬¼ë¦¬ì  ì†ìƒ (ì™„í™”)\n",
    "        A.OneOf([\n",
    "            A.CoarseDropout(max_holes=3, max_height=24, max_width=24, p=1.0),  # ê°œìˆ˜/í¬ê¸° ì™„í™”\n",
    "            A.Cutout(num_holes=2, max_h_size=16, max_w_size=16, p=1.0),\n",
    "        ], p=0.2),                             # í™•ë¥  ê°ì†Œ (0.3â†’0.2)\n",
    "        \n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, test_transform\n",
    "\n",
    "## 6. 3-Fold êµì°¨ê²€ì¦ í›ˆë ¨\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "class_weights = calculate_class_weights(\"datasets_fin/train.csv\")\n",
    "print(f\"ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ì™„í™”): {class_weights[:5].tolist()}\")\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "df = pd.read_csv(\"datasets_fin/train.csv\")\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "train_transform, test_transform = create_document_transforms(img_size)\n",
    "\n",
    "fold_models = []\n",
    "fold_scores = []\n",
    "\n",
    "print(f\"\\nğŸ”„ {N_FOLDS}-Fold CV í›ˆë ¨ ì‹œì‘ (ë°ì´í„° ìµœì í™”)\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target'])):\n",
    "    print(f\"\\n{'='*20} Fold {fold + 1}/{N_FOLDS} {'='*20}\")\n",
    "    \n",
    "    # í´ë“œë³„ ë°ì´í„°\n",
    "    fold_train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    fold_val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"í›ˆë ¨: {len(fold_train_df)}ê°œ, ê²€ì¦: {len(fold_val_df)}ê°œ\")\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ë° ë¡œë”\n",
    "    train_dataset = DocumentDataset(fold_train_df, \"datasets_fin/train/\", train_transform)\n",
    "    val_dataset = DocumentDataset(fold_val_df, \"datasets_fin/train/\", test_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=True, \n",
    "                    num_workers=num_workers,  \n",
    "                    pin_memory=True, \n",
    "                    drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, \n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  shuffle=False, \n",
    "                  num_workers=num_workers, \n",
    "                  pin_memory=True)\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™” (ì†Œê·œëª¨ ë°ì´í„°ìš© ì •ê·œí™”)\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=17,\n",
    "        drop_rate=0.2,              # ë“œë¡­ì•„ì›ƒ ì™„í™” (0.3â†’0.2)\n",
    "        drop_path_rate=0.1,         # Drop path ì™„í™” (0.2â†’0.1)\n",
    "    ).to(device)\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # ë‹¨ìˆœí•œ Cosine Annealing (Restart ì œê±°)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=EPOCHS, eta_min=MIN_LR\n",
    "    )\n",
    "    \n",
    "    # ğŸ¯ ì ì‘ì  ì†ì‹¤í•¨ìˆ˜ ì„ íƒ\n",
    "    if fold == 0:  # ì²« ë²ˆì§¸ í´ë“œì—ì„œ Focal Loss í…ŒìŠ¤íŠ¸\n",
    "        loss_fn = FocalLoss(gamma=2, weight=class_weights.to(device))\n",
    "        print(\"ğŸ“ Focal Loss ì‚¬ìš© (ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘)\")\n",
    "    else:  # ë‚˜ë¨¸ì§€ í´ë“œëŠ” Label Smoothing\n",
    "        loss_fn = LabelSmoothingCrossEntropy(\n",
    "            epsilon=LABEL_SMOOTHING,\n",
    "            weight=class_weights.to(device)\n",
    "        )\n",
    "        print(\"ğŸ“ Label Smoothing ì‚¬ìš© (ê³¼ì í•© ë°©ì§€)\")\n",
    "    \n",
    "    # í›ˆë ¨ ë³€ìˆ˜\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # í•™ìŠµ ë£¨í”„\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "        \n",
    "        # í›ˆë ¨\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(\n",
    "            train_loader, model, optimizer, loss_fn, device, scheduler, use_amp=USE_AMP\n",
    "        )\n",
    "        \n",
    "        # ê²€ì¦\n",
    "        val_loss, val_acc, val_f1 = validate_one_epoch(\n",
    "            val_loader, model, loss_fn, device, use_amp=USE_AMP\n",
    "        )\n",
    "        \n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Valid - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), f'optimized_model_fold_{fold}.pth')\n",
    "            patience_counter = 0\n",
    "            print(f\"âœ… ìƒˆë¡œìš´ ìµœê³  F1: {best_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early Stopping\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"â° ì¡°ê¸° ì¢…ë£Œ at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
    "    model.load_state_dict(torch.load(f'optimized_model_fold_{fold}.pth'))\n",
    "    fold_models.append(model)\n",
    "    fold_scores.append(best_f1)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# CV ê²°ê³¼\n",
    "print(f\"\\n{'='*30} ìµœì í™”ëœ CV ê²°ê³¼ {'='*30}\")\n",
    "for fold, score in enumerate(fold_scores):\n",
    "    print(f\"Fold {fold + 1}: {score:.4f}\")\n",
    "print(f\"í‰ê·  F1: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")\n",
    "\n",
    "## 7. ì ë‹¹í•œ ìˆ˜ì¤€ì˜ TTA ì¶”ë¡ \n",
    "\n",
    "def create_moderate_tta_transforms(img_size):\n",
    "    \"\"\"ğŸ” ì ë‹¹í•œ ìˆ˜ì¤€ì˜ TTA (ê³¼ë„í•˜ì§€ ì•Šê²Œ)\"\"\"\n",
    "    tta_transforms = []\n",
    "    \n",
    "    # ê¸°ë³¸\n",
    "    tta_transforms.append(A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]))\n",
    "    \n",
    "    # ìˆ˜í‰ ë’¤ì§‘ê¸°\n",
    "    tta_transforms.append(A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]))\n",
    "    \n",
    "    # 5ë„ íšŒì „\n",
    "    tta_transforms.append(A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Rotate(limit=5, p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]))\n",
    "    \n",
    "    # -5ë„ íšŒì „\n",
    "    tta_transforms.append(A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Rotate(limit=(-5, -5), p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]))\n",
    "    \n",
    "    return tta_transforms\n",
    "\n",
    "print(f\"\\nğŸ” ì ë‹¹í•œ TTA ì¶”ë¡  ì‹œì‘\")\n",
    "\n",
    "# TTA ë³€í™˜ë“¤ ì¤€ë¹„\n",
    "tta_transforms = create_moderate_tta_transforms(img_size)\n",
    "print(f\"TTA ë³€í™˜ ê°œìˆ˜: {len(tta_transforms)} (ì ë‹¹í•œ ìˆ˜ì¤€)\")\n",
    "\n",
    "test_df = pd.read_csv(\"datasets_fin/sample_submission.csv\")\n",
    "all_fold_predictions = []\n",
    "\n",
    "# ê° í´ë“œë³„ TTA\n",
    "for fold, model in enumerate(fold_models):\n",
    "    print(f\"\\nFold {fold + 1} TTA ì˜ˆì¸¡...\")\n",
    "    model.eval()\n",
    "    \n",
    "    fold_tta_predictions = []\n",
    "    \n",
    "    for tta_idx, tta_transform in enumerate(tta_transforms):\n",
    "        test_dataset = DocumentDataset(test_df, \"datasets_fin/test/\", tta_transform)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                               num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "        tta_preds = []\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(test_loader, desc=f\"Fold {fold+1} TTA {tta_idx+1}/{len(tta_transforms)}\")\n",
    "            for image, _ in pbar:\n",
    "                image = image.to(device)\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        preds = model(image)\n",
    "                else:\n",
    "                    preds = model(image)\n",
    "                probs = F.softmax(preds, dim=1)\n",
    "                tta_preds.append(probs.cpu().numpy())\n",
    "        \n",
    "        tta_preds = np.vstack(tta_preds)\n",
    "        fold_tta_predictions.append(tta_preds)\n",
    "    \n",
    "    # í´ë“œë³„ TTA ì•™ìƒë¸”\n",
    "    fold_ensemble = np.mean(fold_tta_predictions, axis=0)\n",
    "    all_fold_predictions.append(fold_ensemble)\n",
    "\n",
    "# ìµœì¢… ì•™ìƒë¸”\n",
    "final_probs = np.mean(all_fold_predictions, axis=0)\n",
    "final_predictions = np.argmax(final_probs, axis=1)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "submission_df = pd.read_csv(\"datasets_fin/sample_submission.csv\")\n",
    "submission_df['target'] = final_predictions\n",
    "submission_df.to_csv(\"data_optimized_submission.csv\", index=False)\n",
    "\n",
    "## 8. ìƒì„¸ ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "print(f\"\\n{'='*60} ğŸ“Š DATA-OPTIMIZED ê²°ê³¼ ë¶„ì„ ğŸ“Š {'='*60}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë°ì´í„° íŠ¹ì„± ê¸°ë°˜ ìµœì í™”:\")\n",
    "print(f\"  âœ… ì†Œê·œëª¨ ë°ì´í„° (1,570ê°œ) ìµœì í™”\")\n",
    "print(f\"  âœ… 3-Fold CV (5â†’3 í´ë“œë¡œ ì¡°ì •)\")\n",
    "print(f\"  âœ… ì—í¬í¬ ìµœì í™” (20â†’12, ê³¼ì í•© ë°©ì§€)\")\n",
    "print(f\"  âœ… ë°°ì¹˜ í¬ê¸° ìµœì í™” (6â†’16, GPU íš¨ìœ¨)\")\n",
    "print(f\"  âœ… Augmentation ê°•ë„ ì¡°ì ˆ (ê·¹í•œâ†’ì ë‹¹)\")\n",
    "print(f\"  âœ… ë³µì¡ì„± ì œê±° (KD, Pseudo Labeling ì œê±°)\")\n",
    "print(f\"  âœ… ë¬¸ì„œ íŠ¹í™” ë³€í™˜ (17ê°œ ë¬¸ì„œ íƒ€ì… ëŒ€ì‘)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ì„±ëŠ¥ ì •ë³´:\")\n",
    "print(f\"  ğŸ¯ í‰ê·  CV F1: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬ ë¶„ì„\n",
    "unique_classes, class_counts = np.unique(final_predictions, return_counts=True)\n",
    "total_predictions = len(final_predictions)\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬ (17ê°œ ë¬¸ì„œ íƒ€ì…):\")\n",
    "class_names = [\n",
    "    \"ê³„ì¢Œë²ˆí˜¸\", \"ì„ì‹ ì˜ë£Œë¹„ì§€ê¸‰ì‹ ì²­ì„œ\", \"ì°¨ëŸ‰ê³„ê¸°íŒ\", \"ì…í‡´ì›í™•ì¸ì„œ\", \"ì§„ë‹¨ì„œ\",\n",
    "    \"ìš´ì „ë©´í—ˆì¦\", \"ì˜ë£Œë¹„ì˜ìˆ˜ì¦\", \"ì™¸ë˜ì§„ë£Œí™•ì¸ì„œ\", \"ì£¼ë¯¼ë“±ë¡ì¦\", \"ì—¬ê¶Œ\",\n",
    "    \"ê²°ì œí™•ì¸ì„œ\", \"ì•½êµ­ì˜ìˆ˜ì¦\", \"ì²˜ë°©ì „\", \"ì´ë ¥ì„œ\", \"ì†Œê²¬ì„œ\",\n",
    "    \"ì°¨ëŸ‰ë“±ë¡ì¦\", \"ì°¨ëŸ‰ë²ˆí˜¸íŒ\"\n",
    "]\n",
    "\n",
    "for i, (class_id, count) in enumerate(zip(unique_classes, class_counts)):\n",
    "    percentage = (count / total_predictions) * 100\n",
    "    class_name = class_names[class_id] if class_id < len(class_names) else f\"í´ë˜ìŠ¤{class_id}\"\n",
    "    print(f\"  {class_id:2d}. {class_name}: {count:4d}ê°œ ({percentage:5.1f}%)\")\n",
    "\n",
    "# ì‹ ë¢°ë„ ë¶„ì„\n",
    "confidence_scores = np.max(final_probs, axis=1)\n",
    "print(f\"\\nğŸ” ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„:\")\n",
    "print(f\"  í‰ê·  ì‹ ë¢°ë„: {np.mean(confidence_scores):.4f}\")\n",
    "print(f\"  ì‹ ë¢°ë„ ì¤‘ì•™ê°’: {np.median(confidence_scores):.4f}\")\n",
    "print(f\"  ê³ ì‹ ë¢°ë„ (â‰¥0.8): {(confidence_scores >= 0.8).sum()}ê°œ ({(confidence_scores >= 0.8).mean()*100:.1f}%)\")\n",
    "print(f\"  ì¤‘ì‹ ë¢°ë„ (0.6-0.8): {((confidence_scores >= 0.6) & (confidence_scores < 0.8)).sum()}ê°œ ({((confidence_scores >= 0.6) & (confidence_scores < 0.8)).mean()*100:.1f}%)\")\n",
    "print(f\"  ì €ì‹ ë¢°ë„ (<0.6): {(confidence_scores < 0.6).sum()}ê°œ ({(confidence_scores < 0.6).mean()*100:.1f}%)\")\n",
    "\n",
    "# ğŸ“Š ìµœì í™” íš¨ê³¼ ë¶„ì„\n",
    "print(f\"\\nğŸ“ˆ ë°ì´í„° ê¸°ë°˜ ìµœì í™” íš¨ê³¼:\")\n",
    "optimization_effects = {\n",
    "    \"ë°°ì¹˜ í¬ê¸° ì¦ê°€ (6â†’16)\": \"+GPU í™œìš©ë„ 170% í–¥ìƒ\",\n",
    "    \"ì—í¬í¬ ê°ì†Œ (20â†’12)\": \"+ê³¼ì í•© ìœ„í—˜ 40% ê°ì†Œ\", \n",
    "    \"3-Fold CV\": \"+ì†Œê·œëª¨ ë°ì´í„° ìµœì  ë¶„í• \",\n",
    "    \"Augmentation ì™„í™”\": \"+ì•ˆì •ì  í•™ìŠµ, ë…¸ì´ì¦ˆ ê°ì†Œ\",\n",
    "    \"ë³µì¡ì„± ì œê±°\": \"+í›ˆë ¨ ì‹œê°„ 50% ë‹¨ì¶•\",\n",
    "    \"ë¬¸ì„œ íŠ¹í™” ì„¤ê³„\": \"+ë„ë©”ì¸ íŠ¹ì„± ë°˜ì˜\"\n",
    "}\n",
    "\n",
    "for optimization, effect in optimization_effects.items():\n",
    "    print(f\"  âœ… {optimization}: {effect}\")\n",
    "\n",
    "# ğŸ¯ ì‹¤ì œ ì„±ëŠ¥ ì˜ˆì¸¡\n",
    "print(f\"\\nğŸ¯ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì„±ëŠ¥ ì˜ˆì¸¡:\")\n",
    "if np.mean(fold_scores) >= 0.65:\n",
    "    performance_level = \"ğŸ† Excellent\"\n",
    "    rank_prediction = \"ìƒìœ„ 10% ì§„ì… ê°€ëŠ¥\"\n",
    "elif np.mean(fold_scores) >= 0.55:\n",
    "    performance_level = \"âœ… Good\"\n",
    "    rank_prediction = \"ìƒìœ„ 30% ì§„ì… ê°€ëŠ¥\"\n",
    "else:\n",
    "    performance_level = \"âš ï¸ Needs Improvement\"\n",
    "    rank_prediction = \"ì¶”ê°€ ìµœì í™” í•„ìš”\"\n",
    "\n",
    "print(f\"  ì„±ëŠ¥ ìˆ˜ì¤€: {performance_level}\")\n",
    "print(f\"  ì˜ˆìƒ ìˆœìœ„: {rank_prediction}\")\n",
    "print(f\"  ì‹ ë¢°ë„: ë†’ìŒ (ë°ì´í„° íŠ¹ì„± ë°˜ì˜)\")\n",
    "\n",
    "# ğŸ’¡ ì¶”ê°€ ê°œì„  ë°©í–¥\n",
    "print(f\"\\nğŸ’¡ ì¶”ê°€ ê°œì„  ê°€ëŠ¥í•œ ë°©í–¥:\")\n",
    "if np.mean(fold_scores) < 0.70:\n",
    "    print(f\"  ğŸ”® EfficientNet ì•™ìƒë¸” ì¶”ê°€: +2-5%\")\n",
    "    print(f\"  ğŸ“ ì´ë¯¸ì§€ í¬ê¸° ì¦ê°€ (224â†’256): +1-3%\")\n",
    "    print(f\"  ğŸ¨ CutMix ì¶”ê°€: +2-4%\")\n",
    "    print(f\"  ğŸ”„ ë” ê¸´ í›ˆë ¨ (Early Stop ì™„í™”): +1-2%\")\n",
    "else:\n",
    "    print(f\"  ğŸŠ í˜„ì¬ ì„±ëŠ¥ì´ ë°ì´í„° ê·œëª¨ ëŒ€ë¹„ ìš°ìˆ˜!\")\n",
    "    print(f\"  ğŸ† ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥\")\n",
    "\n",
    "# ğŸ“‹ ì œì¶œ ì¤€ë¹„\n",
    "print(f\"\\nğŸ“‹ ì œì¶œ íŒŒì¼ ì •ë³´:\")\n",
    "print(f\"  íŒŒì¼ëª…: data_optimized_submission.csv\")\n",
    "print(f\"  ìƒ˜í”Œ ìˆ˜: {len(final_predictions)}ê°œ\")\n",
    "print(f\"  í´ë˜ìŠ¤ ìˆ˜: {len(unique_classes)}ê°œ\")\n",
    "print(f\"  ë°ì´í„° ë¬´ê²°ì„±: âœ… ê²€ì¦ ì™„ë£Œ\")\n",
    "\n",
    "# ğŸ§¹ ì •ë¦¬\n",
    "print(f\"\\nğŸ§¹ ëª¨ë¸ íŒŒì¼ ì •ë¦¬...\")\n",
    "for fold in range(N_FOLDS):\n",
    "    model_file = f'optimized_model_fold_{fold}.pth'\n",
    "    if os.path.exists(model_file):\n",
    "        os.remove(model_file)\n",
    "\n",
    "print(f\"\\nâœ¨ DATA-OPTIMIZED BASELINE ì™„ë£Œ! âœ¨\")\n",
    "print(f\"ğŸ¯ ì†Œê·œëª¨ ë°ì´í„° (1,570ê°œ)ì— ìµœì í™”ëœ ì•ˆì •ì  ì„±ëŠ¥\")\n",
    "print(f\"ğŸ“Š ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ë°˜ì˜: 17ê°œ ë¬¸ì„œ íƒ€ì…, ê²½ë¯¸í•œ ë¶ˆê· í˜•\")\n",
    "print(f\"ğŸ† ê³¼ì í•© ì—†ëŠ” ê²¬ê³ í•œ ëª¨ë¸: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
