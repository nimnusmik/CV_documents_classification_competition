{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸš€ GT Generation ë…¸íŠ¸ë¶ ì‹œì‘!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: í™˜ê²½ ì„¤ì • & ë°ì´í„° ë¡œë“œ\n",
    "# =============================================================================\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)\n",
    "TEST_PATH = \"./data/raw/test\"  # test ì´ë¯¸ì§€ í´ë” ê²½ë¡œ\n",
    "META_PATH = \"./data/raw/meta.csv\"  # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ë¡œë“œ\n",
    "meta_df = pd.read_csv(META_PATH)\n",
    "print(\"ğŸ“‹ í´ë˜ìŠ¤ ì •ë³´:\")\n",
    "for idx, row in meta_df.iterrows():\n",
    "    print(f\"  {row['target']:2d}: {row['class_name']}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Test ì´ë¯¸ì§€ ê²½ë¡œ: {TEST_PATH}\")\n",
    "test_files = [f for f in os.listdir(TEST_PATH) if f.endswith('.jpg')]\n",
    "print(f\"ğŸ“Š ì´ Test ì´ë¯¸ì§€ ìˆ˜: {len(test_files)}ì¥\")\n",
    "\n",
    "# ì·¨ì•½ í´ë˜ìŠ¤ ì •ì˜\n",
    "VULNERABLE_CLASSES = [3, 4, 7, 14]\n",
    "vuln_names = meta_df[meta_df['target'].isin(VULNERABLE_CLASSES)]['class_name'].tolist()\n",
    "print(f\"\\nğŸ¯ ì·¨ì•½ í´ë˜ìŠ¤ ({VULNERABLE_CLASSES}): {vuln_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35603a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: ì´ë¯¸ì§€ í’ˆì§ˆ ì¸¡ì • í•¨ìˆ˜\n",
    "# =============================================================================\n",
    "\n",
    "def assess_image_quality(image_path):\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ í’ˆì§ˆì„ 0-1 ì ìˆ˜ë¡œ í‰ê°€ (ë‚®ì„ìˆ˜ë¡ ì˜¤ì—¼ë¨)\n",
    "    ë¸”ëŸ¬, ëŒ€ë¹„, ë°ê¸° ë¶„í¬ë¥¼ ì¢…í•©í•˜ì—¬ ê³„ì‚°\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return 0.0\n",
    "            \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 1) ë¸”ëŸ¬ ì¸¡ì • (Laplacian variance)\n",
    "        # ë†’ì„ìˆ˜ë¡ ì„ ëª…í•¨\n",
    "        blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        blur_normalized = min(blur_score / 1000.0, 1.0)  # 1000ìœ¼ë¡œ ì •ê·œí™”\n",
    "        \n",
    "        # 2) ëŒ€ë¹„ ì¸¡ì • (í‘œì¤€í¸ì°¨)\n",
    "        # ë†’ì„ìˆ˜ë¡ ëŒ€ë¹„ê°€ ì¢‹ìŒ\n",
    "        contrast_score = gray.std()\n",
    "        contrast_normalized = min(contrast_score / 80.0, 1.0)  # 80ìœ¼ë¡œ ì •ê·œí™”\n",
    "        \n",
    "        # 3) ë°ê¸° ë¶„í¬ ì¸¡ì • (íˆìŠ¤í† ê·¸ë¨ ì—”íŠ¸ë¡œí”¼)\n",
    "        # ë†’ì„ìˆ˜ë¡ ë°ê¸° ë¶„í¬ê°€ ë‹¤ì–‘í•¨\n",
    "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "        hist = hist.flatten()\n",
    "        hist = hist[hist > 0]  # 0ì¸ ê°’ ì œê±°\n",
    "        if len(hist) > 1:\n",
    "            prob = hist / hist.sum()\n",
    "            brightness_score = -np.sum(prob * np.log2(prob + 1e-8))\n",
    "            brightness_normalized = brightness_score / 8.0  # 8ë¡œ ì •ê·œí™” (log2(256))\n",
    "        else:\n",
    "            brightness_normalized = 0.0\n",
    "            \n",
    "        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°\n",
    "        quality_score = (\n",
    "            0.5 * blur_normalized +      # ë¸”ëŸ¬ê°€ ê°€ì¥ ì¤‘ìš”\n",
    "            0.3 * contrast_normalized +   # ëŒ€ë¹„\n",
    "            0.2 * brightness_normalized   # ë°ê¸° ë¶„í¬\n",
    "        )\n",
    "        \n",
    "        return min(quality_score, 1.0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í’ˆì§ˆ ì¸¡ì • ì‹¤íŒ¨ ({image_path}): {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
    "print(\"\\nğŸ§ª í’ˆì§ˆ ì¸¡ì • í•¨ìˆ˜ í…ŒìŠ¤íŠ¸:\")\n",
    "if test_files:\n",
    "    sample_path = os.path.join(TEST_PATH, test_files[0])\n",
    "    sample_score = assess_image_quality(sample_path)\n",
    "    print(f\"  ìƒ˜í”Œ ì´ë¯¸ì§€ ({test_files[0]}): í’ˆì§ˆì ìˆ˜ {sample_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a79dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_preprocessing(image_path, quality_threshold=0.4):\n",
    "    \"\"\"ë¬¸ì„œ ì´ë¯¸ì§€ ì „ë¬¸ê°€ ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ì „ì²˜ë¦¬\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None, 0.0, 0.0\n",
    "        \n",
    "        original_quality = assess_image_quality(image_path)\n",
    "        if original_quality > quality_threshold:\n",
    "            return img, original_quality, original_quality\n",
    "        \n",
    "        # 1) í’ˆì§ˆ ì§„ë‹¨\n",
    "        processed_img = img.copy()\n",
    "        gray = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        g_std = gray.std()  # ëŒ€ë¹„\n",
    "        lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()  # ë¸”ëŸ¬\n",
    "        mean_bright = gray.mean()  # ë°ê¸°\n",
    "        \n",
    "        # ìŠ¤í ê°ë„ ì¸¡ì •\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)\n",
    "        skew_angle = 0\n",
    "        if lines is not None and len(lines) > 5:\n",
    "            angles = []\n",
    "            for line in lines[:20]:\n",
    "                if len(line[0]) >= 2:\n",
    "                    rho, theta = line[0]\n",
    "                    angle = theta * 180 / np.pi - 90\n",
    "                    if abs(angle) < 45:\n",
    "                        angles.append(angle)\n",
    "            if angles:\n",
    "                skew_angle = abs(np.median(angles))\n",
    "        \n",
    "        # 2) ë¼ìš°íŒ… ê²°ì •\n",
    "        needs_deskew = skew_angle >= 8\n",
    "        low_contrast = g_std < 35\n",
    "        is_blurry = 50 <= lap_var < 150\n",
    "        very_blurry = lap_var < 50\n",
    "        too_bright = mean_bright > 180\n",
    "        has_noise = lap_var < 100  # ë…¸ì´ì¦ˆ ì¶”ì •\n",
    "        \n",
    "        # 3) í”„ë¦¬ì…‹ ì„ íƒ\n",
    "        if very_blurry and too_bright and has_noise:\n",
    "            preset = \"HEAVY\"\n",
    "        elif needs_deskew or skew_angle >= 5:\n",
    "            preset = \"MEDIUM\"  \n",
    "        else:\n",
    "            preset = \"LIGHT\"\n",
    "        \n",
    "        # 4) ì „ì²˜ë¦¬ ì ìš© (ê¶Œì¥ ìˆœì„œëŒ€ë¡œ)\n",
    "        \n",
    "        # ë‹¨ê³„ 0: í”Œë¦½ ê°ì§€ ë° ë³´ì • (ëª¨ë“  í”„ë¦¬ì…‹ì— ì ìš©)\n",
    "        flipped = cv2.flip(processed_img, 1)\n",
    "        gray_current = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "        gray_flipped = cv2.cvtColor(flipped, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        def get_text_density(image):\n",
    "            binary = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                         cv2.THRESH_BINARY, 15, 10)\n",
    "            return np.sum(binary == 0) / binary.size\n",
    "        \n",
    "        original_density = get_text_density(gray_current)\n",
    "        flipped_density = get_text_density(gray_flipped)\n",
    "        \n",
    "        if flipped_density > original_density * 1.1:  # 10% ì´ìƒ ì°¨ì´\n",
    "            processed_img = flipped\n",
    "        \n",
    "        # ë‹¨ê³„ 1: Deskew (í•„ìš”ì‹œ)\n",
    "        if needs_deskew and preset in [\"MEDIUM\", \"HEAVY\"]:\n",
    "            h, w = processed_img.shape[:2]\n",
    "            center = (w//2, h//2)\n",
    "            \n",
    "            # ê°ë„ ê³„ì‚° ë¡œì§ ìˆ˜ì •\n",
    "            valid_angles = []\n",
    "            if lines is not None:\n",
    "                for line in lines[:10]:\n",
    "                    if len(line[0]) >= 2:\n",
    "                        rho, theta = line[0]\n",
    "                        angle = theta * 180 / np.pi - 90\n",
    "                        if abs(angle) < 15:\n",
    "                            valid_angles.append(angle)\n",
    "            \n",
    "            if len(valid_angles) > 0:\n",
    "                rotation_angle = np.median(valid_angles)\n",
    "                if abs(rotation_angle) >= 3:\n",
    "                    M = cv2.getRotationMatrix2D(center, rotation_angle, 1.0)\n",
    "                    processed_img = cv2.warpAffine(processed_img, M, (w, h), \n",
    "                                                borderMode=cv2.BORDER_CONSTANT, \n",
    "                                                borderValue=(255, 255, 255))\n",
    "        \n",
    "        # ë‹¨ê³„ 2: ë°ê¸° ì¡°ì • (HEAVYë§Œ)\n",
    "        if too_bright and preset == \"HEAVY\":\n",
    "            processed_img = np.power(processed_img/255.0, 0.92) * 255\n",
    "            processed_img = processed_img.astype(np.uint8)\n",
    "        \n",
    "        # ë‹¨ê³„ 3: NLM ë…¸ì´ì¦ˆ ì œê±° (HEAVYë§Œ)\n",
    "        if has_noise and preset == \"HEAVY\":\n",
    "            processed_img = cv2.fastNlMeansDenoisingColored(processed_img, None, 3, 3, 7, 21)\n",
    "        \n",
    "        # ë‹¨ê³„ 4: CLAHE (ì €ëŒ€ë¹„ì¸ ê²½ìš°)\n",
    "        if low_contrast:\n",
    "            lab = cv2.cvtColor(processed_img, cv2.COLOR_BGR2LAB)\n",
    "            l_channel = lab[:, :, 0]\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "            l_channel = clahe.apply(l_channel)\n",
    "            lab[:, :, 0] = l_channel\n",
    "            processed_img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # ë‹¨ê³„ 5: Unsharp (ë¸”ëŸ¬ì¸ ê²½ìš°)\n",
    "        if is_blurry or very_blurry:\n",
    "            gray = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "            gaussian = cv2.GaussianBlur(gray, (0, 0), 1.0)\n",
    "            unsharp = cv2.addWeighted(gray, 1.4, gaussian, -0.4, 0)\n",
    "            unsharp = np.clip(unsharp, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # HSVì—ì„œ Vì±„ë„ë§Œ êµì²´\n",
    "            hsv = cv2.cvtColor(processed_img, cv2.COLOR_BGR2HSV)\n",
    "            hsv[:, :, 2] = unsharp\n",
    "            processed_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "        \n",
    "        # í’ˆì§ˆ ê²€ì‚¬\n",
    "        temp_path = image_path.replace('.jpg', '_temp_processed.jpg')\n",
    "        cv2.imwrite(temp_path, processed_img)\n",
    "        processed_quality = assess_image_quality(temp_path)\n",
    "        os.remove(temp_path)\n",
    "        \n",
    "        if processed_quality < original_quality * 0.8:\n",
    "            return img, original_quality, original_quality\n",
    "        \n",
    "        return processed_img, original_quality, processed_quality\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ì „ì²˜ë¦¬ ì‹¤íŒ¨ ({image_path}): {e}\")\n",
    "        img = cv2.imread(image_path)\n",
    "        original_quality = assess_image_quality(image_path) if img is not None else 0.0\n",
    "        return img, original_quality, original_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦ ì‹¤í–‰\n",
    "validation_results = test_preprocessing_effects(selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bf36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_after_visualization(selected_images, sample_count=20):\n",
    "    \"\"\"\n",
    "    ì „ì²˜ë¦¬ ì „í›„ ë¹„êµ ì‹œê°í™” (20ê°œ ëœë¤ ìƒ˜í”Œ)\n",
    "    \"\"\"\n",
    "    print(f\"\\nì „ì²˜ë¦¬ íš¨ê³¼ ì‹œê°í™” - {sample_count}ê°œ ëœë¤ ìƒ˜í”Œ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ëœë¤ ìƒ˜í”Œ ì„ íƒ\n",
    "    import random\n",
    "    random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ëœë¤\n",
    "    sample_images = random.sample(selected_images, min(sample_count, len(selected_images)))\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ìš©\n",
    "    results = []\n",
    "    \n",
    "    for i, img_info in enumerate(sample_images):\n",
    "        img_path = os.path.join(TEST_PATH, img_info['filename'])\n",
    "        \n",
    "        # ì „ì²˜ë¦¬ ì ìš©\n",
    "        processed_img, orig_quality, new_quality = adaptive_preprocessing(img_path)\n",
    "        \n",
    "        if processed_img is not None:\n",
    "            results.append({\n",
    "                'filename': img_info['filename'],\n",
    "                'original_quality': orig_quality,\n",
    "                'processed_quality': new_quality,\n",
    "                'improvement': new_quality - orig_quality,\n",
    "                'processed_img': processed_img\n",
    "            })\n",
    "            \n",
    "            print(f\"{i+1:2d}. {img_info['filename'][:20]:20s} | \"\n",
    "                  f\"í’ˆì§ˆ: {orig_quality:.3f} â†’ {new_quality:.3f} | \"\n",
    "                  f\"ê°œì„ : {new_quality - orig_quality:+.3f}\")\n",
    "    \n",
    "    # ì‹œê°í™” (4x5 ê·¸ë¦¬ë“œë¡œ 20ê°œ)\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(25, 20))\n",
    "    fig.suptitle(f'ì „ì²˜ë¦¬ íš¨ê³¼ ë¹„êµ - {len(results)}ê°œ ìƒ˜í”Œ', fontsize=16)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        \n",
    "        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        original_img = cv2.imread(os.path.join(TEST_PATH, result['filename']))\n",
    "        original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ë³€í™˜\n",
    "        processed_img = cv2.cvtColor(result['processed_img'], cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # ì¢Œìš° ë¹„êµ ì´ë¯¸ì§€ ìƒì„±\n",
    "        combined = np.hstack([original_img, processed_img])\n",
    "        \n",
    "        axes[row, col].imshow(combined)\n",
    "        axes[row, col].set_title(f\"{result['filename'][:15]}\\n\"\n",
    "                                f\"í’ˆì§ˆ: {result['original_quality']:.3f} â†’ \"\n",
    "                                f\"{result['processed_quality']:.3f}\\n\"\n",
    "                                f\"ê°œì„ : {result['improvement']:+.3f}\",\n",
    "                                fontsize=8)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67def156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_improvement_analysis(preprocessing_results):\n",
    "    \"\"\"\n",
    "    ì „ì²˜ë¦¬ í’ˆì§ˆ ê°œì„  ìˆ˜ì¹˜ ë¶„ì„\n",
    "    \"\"\"\n",
    "    print(\"\\ní’ˆì§ˆ ê°œì„  ë¶„ì„ ê²°ê³¼\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not preprocessing_results:\n",
    "        print(\"ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # ê°œì„ ë„ ê³„ì‚°\n",
    "    improvements = [r['improvement'] for r in preprocessing_results]\n",
    "    original_qualities = [r['original_quality'] for r in preprocessing_results]\n",
    "    processed_qualities = [r['processed_quality'] for r in preprocessing_results]\n",
    "    \n",
    "    # í†µê³„ ê³„ì‚°\n",
    "    avg_improvement = np.mean(improvements)\n",
    "    positive_count = sum(1 for imp in improvements if imp > 0)\n",
    "    significant_count = sum(1 for imp in improvements if imp > 0.05)  # 5% ì´ìƒ ê°œì„ \n",
    "    \n",
    "    print(f\"ğŸ“Š ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(preprocessing_results)}ê°œ\")\n",
    "    print(f\"ğŸ“ˆ í‰ê·  í’ˆì§ˆ ê°œì„ : {avg_improvement:+.3f}\")\n",
    "    print(f\"ğŸ“ˆ í’ˆì§ˆ í–¥ìƒ ì´ë¯¸ì§€: {positive_count}ê°œ ({positive_count/len(preprocessing_results)*100:.1f}%)\")\n",
    "    print(f\"ğŸ“ˆ ìœ ì˜ë¯¸í•œ ê°œì„  (5%+): {significant_count}ê°œ ({significant_count/len(preprocessing_results)*100:.1f}%)\")\n",
    "    print()\n",
    "    print(f\"ğŸ“Š ì›ë³¸ í’ˆì§ˆ ì ìˆ˜: {np.mean(original_qualities):.3f} Â± {np.std(original_qualities):.3f}\")\n",
    "    print(f\"ğŸ“Š ì „ì²˜ë¦¬ í›„ í’ˆì§ˆ: {np.mean(processed_qualities):.3f} Â± {np.std(processed_qualities):.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # ê°œì„  íš¨ê³¼ë³„ ë¶„ë¥˜\n",
    "    excellent = [r for r in preprocessing_results if r['improvement'] > 0.1]  # 10% ì´ìƒ ê°œì„ \n",
    "    good = [r for r in preprocessing_results if 0.05 <= r['improvement'] <= 0.1]  # 5-10% ê°œì„ \n",
    "    moderate = [r for r in preprocessing_results if 0.02 <= r['improvement'] < 0.05]  # 2-5% ê°œì„ \n",
    "    minimal = [r for r in preprocessing_results if 0 < r['improvement'] < 0.02]  # ë¯¸ë¯¸í•œ ê°œì„ \n",
    "    no_effect = [r for r in preprocessing_results if r['improvement'] <= 0]  # ê°œì„  ì—†ìŒ/ì•…í™”\n",
    "    \n",
    "    print(\"ğŸ“‹ ê°œì„  íš¨ê³¼ë³„ ë¶„ë¥˜:\")\n",
    "    print(f\"   ğŸŒŸ ë›°ì–´ë‚¨ (10%+): {len(excellent)}ê°œ\")\n",
    "    print(f\"   âœ… ì–‘í˜¸ (5-10%): {len(good)}ê°œ\") \n",
    "    print(f\"   ğŸ“Š ë³´í†µ (2-5%): {len(moderate)}ê°œ\")\n",
    "    print(f\"   ğŸ“‰ ë¯¸ë¯¸ (0-2%): {len(minimal)}ê°œ\")\n",
    "    print(f\"   âŒ íš¨ê³¼ì—†ìŒ/ì•…í™”: {len(no_effect)}ê°œ\")\n",
    "    \n",
    "    # ê°€ì¥ ê°œì„ ëœ ì¼€ì´ìŠ¤ ì¶œë ¥\n",
    "    if excellent:\n",
    "        best_case = max(excellent, key=lambda x: x['improvement'])\n",
    "        print(f\"\\nğŸ† ìµœê³  ê°œì„  ì‚¬ë¡€:\")\n",
    "        print(f\"   íŒŒì¼: {best_case['filename']}\")\n",
    "        print(f\"   í’ˆì§ˆ: {best_case['original_quality']:.3f} â†’ {best_case['processed_quality']:.3f}\")\n",
    "        print(f\"   ê°œì„ : +{best_case['improvement']:.3f} ({best_case['improvement']/best_case['original_quality']*100:+.1f}%)\")\n",
    "    \n",
    "    # ê²°ë¡ \n",
    "    print(\"\\nğŸ¯ ê²°ë¡ :\")\n",
    "    if avg_improvement > 0.05:\n",
    "        print(\"   âœ… ì „ì²˜ë¦¬ íš¨ê³¼ ë›°ì–´ë‚¨ - ì‹¤ì œ ì¶”ë¡ ì— ì ìš© ê¶Œì¥\")\n",
    "    elif avg_improvement > 0.02:\n",
    "        print(\"   ğŸ“Š ì „ì²˜ë¦¬ íš¨ê³¼ ë³´í†µ - ì¶”ê°€ ì‹¤í—˜ í›„ ê²°ì •\")\n",
    "    else:\n",
    "        print(\"   âŒ ì „ì²˜ë¦¬ íš¨ê³¼ ë¯¸ë¯¸ - ë‹¤ë¥¸ ë°©ë²• ëª¨ìƒ‰ í•„ìš”\")\n",
    "    \n",
    "    return {\n",
    "        'avg_improvement': avg_improvement,\n",
    "        'positive_ratio': positive_count / len(preprocessing_results),\n",
    "        'significant_ratio': significant_count / len(preprocessing_results),\n",
    "        'excellent_cases': len(excellent),\n",
    "        'total_samples': len(preprocessing_results)\n",
    "    }\n",
    "\n",
    "print(\"âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"   - adaptive_preprocessing(): í’ˆì§ˆ ê¸°ë°˜ ì ì‘í˜• ì „ì²˜ë¦¬\")  \n",
    "print(\"   - before_after_visualization(): ì „í›„ ë¹„êµ ì‹œê°í™”\")\n",
    "print(\"   - quality_improvement_analysis(): ìˆ˜ì¹˜ ë¶„ì„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf885270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: ë©”ì¸ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "# =============================================================================\n",
    "\n",
    "class SimpleTestDataset(Dataset):\n",
    "    \"\"\"ë©”ì¸ëª¨ë¸ ì¶”ë¡ ìš© ê°„ë‹¨í•œ ë°ì´í„°ì…‹\"\"\"\n",
    "    def __init__(self, image_files, test_path, img_size=384):\n",
    "        self.image_files = image_files\n",
    "        self.test_path = test_path\n",
    "        self.img_size = img_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.test_path, self.image_files[idx])\n",
    "        \n",
    "        try:        \n",
    "            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize((self.img_size, self.img_size))\n",
    "            img = np.array(img, dtype=np.float32) / 255.0\n",
    "            \n",
    "            # ì •ê·œí™” (ImageNet í†µê³„)\n",
    "            mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "            std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "            img = (img - mean) / std\n",
    "            \n",
    "            # CHW í˜•íƒœë¡œ ë³€í™˜\n",
    "            img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "            \n",
    "            return img, self.image_files[idx]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ({self.image_files[idx]}): {e}\")\n",
    "            # ì—ëŸ¬ ì‹œ ë”ë¯¸ í…ì„œ ë°˜í™˜ (float32)\n",
    "            dummy_tensor = torch.zeros(3, self.img_size, self.img_size, dtype=torch.float32)\n",
    "            return dummy_tensor, self.image_files[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: ë©”ì¸ëª¨ë¸ ë¡œë“œ ë° ì·¨ì•½í´ë˜ìŠ¤ í›„ë³´ ì¶”ì¶œ\n",
    "# =============================================================================\n",
    "\n",
    "# def extract_vulnerable_candidates():\n",
    "#     \"\"\"ë©”ì¸ëª¨ë¸ë¡œ ì·¨ì•½í´ë˜ìŠ¤ í›„ë³´ ì´ë¯¸ì§€ë“¤ì„ ì¶”ì¶œ\"\"\"\n",
    "#     print(\"\\nğŸ”„ ë©”ì¸ëª¨ë¸ë¡œ ì·¨ì•½í´ë˜ìŠ¤ í›„ë³´ ì¶”ì¶œ ì¤‘...\")\n",
    "    \n",
    "#     # TODO: ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”\n",
    "#     # ì—¬ê¸°ì„œëŠ” ë”ë¯¸ ë°ì´í„°ë¡œ ëŒ€ì²´\n",
    "#     print(\"âš ï¸  ì‹¤ì œ êµ¬í˜„ ì‹œ ëª¨ë¸ ë¡œë“œ ì½”ë“œë¡œ êµì²´ í•„ìš”\")\n",
    "#     print(\"   í˜„ì¬ëŠ” ë”ë¯¸ ë°ì´í„°ë¡œ ì§„í–‰\")\n",
    "    \n",
    "#     # ë”ë¯¸ ì¶”ë¡  ê²°ê³¼ ìƒì„± (ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¶”ë¡  ê²°ê³¼ ì‚¬ìš©)\n",
    "#     # ì·¨ì•½í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ì´ë¯¸ì§€ë“¤ì„ ì„ì˜ë¡œ ì„ íƒ\n",
    "#     np.random.seed(42)\n",
    "    \n",
    "#     # ì „ì²´ ì´ë¯¸ì§€ ì¤‘ 20-30% ì •ë„ê°€ ì·¨ì•½í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡ëœë‹¤ê³  ê°€ì •\n",
    "#     n_candidates = min(int(len(test_files) * 0.25), 1200)\n",
    "#     candidate_indices = np.random.choice(len(test_files), n_candidates, replace=False)\n",
    "    \n",
    "#     # í›„ë³´ ì´ë¯¸ì§€ íŒŒì¼ëª…ê³¼ ì˜ˆì¸¡ í´ë˜ìŠ¤ ìƒì„±\n",
    "#     candidates = []\n",
    "#     for idx in candidate_indices:\n",
    "#         filename = test_files[idx]\n",
    "#         predicted_class = np.random.choice(VULNERABLE_CLASSES)\n",
    "#         candidates.append({\n",
    "#             'filename': filename,\n",
    "#             'predicted_class': predicted_class,\n",
    "#             'confidence': np.random.uniform(0.3, 0.9)  # ë”ë¯¸ ì‹ ë¢°ë„\n",
    "#         })\n",
    "    \n",
    "#     print(f\"âœ… ì·¨ì•½í´ë˜ìŠ¤ í›„ë³´ {len(candidates)}ì¥ ì¶”ì¶œ ì™„ë£Œ\")\n",
    "#     print(f\"   í´ë˜ìŠ¤ë³„ ë¶„í¬: {Counter(c['predicted_class'] for c in candidates)}\")\n",
    "    \n",
    "#     return candidates\n",
    "\n",
    "def load_ensemble_models(model_paths):\n",
    "    \"\"\"5-fold ì•™ìƒë¸” ëª¨ë¸ë“¤ì„ ë¡œë“œ\"\"\"\n",
    "    models = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for i, model_path in enumerate(model_paths):\n",
    "        print(f\"   ğŸ“‚ Fold {i+1} ëª¨ë¸ ë¡œë”©: {model_path}\")\n",
    "        \n",
    "        # ConvNeXt Base ëª¨ë¸ ìƒì„± (ì‹¤ì œ ëª¨ë¸ì— ë§ê²Œ ìˆ˜ì •)\n",
    "        model = timm.create_model('convnext_base_384_in22ft1k', \n",
    "                                  pretrained=False, \n",
    "                                  num_classes=17)\n",
    "        \n",
    "        # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # state_dict í‚¤ ì²˜ë¦¬ (ì €ì¥ ë°©ì‹ì— ë”°ë¼ ì¡°ì • í•„ìš”)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        elif 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        print(f\"   âœ… Fold {i+1} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    return models, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vulnerable_candidates():\n",
    "    \"\"\"ë©”ì¸ëª¨ë¸ë¡œ ì·¨ì•½í´ë˜ìŠ¤ í›„ë³´ ì´ë¯¸ì§€ë“¤ì„ ì¶”ì¶œ\"\"\"\n",
    "    print(\"\\nğŸ”„ 5-fold ì•™ìƒë¸” ëª¨ë¸ë¡œ ì·¨ì•½í´ë˜ìŠ¤ í›„ë³´ ì¶”ì¶œ ì¤‘...\")\n",
    "\n",
    "    MODEL_PATHS = [\n",
    "        # ì˜ˆì‹œ ê²½ë¡œ - ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”\n",
    "        \"./notebooks/team/KBH/models/fold_1_best.pth\",\n",
    "        \"./notebooks/team/KBH/models/fold_2_best.pth\", \n",
    "        \"./notebooks/team/KBH/models/fold_3_best.pth\",\n",
    "        \"./notebooks/team/KBH/models/fold_4_best.pth\",\n",
    "        \"./notebooks/team/KBH/models/fold_5_best.pth\"\n",
    "    ]\n",
    "    \n",
    "    # ì‹¤ì œ íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "    missing_files = [path for path in MODEL_PATHS if not os.path.exists(path)]\n",
    "    if missing_files:\n",
    "        print(\"âŒ ë‹¤ìŒ ëª¨ë¸ íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:\")\n",
    "        for missing in missing_files:\n",
    "            print(f\"   {missing}\")\n",
    "        print(\"\\nğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "        print(\"   1) MODEL_PATHSë¥¼ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •\")\n",
    "        print(\"   2) ë˜ëŠ” ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰í•˜ë ¤ë©´ return ë¬¸ ë’¤ì˜ ë”ë¯¸ ì½”ë“œ ì‚¬ìš©\")\n",
    "        print(\"   3) main.ipynbì—ì„œ ëª¨ë¸ íŒŒì¼ë“¤ì„ í™•ì¸ í›„ ê²½ë¡œ ë³µì‚¬\")\n",
    "        \n",
    "        # ë”ë¯¸ ëª¨ë“œë¡œ ëŒ€ì²´\n",
    "        print(\"\\nğŸ”„ ë”ë¯¸ ëª¨ë“œë¡œ ì§„í–‰...\")\n",
    "        return extract_vulnerable_candidates_dummy()\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        models, device = load_ensemble_models(MODEL_PATHS)\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "        test_dataset = SimpleTestDataset(test_files, TEST_PATH, img_size=384)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "        \n",
    "        print(f\"ğŸ“Š ì „ì²´ {len(test_files)}ì¥ ì´ë¯¸ì§€ ì¶”ë¡  ì‹œì‘...\")\n",
    "        \n",
    "        # 5-fold ì•™ìƒë¸” ì¶”ë¡ \n",
    "        all_predictions = []\n",
    "        all_confidences = []\n",
    "        all_filenames = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, filenames) in enumerate(test_loader):\n",
    "                if batch_idx % 20 == 0:\n",
    "                    progress = (batch_idx * test_loader.batch_size) / len(test_files) * 100\n",
    "                    print(f\"   ì§„í–‰ë¥ : {progress:.1f}% ({batch_idx * test_loader.batch_size}/{len(test_files)})\")\n",
    "                \n",
    "                images = images.to(device)\n",
    "                \n",
    "                # 5ê°œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· \n",
    "                ensemble_outputs = []\n",
    "                for model in models:\n",
    "                    outputs = model(images)\n",
    "                    probabilities = torch.softmax(outputs, dim=1)\n",
    "                    ensemble_outputs.append(probabilities)\n",
    "                \n",
    "                # ì•™ìƒë¸” í‰ê· \n",
    "                ensemble_probs = torch.stack(ensemble_outputs).mean(dim=0)\n",
    "                predicted_classes = ensemble_probs.argmax(dim=1)\n",
    "                max_confidences = ensemble_probs.max(dim=1)[0]\n",
    "                \n",
    "                # ê²°ê³¼ ì €ì¥\n",
    "                for i in range(len(filenames)):\n",
    "                    pred_class = predicted_classes[i].cpu().item()\n",
    "                    confidence = max_confidences[i].cpu().item()\n",
    "                    filename = filenames[i]\n",
    "                    \n",
    "                    all_predictions.append(pred_class)\n",
    "                    all_confidences.append(confidence)\n",
    "                    all_filenames.append(filename)\n",
    "        \n",
    "        # ì·¨ì•½í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡ëœ ì´ë¯¸ì§€ë“¤ë§Œ í•„í„°ë§\n",
    "        candidates = []\n",
    "        for i, (filename, pred_class, confidence) in enumerate(zip(all_filenames, all_predictions, all_confidences)):\n",
    "            if pred_class in VULNERABLE_CLASSES:\n",
    "                candidates.append({\n",
    "                    'filename': filename,\n",
    "                    'predicted_class': pred_class,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        \n",
    "        print(f\"âœ… ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ!\")\n",
    "        print(f\"   ì „ì²´ ì´ë¯¸ì§€: {len(test_files)}ì¥\")\n",
    "        print(f\"   ì·¨ì•½í´ë˜ìŠ¤ í›„ë³´: {len(candidates)}ì¥ ({len(candidates)/len(test_files)*100:.1f}%)\")\n",
    "        print(f\"   í´ë˜ìŠ¤ë³„ ë¶„í¬: {Counter(c['predicted_class'] for c in candidates)}\")\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        del models\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        return candidates\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ ì¶”ë¡  ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "        print(\"ğŸ”„ ë”ë¯¸ ëª¨ë“œë¡œ ëŒ€ì²´...\")\n",
    "        return extract_vulnerable_candidates_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1184ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_vulnerable_candidates_dummy():\n",
    "#     \"\"\"ë”ë¯¸ ë°ì´í„°ë¡œ í›„ë³´ ì¶”ì¶œ (ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ì‹œ ëŒ€ì²´ìš©)\"\"\"\n",
    "#     print(\"âš ï¸  ë”ë¯¸ ëª¨ë“œë¡œ ì·¨ì•½í´ë˜ìŠ¤ í›„ë³´ ìƒì„±\")\n",
    "    \n",
    "#     np.random.seed(42)\n",
    "#     n_candidates = min(int(len(test_files) * 0.25), 1200)\n",
    "#     candidate_indices = np.random.choice(len(test_files), n_candidates, replace=False)\n",
    "    \n",
    "#     candidates = []\n",
    "#     for idx in candidate_indices:\n",
    "#         filename = test_files[idx]\n",
    "#         predicted_class = np.random.choice(VULNERABLE_CLASSES)\n",
    "#         candidates.append({\n",
    "#             'filename': filename,\n",
    "#             'predicted_class': predicted_class,\n",
    "#             'confidence': np.random.uniform(0.3, 0.9)\n",
    "#         })\n",
    "    \n",
    "#     print(f\"âœ… ë”ë¯¸ ì·¨ì•½í´ë˜ìŠ¤ í›„ë³´ {len(candidates)}ì¥ ìƒì„± ì™„ë£Œ\")\n",
    "#     return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›„ë³´ ì¶”ì¶œ ì‹¤í–‰\n",
    "vulnerable_candidates = extract_vulnerable_candidates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84620efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: ì˜¤ì—¼ë„ ê¸°ë°˜ ì´ë¯¸ì§€ ì„ ë³„\n",
    "# =============================================================================\n",
    "\n",
    "def select_most_corrupted(candidates, target_count=500):\n",
    "    \"\"\"ì˜¤ì—¼ë„ê°€ ë†’ì€ ì´ë¯¸ì§€ë“¤ì„ ì„ ë³„\"\"\"\n",
    "    print(f\"\\nğŸ“Š ì˜¤ì—¼ë„ ê¸°ë°˜ ìƒìœ„ {target_count}ì¥ ì„ ë³„ ì¤‘...\")\n",
    "    \n",
    "    # ê° í›„ë³´ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì ìˆ˜ ê³„ì‚°\n",
    "    quality_scores = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"   ì§„í–‰ë¥ : {i}/{len(candidates)} ({i/len(candidates)*100:.1f}%)\")\n",
    "        \n",
    "        img_path = os.path.join(TEST_PATH, candidate['filename'])\n",
    "        quality_score = assess_image_quality(img_path)\n",
    "        \n",
    "        quality_scores.append({\n",
    "            'filename': candidate['filename'],\n",
    "            'predicted_class': candidate['predicted_class'],\n",
    "            'confidence': candidate['confidence'],\n",
    "            'quality_score': quality_score\n",
    "        })\n",
    "    \n",
    "    # í’ˆì§ˆì ìˆ˜ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (ì˜¤ì—¼ë„ ë†’ì€ ìˆœ)\n",
    "    quality_scores.sort(key=lambda x: x['quality_score'])\n",
    "    \n",
    "    # ìƒìœ„ target_countê°œ ì„ íƒ\n",
    "    selected = quality_scores[:target_count]\n",
    "    \n",
    "    print(f\"âœ… ìµœì¢… {len(selected)}ì¥ ì„ ë³„ ì™„ë£Œ\")\n",
    "    print(f\"   í’ˆì§ˆì ìˆ˜ ë²”ìœ„: {selected[0]['quality_score']:.3f} ~ {selected[-1]['quality_score']:.3f}\")\n",
    "    \n",
    "    # CSVë¡œ ì €ì¥\n",
    "    selected_df = pd.DataFrame(selected)\n",
    "    selected_df.to_csv('selected_images_500.csv', index=False)\n",
    "    print(\"ğŸ’¾ selected_images_500.csv íŒŒì¼ ì €ì¥ ì™„ë£Œ\")\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8985a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜¤ì—¼ë„ ê¸°ë°˜ ì„ ë³„ ì‹¤í–‰\n",
    "selected_images = select_most_corrupted(vulnerable_candidates, target_count=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5.5: ì „ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦ (STEP 5 ë‹¤ìŒì— ì‚½ì…)  \n",
    "# =============================================================================\n",
    "\n",
    "def test_preprocessing_effects(selected_images):\n",
    "    \"\"\"\n",
    "    ì„ ë³„ëœ ì˜¤ì—¼ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬ íš¨ê³¼ í…ŒìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ§ª ì „ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦ ì‹œì‘!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ğŸ“‹ ê²€ì¦ ë‹¨ê³„:\")\n",
    "    print(\"   1ï¸âƒ£ 20ê°œ ëœë¤ ìƒ˜í”Œ ì „ì²˜ë¦¬ ì ìš©\")  \n",
    "    print(\"   2ï¸âƒ£ ì „í›„ ë¹„êµ ì‹œê°í™”\")\n",
    "    print(\"   3ï¸âƒ£ í’ˆì§ˆ ê°œì„  ìˆ˜ì¹˜ ë¶„ì„\")\n",
    "    print(\"   4ï¸âƒ£ íš¨ê³¼ íŒì • ë° ê¶Œì¥ì‚¬í•­\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # STEP 1: ì „ì²˜ë¦¬ ì ìš© ë° ì‹œê°í™”\n",
    "    print(\"\\n1ï¸âƒ£ ì „ì²˜ë¦¬ ì ìš© ë° ì‹œê°í™” ì§„í–‰ì¤‘...\")\n",
    "    preprocessing_results = before_after_visualization(selected_images, sample_count=20)\n",
    "    \n",
    "    # STEP 2: ìˆ˜ì¹˜ ë¶„ì„\n",
    "    print(\"\\n2ï¸âƒ£ í’ˆì§ˆ ê°œì„  ìˆ˜ì¹˜ ë¶„ì„ ì§„í–‰ì¤‘...\")\n",
    "    analysis_results = quality_improvement_analysis(preprocessing_results)\n",
    "    \n",
    "    # STEP 3: íš¨ê³¼ íŒì •\n",
    "    print(\"\\n3ï¸âƒ£ ìµœì¢… íš¨ê³¼ íŒì •\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    avg_improvement = analysis_results['avg_improvement']\n",
    "    positive_ratio = analysis_results['positive_ratio'] \n",
    "    significant_ratio = analysis_results['significant_ratio']\n",
    "    \n",
    "    # íŒì • ê¸°ì¤€\n",
    "    if avg_improvement >= 0.05 and significant_ratio >= 0.4:\n",
    "        recommendation = \"ğŸŒŸ ì „ì²˜ë¦¬ ì ìš© ê°•ë ¥ ê¶Œì¥\"\n",
    "        action = \"inf_new.ipynbì— ì „ì²˜ë¦¬ ë¡œì§ í†µí•© í›„ ì „ì²´ ì¶”ë¡  ì‹¤í–‰\"\n",
    "        confidence = \"ë†’ìŒ\"\n",
    "    elif avg_improvement >= 0.02 and positive_ratio >= 0.6:\n",
    "        recommendation = \"âœ… ì „ì²˜ë¦¬ ì ìš© ê¶Œì¥\" \n",
    "        action = \"ì¶”ê°€ ìƒ˜í”Œë¡œ ê²€ì¦ í›„ ì ìš© ì—¬ë¶€ ê²°ì •\"\n",
    "        confidence = \"ë³´í†µ\"\n",
    "    else:\n",
    "        recommendation = \"âŒ ì „ì²˜ë¦¬ íš¨ê³¼ ë¶€ì¡±\"\n",
    "        action = \"ë‹¤ë¥¸ ì „ì²˜ë¦¬ ë°©ë²• ë˜ëŠ” ì¦ê°• ê¸°ë²• ëª¨ìƒ‰\"\n",
    "        confidence = \"ë‚®ìŒ\"\n",
    "    \n",
    "    print(f\"ğŸ“Š íŒì • ê²°ê³¼: {recommendation}\")\n",
    "    print(f\"ğŸ¯ ê¶Œì¥ ì¡°ì¹˜: {action}\")  \n",
    "    print(f\"ğŸ” ì‹ ë¢°ë„: {confidence}\")\n",
    "    \n",
    "    # ìƒì„¸ ìˆ˜ì¹˜\n",
    "    print(\"\\nğŸ“ˆ ìƒì„¸ ì§€í‘œ:\")\n",
    "    print(f\"   â€¢ í‰ê·  í’ˆì§ˆ ê°œì„ : {avg_improvement:+.3f}\")\n",
    "    print(f\"   â€¢ ê°œì„ ëœ ì´ë¯¸ì§€ ë¹„ìœ¨: {positive_ratio*100:.1f}%\")\n",
    "    print(f\"   â€¢ ìœ ì˜ë¯¸í•œ ê°œì„  ë¹„ìœ¨: {significant_ratio*100:.1f}%\")  \n",
    "    print(f\"   â€¢ ë›°ì–´ë‚œ ê°œì„  ì‚¬ë¡€: {analysis_results['excellent_cases']}ê°œ\")\n",
    "    \n",
    "    # ë‹¤ìŒ ë‹¨ê³„ ê°€ì´ë“œ\n",
    "    print(\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    if avg_improvement >= 0.02:\n",
    "        print(\"   1. inf_new.ipynb ì½”ë“œì— ì „ì²˜ë¦¬ í•¨ìˆ˜ í†µí•©\")\n",
    "        print(\"   2. TTAImageDataset.__getitem__ì— adaptive_preprocessing ì¶”ê°€\")\n",
    "        print(\"   3. ì „ì²´ 3140ê°œ ì´ë¯¸ì§€ë¡œ ì¶”ë¡  ì‹¤í–‰\")\n",
    "        print(\"   4. ì œì¶œ ì ìˆ˜ ë¹„êµ (ì „ì²˜ë¦¬ ì „ vs í›„)\")\n",
    "    else:\n",
    "        print(\"   1. ë‹¤ë¥¸ ì „ì²˜ë¦¬ ê¸°ë²• ì‹œë„:\")\n",
    "        print(\"      - ë” ê°•í•œ ë…¸ì´ì¦ˆ ì œê±°\")\n",
    "        print(\"      - íšŒì „ ê°ë„ ë³´ì •\")  \n",
    "        print(\"      - ê¹Šì´ í•™ìŠµ ê¸°ë°˜ super-resolution\")\n",
    "        print(\"   2. ë˜ëŠ” ë°ì´í„° ì¦ê°• ê°•í™”\")\n",
    "        print(\"   3. ë˜ëŠ” ëª¨ë¸ ì•™ìƒë¸” ìµœì í™”\")\n",
    "    \n",
    "    return {\n",
    "        'recommendation': recommendation,\n",
    "        'action': action,\n",
    "        'confidence': confidence,\n",
    "        'should_apply': avg_improvement >= 0.02,\n",
    "        'preprocessing_results': preprocessing_results,\n",
    "        'analysis_results': analysis_results\n",
    "    }\n",
    "\n",
    "# ì‹¤í–‰ ê°€ì´ë“œ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ ì „ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦ ì‹¤í–‰ ê°€ì´ë“œ\")  \n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“ ë‹¤ìŒ ì…€ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "print(\"   validation_results = test_preprocessing_effects(selected_images)\")\n",
    "print()\n",
    "print(\"ğŸ” ì´ë¯¸ selected_images_500.csv íŒŒì¼ì´ ìˆë‹¤ë©´:\")\n",
    "print(\"   selected_images = pd.read_csv('selected_images_500.csv').to_dict('records')\")\n",
    "print(\"   validation_results = test_preprocessing_effects(selected_images)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_images_500.csvê°€ ì´ë¯¸ ìˆë‹¤ë©´:\n",
    "selected_images = pd.read_csv('selected_images_500.csv').to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦ ì‹¤í–‰\n",
    "validation_results = test_preprocessing_effects(selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a124dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: ìˆ˜ë™ ë¼ë²¨ë§ ì¸í„°í˜ì´ìŠ¤\n",
    "# =============================================================================\n",
    "\n",
    "def display_images_grid(image_list, start_idx=0, grid_size=10):\n",
    "    \"\"\"ì´ë¯¸ì§€ë¥¼ 5x2 ê²©ìë¡œ ì¶œë ¥\"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 12))\n",
    "    fig.suptitle(f'Images {start_idx+1}-{start_idx+grid_size}', fontsize=16)\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        \n",
    "        if start_idx + i < len(image_list):\n",
    "            img_info = image_list[start_idx + i]\n",
    "            img_path = os.path.join(TEST_PATH, img_info['filename'])\n",
    "            \n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                axes[row, col].imshow(img)\n",
    "                axes[row, col].set_title(f\"{start_idx+i+1}: {img_info['filename']}\\n\"\n",
    "                                       f\"ì˜ˆì¸¡: {img_info['predicted_class']} \"\n",
    "                                       f\"(ì‹ ë¢°ë„: {img_info['confidence']:.2f})\\n\"\n",
    "                                       f\"í’ˆì§ˆ: {img_info['quality_score']:.3f}\",\n",
    "                                       fontsize=10)\n",
    "            except Exception as e:\n",
    "                axes[row, col].text(0.5, 0.5, f\"Error\\n{e}\", ha='center', va='center')\n",
    "        else:\n",
    "            axes[row, col].set_visible(False)\n",
    "        \n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26479a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_labeling_interface(selected_images, start_idx=0):\n",
    "    \"\"\"ìˆ˜ë™ ë¼ë²¨ë§ ì¸í„°í˜ì´ìŠ¤\"\"\"\n",
    "    print(\"\\nğŸ¯ ìˆ˜ë™ ë¼ë²¨ë§ ì‹œì‘!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ğŸ“‹ í´ë˜ìŠ¤ ì •ë³´ (ì°¸ê³ ìš©):\")\n",
    "    for idx, row in meta_df.iterrows():\n",
    "        print(f\"  {row['target']:2d}: {row['class_name']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "    manual_labels = []\n",
    "    \n",
    "    # ì´ì „ ì‘ì—… ë¡œë“œ (ìˆë‹¤ë©´)\n",
    "    progress_file = 'labeling_progress.csv'\n",
    "    if os.path.exists(progress_file):\n",
    "        print(f\"ğŸ“‚ ì´ì „ ì‘ì—… ì§„í–‰ìƒí™© ë¡œë“œ: {progress_file}\")\n",
    "        prev_df = pd.read_csv(progress_file)\n",
    "        manual_labels = prev_df.to_dict('records')\n",
    "        start_idx = len(manual_labels)\n",
    "        print(f\"   ì´ì „ ì‘ì—…: {len(manual_labels)}ì¥ ì™„ë£Œ\")\n",
    "    \n",
    "    current_idx = start_idx\n",
    "    \n",
    "    while current_idx < len(selected_images):\n",
    "        # 10ì¥ì”© ë³´ì—¬ì£¼ê¸°\n",
    "        display_images_grid(selected_images, current_idx, min(10, len(selected_images) - current_idx))\n",
    "        \n",
    "        # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ë¼ë²¨ ì…ë ¥ë°›ê¸°\n",
    "        batch_end = min(current_idx + 10, len(selected_images))\n",
    "        for i in range(current_idx, batch_end):\n",
    "            img_info = selected_images[i]\n",
    "            print(f\"\\n[{i+1}/{len(selected_images)}] {img_info['filename']}\")\n",
    "            print(f\"  ì˜ˆì¸¡í´ë˜ìŠ¤: {img_info['predicted_class']}, í’ˆì§ˆì ìˆ˜: {img_info['quality_score']:.3f}\")\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    user_input = input(f\"  ì •ë‹µ í´ë˜ìŠ¤ ì…ë ¥ (0-16, s=ê±´ë„ˆë›°ê¸°, q=ì¢…ë£Œ): \").strip()\n",
    "                    \n",
    "                    if user_input.lower() == 'q':\n",
    "                        print(\"ğŸ›‘ ë¼ë²¨ë§ ì¤‘ë‹¨\")\n",
    "                        return manual_labels\n",
    "                    elif user_input.lower() == 's':\n",
    "                        print(\"  â­ï¸  ê±´ë„ˆë›°ê¸°\")\n",
    "                        break\n",
    "                    else:\n",
    "                        label = int(user_input)\n",
    "                        if 0 <= label <= 16:\n",
    "                            manual_labels.append({\n",
    "                                'filename': img_info['filename'],\n",
    "                                'true_label': label,\n",
    "                                'predicted_class': img_info['predicted_class'],\n",
    "                                'quality_score': img_info['quality_score']\n",
    "                            })\n",
    "                            print(f\"  âœ… ë¼ë²¨ {label} ì €ì¥\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"  âŒ 0-16 ë²”ìœ„ì˜ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”\")\n",
    "                except ValueError:\n",
    "                    print(\"  âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”\")\n",
    "        \n",
    "        current_idx = batch_end\n",
    "        \n",
    "        # ì¤‘ê°„ ì €ì¥ (50ì¥ë§ˆë‹¤)\n",
    "        if len(manual_labels) % 50 == 0 and len(manual_labels) > 0:\n",
    "            temp_df = pd.DataFrame(manual_labels)\n",
    "            temp_df.to_csv(progress_file, index=False)\n",
    "            print(f\"ğŸ’¾ ì§„í–‰ìƒí™© ì €ì¥: {len(manual_labels)}ì¥ ì™„ë£Œ\")\n",
    "        \n",
    "        # ê³„ì†í• ì§€ í™•ì¸\n",
    "        if current_idx < len(selected_images):\n",
    "            continue_input = input(f\"\\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, í˜„ì¬ {len(manual_labels)}ì¥ ì™„ë£Œ): \")\n",
    "            if continue_input.lower() != 'y':\n",
    "                break\n",
    "    \n",
    "    return manual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da334651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 7: ìµœì¢… GT íŒŒì¼ ìƒì„±\n",
    "# =============================================================================\n",
    "\n",
    "def save_final_gt(manual_labels):\n",
    "    \"\"\"ìµœì¢… GT íŒŒì¼ ìƒì„± ë° ê²€ì¦\"\"\"\n",
    "    print(f\"\\nğŸ’¾ ìµœì¢… GT íŒŒì¼ ìƒì„± ì¤‘... (ì´ {len(manual_labels)}ì¥)\")\n",
    "    \n",
    "    # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    gt_df = pd.DataFrame(manual_labels)\n",
    "    \n",
    "    # íŒŒì¼ëª…ì—ì„œ ID ì¶”ì¶œ (í™•ì¥ì ì œê±°)\n",
    "    gt_df['ID'] = gt_df['filename'].str.replace('.jpg', '')\n",
    "    \n",
    "    # ìµœì¢… GT í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    final_gt = gt_df[['ID', 'true_label']].copy()\n",
    "    final_gt.columns = ['ID', 'target']\n",
    "    \n",
    "    # GT íŒŒì¼ ì €ì¥\n",
    "    final_gt.to_csv('mini_gt_500.csv', index=False)\n",
    "    \n",
    "    # ë°±ì—… ì €ì¥\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_filename = f'mini_gt_500_backup_{timestamp}.csv'\n",
    "    final_gt.to_csv(backup_filename, index=False)\n",
    "    \n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    print(\"ğŸ“Š ìµœì¢… GT í†µê³„:\")\n",
    "    print(f\"   ì´ ë¼ë²¨ë§ëœ ì´ë¯¸ì§€: {len(final_gt)}ì¥\")\n",
    "    print(\"\\n   í´ë˜ìŠ¤ë³„ ë¶„í¬:\")\n",
    "    class_counts = final_gt['target'].value_counts().sort_index()\n",
    "    for class_id, count in class_counts.items():\n",
    "        class_name = meta_df[meta_df['target'] == class_id]['class_name'].iloc[0]\n",
    "        print(f\"     {class_id:2d} ({class_name}): {count:3d}ì¥\")\n",
    "    \n",
    "    # í’ˆì§ˆì ìˆ˜ ë¶„í¬\n",
    "    if 'quality_score' in gt_df.columns:\n",
    "        print(f\"\\n   í’ˆì§ˆì ìˆ˜ ë¶„í¬:\")\n",
    "        print(f\"     í‰ê· : {gt_df['quality_score'].mean():.3f}\")\n",
    "        print(f\"     ë²”ìœ„: {gt_df['quality_score'].min():.3f} ~ {gt_df['quality_score'].max():.3f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… GT íŒŒì¼ ì €ì¥ ì™„ë£Œ:\")\n",
    "    print(f\"   ğŸ“ main: mini_gt_500.csv\")\n",
    "    print(f\"   ğŸ“ backup: {backup_filename}\")\n",
    "    \n",
    "    return final_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ì‹¤í–‰ ê°€ì´ë“œ\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ ìˆ˜ë™ ë¼ë²¨ë§ ì‹¤í–‰ ê°€ì´ë“œ\")\n",
    "print(\"=\"*60)\n",
    "print(\"1ï¸âƒ£  manual_labels = manual_labeling_interface(selected_images)\")\n",
    "print(\"2ï¸âƒ£  final_gt = save_final_gt(manual_labels)\")\n",
    "print(\"3ï¸âƒ£  íŒ€ì›ê³¼ ì‘ì—… ë¶„ë‹´ ì‹œ: ë¦¬ìŠ¤íŠ¸ë¥¼ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì§„í–‰\")\n",
    "print(\"4ï¸âƒ£  ì™„ë£Œ í›„: mini_gt_500.csv íŒŒì¼ì„ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60323f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT ì‘ì—… ì‹œì‘\n",
    "manual_labels = manual_labeling_interface(selected_images, start_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7745d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT ì‘ì—… ì„¸ì´ë¸Œ\n",
    "final_gt = save_final_gt(manual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843c77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8a634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
