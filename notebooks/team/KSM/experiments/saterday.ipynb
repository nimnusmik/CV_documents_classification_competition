{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bbe02e",
   "metadata": {},
   "source": [
    "# ğŸ“„ Document type classification baseline code with WandB Integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92dc69ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations==1.3.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: ipykernel==6.27.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (6.27.1)\n",
      "Requirement already satisfied: ipython==8.15.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (8.15.0)\n",
      "Requirement already satisfied: ipywidgets==8.1.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (8.1.1)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (0.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (1.26.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 8)) (2.1.4)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 9)) (9.4.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: timm==0.9.12 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 11)) (0.9.12)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 12)) (6.3.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 13)) (3.10.6)\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 14)) (4.5.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.11.4)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.22.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (4.8.1.78)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.5.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (1.5.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.7.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1->-r ../requirements.txt (line 4)) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1->-r ../requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.0.6)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.12.0)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.16.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.19.4)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.10/site-packages (from plotly->-r ../requirements.txt (line 12)) (2.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (3.2.3)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (1.16.5)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (2.0.43)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (4.65.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (4.15.0)\n",
      "Requirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (2.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.27.1->-r ../requirements.txt (line 2)) (4.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 8)) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (2023.12.9)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.3)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna->-r ../requirements.txt (line 14)) (3.2.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (3.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (2023.9.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (5.9.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.12.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.25.2)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.0.9)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.2.3)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.1.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.5.0)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.20.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (21.2.0)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.1)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.8.19.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 0. Prepare Environments & Install Libraries\n",
    "# =============================================================================\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "773408ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Import Libraries & Define Functions\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import optuna, math\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed Precisionìš©\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# WandB ê´€ë ¨ import ì¶”ê°€\n",
    "import wandb\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e142354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB ë¡œê·¸ì¸ ìƒíƒœ: kimsunmin0227\n",
      "í”„ë¡œì íŠ¸: document-classification-team-CV\n",
      "ì‹¤í—˜ëª…: efficientnet-b3-baseline\n",
      "íŒ€ì›ë“¤ì€ EXPERIMENT_NAMEì„ ê°ì ë‹¤ë¥´ê²Œ ë³€ê²½í•´ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1-1. WandB Login and Configuration\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "ğŸš€ íŒ€ì› ì‚¬ìš© ê°€ì´ë“œ:\n",
    "\n",
    "1. WandB ê³„ì • ìƒì„±: https://wandb.ai/signup\n",
    "2. ì´ ì…€ ì‹¤í–‰ ì‹œ ë¡œê·¸ì¸ í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚˜ë©´ ê°œì¸ API í‚¤ ì…ë ¥\n",
    "3. EXPERIMENT_NAMEì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½:\n",
    "   - \"member1-baseline\"\n",
    "   - \"member2-augmentation-test\"  \n",
    "   - \"member3-hyperparameter-tuning\"\n",
    "   ë“±ë“± ê°ì ë‹¤ë¥¸ ì´ë¦„ ì‚¬ìš©\n",
    "\n",
    "4. íŒ€ ëŒ€ì‹œë³´ë“œ URL: [ì—¬ê¸°ì— ë‹¹ì‹ ì˜ í”„ë¡œì íŠ¸ URL ì¶”ê°€]\n",
    "\n",
    "âš ï¸ ì£¼ì˜ì‚¬í•­:\n",
    "- ì ˆëŒ€ API í‚¤ë¥¼ ì½”ë“œì— í•˜ë“œì½”ë”©í•˜ì§€ ë§ˆì„¸ìš”\n",
    "- EXPERIMENT_NAMEë§Œ ë³€ê²½í•˜ê³  PROJECT_NAMEì€ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”\n",
    "- ê°ì ê°œì¸ ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸í•´ì„œ ì‹¤í—˜ì„ ì¶”ê°€í•˜ì„¸ìš”\n",
    "\"\"\"\n",
    "\n",
    "# WandB ë¡œê·¸ì¸ (ê°ì ì‹¤í–‰)\n",
    "try:\n",
    "    if wandb.api.api_key is None:\n",
    "        print(\"WandBì— ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        wandb.login()\n",
    "    else:\n",
    "        print(f\"WandB ë¡œê·¸ì¸ ìƒíƒœ: {wandb.api.viewer()['username']}\")\n",
    "except:\n",
    "    print(\"WandB ë¡œê·¸ì¸ì„ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
    "    wandb.login()\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì„¤ì • (ê°ì ìˆ˜ì •í•  ë¶€ë¶„)\n",
    "PROJECT_NAME = \"document-classification-team-CV\"  # ëª¨ë“  íŒ€ì› ë™ì¼\n",
    "ENTITY = None  # ê°ì ê°œì¸ ê³„ì • ì‚¬ìš©\n",
    "EXPERIMENT_NAME = \"efficientnet-b3-baseline\"  # íŒ€ì›ë³„ë¡œ ë³€ê²½ (ì˜ˆ: \"member1-hyperopt\", \"member2-augmentation\")\n",
    "\n",
    "print(f\"í”„ë¡œì íŠ¸: {PROJECT_NAME}\")\n",
    "print(f\"ì‹¤í—˜ëª…: {EXPERIMENT_NAME}\")\n",
    "print(\"íŒ€ì›ë“¤ì€ EXPERIMENT_NAMEì„ ê°ì ë‹¤ë¥´ê²Œ ë³€ê²½í•´ì£¼ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "448a2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. Seed & basic augmentations (Mixup)\n",
    "# =============================================================================\n",
    "\n",
    "# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f9d1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. Dataset Class\n",
    "# =============================================================================\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transform=None):\n",
    "        # CSV íŒŒì¼ì´ë©´ ì½ê³ , DataFrameì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values  # DataFrameì„ numpy arrayë¡œ ë³€í™˜\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0cacecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss êµ¬í˜„\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for addressing class imbalance\n",
    "    \n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    \n",
    "    Args:\n",
    "        alpha: Weighting factor for rare class (default: 1.0)\n",
    "        gamma: Focusing parameter (default: 2.0)\n",
    "        reduction: Specifies the reduction to apply to the output\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Cross entropy loss\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Get probabilities\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Cutout (Random Erasing) í•¨ìˆ˜ ì •ì˜\n",
    "def random_erasing(image, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)):\n",
    "    if random.random() > p:\n",
    "        return image\n",
    "    img_c, img_h, img_w = image.shape[1], image.shape[2], image.shape[3]\n",
    "    area = img_h * img_w\n",
    "    \n",
    "    target_area = random.uniform(scale[0], scale[1]) * area\n",
    "    aspect_ratio = random.uniform(ratio[0], ratio[1])\n",
    "    h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "    w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "    \n",
    "    if h < img_h and w < img_w:\n",
    "        x = random.randint(0, img_w - w)\n",
    "        y = random.randint(0, img_h - h)\n",
    "        image[:, :, y:y+h, x:x+w] = 0.0  # ì œê±°ëœ ì˜ì—­ì„ 0ìœ¼ë¡œ ì„¤ì •\n",
    "    return image\n",
    "\n",
    "# RandomCrop í•¨ìˆ˜ ì •ì˜\n",
    "def random_crop(image, crop_size=0.8):\n",
    "    img_c, img_h, img_w = image.shape[1], image.shape[2], image.shape[3]\n",
    "    crop_h = int(img_h * crop_size)\n",
    "    crop_w = int(img_w * crop_size)\n",
    "    \n",
    "    if crop_h >= img_h or crop_w >= img_w:\n",
    "        return image\n",
    "    \n",
    "    x = random.randint(0, img_w - crop_w)\n",
    "    y = random.randint(0, img_h - crop_h)\n",
    "    cropped_image = image[:, :, y:y+crop_h, x:x+crop_w]\n",
    "    \n",
    "    # ì›ë˜ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³µì› (íŒ¨ë”© ë˜ëŠ” ë¦¬ì‚¬ì´ì¦ˆ)\n",
    "    cropped_image = torch.nn.functional.interpolate(cropped_image, size=(img_h, img_w), mode='bilinear', align_corners=False)\n",
    "    return cropped_image\n",
    "\n",
    "# Mixup í•¨ìˆ˜ ì •ì˜\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, epoch=None, fold=None):\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Training Epoch {epoch+1 if epoch else '?'}\")\n",
    "    batch_count = 0\n",
    "    \n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # ì¦ê°• ê¸°ë²• ì„ íƒ (Mixup 25%, Cutout 25%, RandomCrop 25%, None 25%) -> (Mixup 25%, Cutout 25%, RandomCrop 50%)\n",
    "        aug_type = random.choices(['mixup', 'cutout', 'random_crop'], weights=[0.25, 0.25, 0.5])[0]\n",
    "        mixup_applied = False\n",
    "        cutout_applied = False\n",
    "        random_crop_applied = False\n",
    "        \n",
    "        if aug_type == 'mixup':\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(image, targets, alpha=1.0)\n",
    "            with autocast(): \n",
    "                preds = model(mixed_x)\n",
    "            loss = lam * loss_fn(preds, y_a) + (1 - lam) * loss_fn(preds, y_b)\n",
    "            mixup_applied = True\n",
    "        elif aug_type == 'cutout':\n",
    "            image = random_erasing(image, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            cutout_applied = True\n",
    "        elif aug_type == 'random_crop':\n",
    "            image = random_crop(image, crop_size=0.8)\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            random_crop_applied = True\n",
    "        else:\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        # ë°°ì¹˜ë³„ ìƒì„¸ ë¡œê¹… (100 ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if batch_count % 100 == 0 and wandb.run is not None:\n",
    "            step = epoch * len(loader) + batch_count if epoch is not None else batch_count\n",
    "            wandb.log({\n",
    "                f\"fold_{fold}/train_batch_loss\": loss.item(),\n",
    "                f\"fold_{fold}/mixup_applied\": int(mixup_applied),\n",
    "                f\"fold_{fold}/cutout_applied\": int(cutout_applied),\n",
    "                f\"fold_{fold}/random_crop_applied\": int(random_crop_applied),\n",
    "                f\"fold_{fold}/batch_step\": step\n",
    "            })\n",
    "        \n",
    "        batch_count += 1\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}, Mixup: {mixup_applied}, Cutout: {cutout_applied}, RandomCrop: {random_crop_applied}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def validate_one_epoch(loader, model, loss_fn, device, epoch=None, fold=None, log_confusion=False):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=f\"Validating Epoch {epoch+1 if epoch else '?'}\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "            \n",
    "            pbar.set_description(f\"Val Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "    \n",
    "    # Confusion Matrix ë¡œê¹… (ë§ˆì§€ë§‰ epochì—ë§Œ)\n",
    "    if log_confusion and wandb.run is not None:\n",
    "        try:\n",
    "            wandb.log({\n",
    "                f\"fold_{fold}/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=targets_list,\n",
    "                    preds=preds_list,\n",
    "                    class_names=[f\"Class_{i}\" for i in range(17)]\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            # í´ë˜ìŠ¤ë³„ F1 ìŠ¤ì½”ì–´\n",
    "            class_f1_scores = f1_score(targets_list, preds_list, average=None)\n",
    "            for i, class_f1 in enumerate(class_f1_scores):\n",
    "                wandb.log({f\"fold_{fold}/class_{i}_f1\": class_f1})\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Confusion matrix ë¡œê¹… ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,  \n",
    "        \"val_f1\": val_f1,\n",
    "    }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "193dae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using device: cuda\n",
      " í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ!\n",
      " ëª¨ë¸: efficientnet_b3\n",
      " ì´ë¯¸ì§€ í¬ê¸°: 384x384\n",
      " ë°°ì¹˜ í¬ê¸°: 64\n",
      " í•™ìŠµë¥ : 0.0005\n",
      " ì—í­: 50\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. Hyper-parameters with WandB Config\n",
    "# =============================================================================\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "\n",
    "# model config\n",
    "model_name = 'efficientnet_b3' # 'resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "# training config\n",
    "img_size = 384\n",
    "LR = 5e-4\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "num_workers = 30\n",
    "\n",
    "# K-Fold config\n",
    "N_FOLDS = 5  # 5-foldë¡œ ì„¤ì •\n",
    "\n",
    "# WandB Config ì„¤ì •\n",
    "config = {\n",
    "    # Model config\n",
    "    \"model_name\": model_name,\n",
    "    \"img_size\": img_size,\n",
    "    \"num_classes\": 17,\n",
    "    \"architecture\": \"EfficientNet-B3\",\n",
    "    \n",
    "    # Training config  \n",
    "    \"lr\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"device\": str(device),\n",
    "    \n",
    "    # K-Fold config\n",
    "    \"n_folds\": N_FOLDS,\n",
    "    \"seed\": SEED,\n",
    "    \"cv_strategy\": \"StratifiedKFold\",\n",
    "    \n",
    "    # Augmentation & Training techniques\n",
    "    \"mixup_alpha\": 1.0,\n",
    "    \"mixup_prob\": 0.3,\n",
    "    \"loss_function\": \"FocalLoss\",\n",
    "    \"focal_alpha\": 1.0,\n",
    "    \"focal_gamma\": 2.0,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"mixed_precision\": True,\n",
    "    \n",
    "    # Optimizer & Scheduler\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \n",
    "    # Data\n",
    "    \"data_path\": data_path,\n",
    "    \"train_transforms\": \"Advanced\",\n",
    "    \"test_transforms\": \"Basic\",\n",
    "}\n",
    "\n",
    "print(\" í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\" ëª¨ë¸: {model_name}\")\n",
    "print(f\" ì´ë¯¸ì§€ í¬ê¸°: {img_size}x{img_size}\")\n",
    "print(f\" ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(f\" í•™ìŠµë¥ : {LR}\")\n",
    "print(f\" ì—í­: {EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e28f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2370869770.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    USE_OPTUNA = \bã„¹  # Trueë¡œ ë°”ê¾¸ë©´ íŠœë‹ ì‹¤í–‰\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 7. Optuna Hyperparameter Tuning (ì„ íƒì )\n",
    "# =============================================================================\n",
    "\n",
    "# Optuna ì„¤ì •\n",
    "USE_OPTUNA = False  # Trueë¡œ ë°”ê¾¸ë©´ íŠœë‹ ì‹¤í–‰\n",
    "OPTUNA_TRIALS = 20  # Optuna ì‹œí–‰ íšŸìˆ˜\n",
    "OPTUNA_TIMEOUT = 3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ (ì´ˆ)\n",
    "\n",
    "# Optunaê°€ í™œì„±í™”ëœ ê²½ìš° ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ Noneìœ¼ë¡œ ì„¤ì •\n",
    "if USE_OPTUNA:\n",
    "    print(\"ğŸ” Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í™œì„±í™”!\")\n",
    "    print(f\"   ì‹œí–‰ íšŸìˆ˜: {OPTUNA_TRIALS}\")\n",
    "    print(f\"   íƒ€ì„ì•„ì›ƒ: {OPTUNA_TIMEOUT}ì´ˆ\")\n",
    "    \n",
    "    # ê¸°ë³¸ê°’ë“¤ì„ Noneìœ¼ë¡œ ì„¤ì • (Optunaì—ì„œ ìµœì í™”)\n",
    "    LR = None\n",
    "    BATCH_SIZE = None\n",
    "    EPOCHS = None\n",
    "    img_size = None\n",
    "    focal_alpha = None\n",
    "    focal_gamma = None\n",
    "    mixup_alpha = None\n",
    "    mixup_prob = None\n",
    "    gradient_clipping = None\n",
    "else:\n",
    "    print(\"â­ï¸ Optuna íŠœë‹ ê±´ë„ˆë›°ê¸° (USE_OPTUNA=False)\")\n",
    "    print(\"   ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36e4bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Optuna objective function ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7-1. Optuna Objective Function\n",
    "# =============================================================================\n",
    "\n",
    "def optuna_objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization\n",
    "    \"\"\"\n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ\n",
    "    params = {\n",
    "        # í•™ìŠµë¥  (ë¡œê·¸ ìŠ¤ì¼€ì¼)\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-2, log=True),\n",
    "        \n",
    "        # ë°°ì¹˜ í¬ê¸° (2ì˜ ê±°ë“­ì œê³±)\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "        \n",
    "        # ì—í­ ìˆ˜ (ì ë‹¹í•œ ë²”ìœ„)\n",
    "        'epochs': trial.suggest_int('epochs', 20, 80),\n",
    "        \n",
    "        # ì´ë¯¸ì§€ í¬ê¸°\n",
    "        'img_size': trial.suggest_categorical('img_size', [224, 256, 288, 320, 384, 448]),\n",
    "        \n",
    "        # Focal Loss íŒŒë¼ë¯¸í„°\n",
    "        'focal_alpha': trial.suggest_float('focal_alpha', 0.5, 2.0),\n",
    "        'focal_gamma': trial.suggest_float('focal_gamma', 1.0, 3.0),\n",
    "        \n",
    "        # Mixup íŒŒë¼ë¯¸í„°\n",
    "        'mixup_alpha': trial.suggest_float('mixup_alpha', 0.2, 2.0),\n",
    "        'mixup_prob': trial.suggest_float('mixup_prob', 0.1, 0.5),\n",
    "        \n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "        'gradient_clipping': trial.suggest_float('gradient_clipping', 0.5, 2.0),\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € íŒŒë¼ë¯¸í„°\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n",
    "        \n",
    "        # ìŠ¤ì¼€ì¤„ëŸ¬ íŒŒë¼ë¯¸í„°\n",
    "        'min_lr_ratio': trial.suggest_float('min_lr_ratio', 0.01, 0.1),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nğŸ” Trial {trial.number}: Testing hyperparameters...\")\n",
    "    print(f\"   LR: {params['lr']:.6f}\")\n",
    "    print(f\"   Batch Size: {params['batch_size']}\")\n",
    "    print(f\"   Epochs: {params['epochs']}\")\n",
    "    print(f\"   Image Size: {params['img_size']}\")\n",
    "    print(f\"   Focal Alpha: {params['focal_alpha']:.3f}\")\n",
    "    print(f\"   Focal Gamma: {params['focal_gamma']:.3f}\")\n",
    "    \n",
    "    try:\n",
    "        # ë°ì´í„° ë¡œë“œ\n",
    "        train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "        \n",
    "        # Stratified K-Fold\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        # ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¥¸ transform ì„¤ì •\n",
    "        trn_transform = A.Compose([\n",
    "            A.LongestMaxSize(max_size=params['img_size']),\n",
    "            A.PadIfNeeded(min_height=params['img_size'], min_width=params['img_size'], \n",
    "                          border_mode=0, value=0),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=[90,90], p=1.0),\n",
    "                A.Rotate(limit=[180,180], p=1.0),\n",
    "                A.Rotate(limit=[270,270], p=1.0),\n",
    "                A.Rotate(limit=(-15, 15), p=1.0),\n",
    "            ], p=0.7),\n",
    "            A.OneOf([\n",
    "                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=5, p=1.0),\n",
    "                A.ElasticTransform(alpha=50, sigma=5, p=1.0),\n",
    "                A.GridDistortion(num_steps=5, distort_limit=0.2, p=1.0),\n",
    "                A.OpticalDistortion(distort_limit=0.2, shift_limit=0.1, p=1.0),\n",
    "            ], p=0.6),\n",
    "            A.OneOf([\n",
    "                A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1, p=1.0),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
    "                A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=1.0),\n",
    "                A.RandomGamma(gamma_limit=(70, 130), p=1.0),\n",
    "            ], p=0.9),\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(blur_limit=(5, 15), p=1.0),\n",
    "                A.GaussianBlur(blur_limit=(3, 15), p=1.0),\n",
    "                A.MedianBlur(blur_limit=7, p=1.0),\n",
    "                A.Blur(blur_limit=7, p=1.0),\n",
    "            ], p=0.8),\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(10.0, 150.0), p=1.0),\n",
    "                A.ISONoise(color_shift=(0.01, 0.08), intensity=(0.1, 0.8), p=1.0),\n",
    "                A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n",
    "            ], p=0.8),\n",
    "            A.OneOf([\n",
    "                A.Downscale(scale_min=0.7, scale_max=0.9, p=1.0),\n",
    "                A.ImageCompression(quality_lower=60, quality_upper=95, p=1.0),\n",
    "                A.Posterize(num_bits=6, p=1.0),\n",
    "            ], p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        val_transform = A.Compose([\n",
    "            A.LongestMaxSize(max_size=params['img_size']),\n",
    "            A.PadIfNeeded(min_height=params['img_size'], min_width=params['img_size'], \n",
    "                          border_mode=0, value=0),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        # K-Fold êµì°¨ ê²€ì¦\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "            print(f\"   ğŸ“Š Fold {fold+1}/{N_FOLDS} ì‹œì‘...\")\n",
    "            \n",
    "            # ë°ì´í„° ë¶„í• \n",
    "            train_fold = train_df.iloc[train_idx]\n",
    "            val_fold = train_df.iloc[val_idx]\n",
    "            \n",
    "            # ë°ì´í„°ì…‹ ìƒì„±\n",
    "            train_dataset = ImageDataset(train_fold, os.path.join(data_path, 'train'), trn_transform)\n",
    "            val_dataset = ImageDataset(val_fold, os.path.join(data_path, 'train'), val_transform)\n",
    "            \n",
    "            # ë°ì´í„°ë¡œë” ìƒì„±\n",
    "            train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], \n",
    "                                    shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], \n",
    "                                  shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "            \n",
    "            # ëª¨ë¸ ìƒì„±\n",
    "            model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "            \n",
    "            # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €\n",
    "            loss_fn = FocalLoss(alpha=params['focal_alpha'], gamma=params['focal_gamma'])\n",
    "            optimizer = Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'], \n",
    "                                       eta_min=params['lr'] * params['min_lr_ratio'])\n",
    "            \n",
    "            # í›ˆë ¨\n",
    "            best_val_f1 = 0\n",
    "            for epoch in range(params['epochs']):\n",
    "                # í›ˆë ¨\n",
    "                train_metrics = train_one_epoch(train_loader, model, optimizer, loss_fn, device, \n",
    "                                              epoch=epoch, fold=fold)\n",
    "                \n",
    "                # ê²€ì¦\n",
    "                val_metrics = validate_one_epoch(val_loader, model, loss_fn, device, \n",
    "                                               epoch=epoch, fold=fold)\n",
    "                \n",
    "                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "                scheduler.step()\n",
    "                \n",
    "                # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸\n",
    "                if val_metrics['val_f1'] > best_val_f1:\n",
    "                    best_val_f1 = val_metrics['val_f1']\n",
    "                \n",
    "                # ì¡°ê¸° ì¢…ë£Œ (ì„±ëŠ¥ì´ ë„ˆë¬´ ë‚˜ì˜ë©´)\n",
    "                if epoch > 10 and val_metrics['val_f1'] < 0.1:\n",
    "                    print(f\"      âš ï¸ ì¡°ê¸° ì¢…ë£Œ (epoch {epoch+1})\")\n",
    "                    break\n",
    "            \n",
    "            fold_scores.append(best_val_f1)\n",
    "            print(f\"      âœ… Fold {fold+1} ì™„ë£Œ - Best F1: {best_val_f1:.4f}\")\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del model, train_dataset, val_dataset, train_loader, val_loader\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # í‰ê·  F1 ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "        mean_f1 = np.mean(fold_scores)\n",
    "        std_f1 = np.std(fold_scores)\n",
    "        \n",
    "        print(f\"   ğŸ¯ Trial {trial.number} ì™„ë£Œ - Mean F1: {mean_f1:.4f} Â± {std_f1:.4f}\")\n",
    "        \n",
    "        # WandB ë¡œê¹… (ì„ íƒì )\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                \"trial_number\": trial.number,\n",
    "                \"mean_f1\": mean_f1,\n",
    "                \"std_f1\": std_f1,\n",
    "                **params\n",
    "            })\n",
    "        \n",
    "        return mean_f1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Trial {trial.number} ì‹¤íŒ¨: {str(e)}\")\n",
    "        return 0.0  # ì‹¤íŒ¨í•œ ê²½ìš° 0 ë°˜í™˜\n",
    "\n",
    "print(\"âœ… Optuna objective function ì •ì˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f420558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-06 14:59:38,760] A new study created in memory with name: no-name-1e736f5e-ec0e-4b52-b144-6521f8ae3086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘!\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66d30b5d38b460fbac5b9284ca2a324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Trial 0: Testing hyperparameters...\n",
      "   LR: 0.000133\n",
      "   Batch Size: 16\n",
      "   Epochs: 29\n",
      "   Image Size: 448\n",
      "   Focal Alpha: 1.749\n",
      "   Focal Gamma: 1.425\n",
      "   ğŸ“Š Fold 1/5 ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.5391, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:16<00:00,  4.65it/s]\n",
      "Val Loss: 5.5611: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  7.11it/s]\n",
      "Loss: 4.6797, Mixup: False, Cutout: True, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.69it/s]\n",
      "Val Loss: 5.2895: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.61it/s]\n",
      "Loss: 3.4219, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.76it/s]\n",
      "Val Loss: 5.2376: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.14it/s]\n",
      "Loss: 3.4141, Mixup: False, Cutout: True, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.69it/s]\n",
      "Val Loss: 4.1323: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.47it/s]\n",
      "Loss: 4.2188, Mixup: False, Cutout: False, RandomCrop: True: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.78it/s]\n",
      "Val Loss: 3.8510: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.47it/s]\n",
      "Loss: 3.5293, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.73it/s]\n",
      "Val Loss: 3.5639: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.58it/s]\n",
      "Loss: 2.9648, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.72it/s]\n",
      "Val Loss: 3.1203: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.60it/s]\n",
      "Loss: 2.9355, Mixup: False, Cutout: False, RandomCrop: True: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.78it/s]\n",
      "Val Loss: 3.3092: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.46it/s]\n",
      "Loss: 4.0820, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.77it/s]\n",
      "Val Loss: 3.2539: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.58it/s]\n",
      "Loss: 2.6426, Mixup: False, Cutout: False, RandomCrop: True: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.73it/s]\n",
      "Val Loss: 2.7821: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.47it/s]\n",
      "Loss: 2.5547, Mixup: False, Cutout: False, RandomCrop: True: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.77it/s]\n",
      "Val Loss: 2.1593: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.10it/s]\n",
      "Loss: 3.7070, Mixup: False, Cutout: True, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.70it/s]\n",
      "Val Loss: 2.6669: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.46it/s]\n",
      "Loss: 3.0332, Mixup: False, Cutout: False, RandomCrop: True: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.65it/s]\n",
      "Val Loss: 2.3922: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.07it/s]\n",
      "Loss: 2.7363, Mixup: False, Cutout: True, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:14<00:00,  5.60it/s]\n",
      "Val Loss: 2.2877: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.35it/s]\n",
      "Loss: 2.8594, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.66it/s]\n",
      "Val Loss: 1.9191: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.49it/s]\n",
      "Loss: 2.5215, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.73it/s]\n",
      "Val Loss: 2.5915: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.53it/s]\n",
      "Loss: 2.5762, Mixup: False, Cutout: False, RandomCrop: True: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.81it/s]\n",
      "Val Loss: 1.6418: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.56it/s]\n",
      "Loss: 4.3594, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.68it/s]\n",
      "Val Loss: 1.4647: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.16it/s]\n",
      "Loss: 3.2070, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.77it/s]\n",
      "Val Loss: 1.4792: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7-2. Optuna Study ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬ (WandB ì œê±° ë²„ì „)\n",
    "# =============================================================================\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    print(\"ğŸš€ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Optuna study ìƒì„±\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',  # F1 ìŠ¤ì½”ì–´ ìµœëŒ€í™”\n",
    "        sampler=optuna.samplers.TPESampler(seed=SEED),  # TPE ìƒ˜í”ŒëŸ¬ ì‚¬ìš©\n",
    "        pruner=optuna.pruners.MedianPruner(  # ì¤‘ê°„ê°’ ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ\n",
    "            n_startup_trials=5,  # ì²˜ìŒ 5ê°œ trialì€ ì¡°ê¸° ì¢…ë£Œí•˜ì§€ ì•ŠìŒ\n",
    "            n_warmup_steps=10,   # 10 ìŠ¤í… í›„ ì¡°ê¸° ì¢…ë£Œ ê³ ë ¤\n",
    "            interval_steps=1     # ë§¤ ìŠ¤í…ë§ˆë‹¤ ì¡°ê¸° ì¢…ë£Œ í™•ì¸\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Optuna ìµœì í™” ì‹¤í–‰\n",
    "        study.optimize(\n",
    "            optuna_objective,\n",
    "            n_trials=OPTUNA_TRIALS,\n",
    "            timeout=OPTUNA_TIMEOUT,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ‰ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥\n",
    "        best_params = study.best_params\n",
    "        best_value = study.best_value\n",
    "        \n",
    "        print(f\"ğŸ† ìµœê³  ì„±ëŠ¥: {best_value:.4f}\")\n",
    "        print(f\"ğŸ“Š ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "        for key, value in best_params.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "        LR = best_params['lr']\n",
    "        BATCH_SIZE = best_params['batch_size']\n",
    "        EPOCHS = best_params['epochs']\n",
    "        img_size = best_params['img_size']\n",
    "        focal_alpha = best_params['focal_alpha']\n",
    "        focal_gamma = best_params['focal_gamma']\n",
    "        mixup_alpha = best_params['mixup_alpha']\n",
    "        mixup_prob = best_params['mixup_prob']\n",
    "        gradient_clipping = best_params['gradient_clipping']\n",
    "        \n",
    "        # ìµœì í™” íˆìŠ¤í† ë¦¬ ì‹œê°í™”\n",
    "        print(f\"\\nğŸ“ˆ ìµœì í™” íˆìŠ¤í† ë¦¬:\")\n",
    "        print(f\"   ì´ ì‹œí–‰ íšŸìˆ˜: {len(study.trials)}\")\n",
    "        print(f\"   ì™„ë£Œëœ ì‹œí–‰: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
    "        print(f\"   ì¡°ê¸° ì¢…ë£Œëœ ì‹œí–‰: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "        print(f\"   ì‹¤íŒ¨í•œ ì‹œí–‰: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}\")\n",
    "        \n",
    "        # ìƒìœ„ 5ê°œ ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"\\nğŸ… ìƒìœ„ 5ê°œ ê²°ê³¼:\")\n",
    "        sorted_trials = sorted([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE], \n",
    "                             key=lambda x: x.value, reverse=True)[:5]\n",
    "        for i, trial in enumerate(sorted_trials, 1):\n",
    "            print(f\"   {i}. F1: {trial.value:.4f} - Trial {trial.number}\")\n",
    "        \n",
    "        print(f\"\\nâœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        print(f\"   ì´ì œ ì´ íŒŒë¼ë¯¸í„°ë“¤ë¡œ ìµœì¢… ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        if len(study.trials) > 0:\n",
    "            print(f\"   í˜„ì¬ê¹Œì§€ì˜ ìµœê³  ì„±ëŠ¥: {study.best_value:.4f}\")\n",
    "            print(f\"   ìµœì  íŒŒë¼ë¯¸í„°: {study.best_params}\")\n",
    "        else:\n",
    "            print(\"   ì•„ì§ ì™„ë£Œëœ trialì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Optuna ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        print(f\"   ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µì›\n",
    "        LR = 5e-4\n",
    "        BATCH_SIZE = 32\n",
    "        EPOCHS = 50\n",
    "        img_size = 384\n",
    "        focal_alpha = 1.0\n",
    "        focal_gamma = 2.0\n",
    "        mixup_alpha = 1.0\n",
    "        mixup_prob = 0.3\n",
    "        gradient_clipping = 1.0\n",
    "        \n",
    "else:\n",
    "    print(\"â­ï¸ Optuna ë¹„í™œì„±í™” - ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©\")\n",
    "    # ê¸°ë³¸ê°’ ì„¤ì •\n",
    "    LR = 5e-4\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    img_size = 384\n",
    "    focal_alpha = 1.0\n",
    "    focal_gamma = 2.0\n",
    "    mixup_alpha = 1.0\n",
    "    mixup_prob = 0.3\n",
    "    gradient_clipping = 1.0\n",
    "\n",
    "print(f\"\\nğŸ“‹ ìµœì¢… í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "print(f\"   í•™ìŠµë¥ : {LR}\")\n",
    "print(f\"   ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(f\"   ì—í­: {EPOCHS}\")\n",
    "print(f\"   ì´ë¯¸ì§€ í¬ê¸°: {img_size}\")\n",
    "print(f\"   Focal Alpha: {focal_alpha}\")\n",
    "print(f\"   Focal Gamma: {focal_gamma}\")\n",
    "print(f\"   Mixup Alpha: {mixup_alpha}\")\n",
    "print(f\"   Mixup Prob: {mixup_prob}\")\n",
    "print(f\"   Gradient Clipping: {gradient_clipping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. Data Transforms\n",
    "# =============================================================================\n",
    "\n",
    "# augmentationì„ ìœ„í•œ transform ì½”ë“œ\n",
    "trn_transform = A.Compose([\n",
    "    # ë¹„ìœ¨ ë³´ì¡´ ë¦¬ì‚¬ì´ì§• (í•µì‹¬ ê°œì„ )\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "    \n",
    "    # ë¬¸ì„œ íŠ¹í™” íšŒì „ + ë¯¸ì„¸ íšŒì „ ì¶”ê°€\n",
    "    A.OneOf([\n",
    "        A.Rotate(limit=[90,90], p=1.0),\n",
    "        A.Rotate(limit=[180,180], p=1.0),\n",
    "        A.Rotate(limit=[270,270], p=1.0),\n",
    "        A.Rotate(limit=(-15, 15), p=1.0),  # ë¯¸ì„¸ íšŒì „ ì¶”ê°€\n",
    "    ], p=0.7),\n",
    "    \n",
    "    # ê¸°í•˜í•™ì  ë³€í™˜ ê°•í™”\n",
    "    A.OneOf([\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=5, p=1.0),\n",
    "        A.ElasticTransform(alpha=50, sigma=5, p=1.0),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.2, p=1.0),\n",
    "        A.OpticalDistortion(distort_limit=0.2, shift_limit=0.1, p=1.0),\n",
    "    ], p=0.6),\n",
    "    \n",
    "    # ìƒ‰ìƒ ë° ì¡°ëª… ë³€í™˜ ê°•í™”\n",
    "    A.OneOf([\n",
    "        A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1, p=1.0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=1.0),\n",
    "        A.RandomGamma(gamma_limit=(70, 130), p=1.0),\n",
    "    ], p=0.9),\n",
    "    \n",
    "    # ë¸”ëŸ¬ ë° ë…¸ì´ì¦ˆ ê°•í™”\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=(5, 15), p=1.0),\n",
    "        A.GaussianBlur(blur_limit=(3, 15), p=1.0),\n",
    "        A.MedianBlur(blur_limit=7, p=1.0),\n",
    "        A.Blur(blur_limit=7, p=1.0),\n",
    "    ], p=0.8),\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 150.0), p=1.0),\n",
    "        A.ISONoise(color_shift=(0.01, 0.08), intensity=(0.1, 0.8), p=1.0),\n",
    "        A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n",
    "    ], p=0.8),\n",
    "    \n",
    "    # ë¬¸ì„œ í’ˆì§ˆ ì‹œë®¬ë ˆì´ì…˜ (ìŠ¤ìº”/ë³µì‚¬ íš¨ê³¼)\n",
    "    A.OneOf([\n",
    "        A.Downscale(scale_min=0.7, scale_max=0.9, p=1.0),\n",
    "        A.ImageCompression(quality_lower=60, quality_upper=95, p=1.0),\n",
    "        A.Posterize(num_bits=6, p=1.0),\n",
    "    ], p=0.5),\n",
    "    \n",
    "    # í”½ì…€ ë ˆë²¨ ë³€í™˜\n",
    "    A.OneOf([\n",
    "        A.ChannelShuffle(p=1.0),\n",
    "        A.InvertImg(p=1.0),\n",
    "        A.Solarize(threshold=128, p=1.0),\n",
    "        A.Equalize(p=1.0),\n",
    "    ], p=0.3),\n",
    "    \n",
    "    # ê³µê°„ ë³€í™˜\n",
    "    A.OneOf([\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),  # ë¬¸ì„œì—ì„œë„ ìœ ìš©í•  ìˆ˜ ìˆìŒ\n",
    "        A.Transpose(p=1.0),\n",
    "    ], p=0.6),\n",
    "    \n",
    "    # ì¡°ê° ì œê±° (Cutout ê³„ì—´)\n",
    "    A.OneOf([\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, \n",
    "                       min_holes=1, min_height=8, min_width=8, \n",
    "                       fill_value=0, p=1.0),\n",
    "        A.GridDropout(ratio=0.3, unit_size_min=8, unit_size_max=32, \n",
    "                     holes_number_x=5, holes_number_y=5, p=1.0),\n",
    "    ], p=0.4),\n",
    "    \n",
    "    # ìµœì¢… ì •ê·œí™”\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# test image ë³€í™˜ì„ ìœ„í•œ transform ì½”ë“œ\n",
    "tst_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c320bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„°: 1570ê°œ ìƒ˜í”Œ\n",
      " í´ë˜ìŠ¤ ë¶„í¬: {0: 100, 1: 46, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100, 10: 100, 11: 100, 12: 100, 13: 74, 14: 50, 15: 100, 16: 100}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/computervisioncompetition-cv-1/experiments/wandb/run-20250906_144106-pzrkpkjc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc' target=\"_blank\">efficientnet-b3-baseline-0906-1441</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ WandB ì‹¤í—˜ ì‹œì‘!\n",
      "ğŸ“Š ëŒ€ì‹œë³´ë“œ: https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc\n",
      "ğŸ“‹ ì‹¤í—˜ëª…: efficientnet-b3-baseline-0906-1441\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ 5-FOLD CROSS VALIDATION ì‹œì‘\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. Load Data & Start K-Fold Cross Validation with WandB\n",
    "# =============================================================================\n",
    "\n",
    "# ì „ì²´ í•™ìŠµ ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ ìƒ˜í”Œ\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "class_counts = train_df['target'].value_counts().sort_index()\n",
    "print(f\" í´ë˜ìŠ¤ ë¶„í¬: {dict(class_counts)}\")\n",
    "\n",
    "# K-Fold ì„¤ì •\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# K-Fold ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "fold_results = []\n",
    "fold_models = []  # ê° foldì˜ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥\n",
    "\n",
    "#  WandB ë©”ì¸ ì‹¤í—˜ ì‹œì‘\n",
    "main_run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    entity=ENTITY,\n",
    "    name=f\"{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "    config=config,\n",
    "    tags=[\"k-fold-cv\", \"ensemble\", model_name, \"baseline\", \"main-experiment\"],\n",
    "    group=\"k-fold-experiment\",\n",
    "    job_type=\"cross-validation\",\n",
    "    notes=f\"{N_FOLDS}-Fold Cross Validation with {model_name}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸš€ WandB ì‹¤í—˜ ì‹œì‘!\")\n",
    "print(f\"ğŸ“Š ëŒ€ì‹œë³´ë“œ: {main_run.url}\")\n",
    "print(f\"ğŸ“‹ ì‹¤í—˜ëª…: {main_run.name}\")\n",
    "\n",
    "#  ë°ì´í„°ì…‹ ì •ë³´ ë¡œê¹…\n",
    "wandb.log({\n",
    "    \"dataset/total_samples\": len(train_df),\n",
    "    \"dataset/num_classes\": 17,\n",
    "    \"dataset/samples_per_fold\": len(train_df) // N_FOLDS,\n",
    "})\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”\n",
    "class_dist_data = [[f\"Class_{i}\", count] for i, count in enumerate(class_counts)]\n",
    "wandb.log({\n",
    "    \"dataset/class_distribution\": wandb.plot.bar(\n",
    "        wandb.Table(data=class_dist_data, columns=[\"Class\", \"Count\"]),\n",
    "        \"Class\", \"Count\", \n",
    "        title=\"Training Data Class Distribution\"\n",
    "    )\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ¯ {N_FOLDS}-FOLD CROSS VALIDATION ì‹œì‘\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " FOLD 1/5\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dataset/num_classes</td><td>â–</td></tr><tr><td>dataset/samples_per_fold</td><td>â–</td></tr><tr><td>dataset/total_samples</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dataset/num_classes</td><td>17</td></tr><tr><td>dataset/samples_per_fold</td><td>314</td></tr><tr><td>dataset/total_samples</td><td>1570</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficientnet-b3-baseline-0906-1441</strong> at: <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc</a><br> View project at: <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250906_144106-pzrkpkjc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/computervisioncompetition-cv-1/experiments/wandb/run-20250906_144107-2tjtce0l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/2tjtce0l' target=\"_blank\">fold-1-efficientnet_b3-1441</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/2tjtce0l' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/2tjtce0l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Fold 1 Dashboard: https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/2tjtce0l\n",
      "Train samples: 1256, Validation samples: 314\n",
      " ëª¨ë¸ í•™ìŠµ ì‹œì‘ - Fold 1\n",
      "\n",
      "ğŸ“ˆ Epoch 1/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6.0430, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:20<00:00,  3.95it/s]\n",
      "Val Loss: 6.6661: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  1 | Train Loss: 5.5644 | Train F1: 0.0499 | Val Loss: 5.3148 | Val F1: 0.0848 | LR: 1.33e-04\n",
      "ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! F1: 0.0848\n",
      "\n",
      "ğŸ“ˆ Epoch 2/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.7266, Mixup: False, Cutout: True, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:14<00:00,  5.63it/s]\n",
      "Val Loss: 6.4441: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  2 | Train Loss: 5.3612 | Train F1: 0.0506 | Val Loss: 4.8685 | Val F1: 0.1196 | LR: 1.33e-04\n",
      "ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! F1: 0.1196\n",
      "\n",
      "ğŸ“ˆ Epoch 3/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.8223, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.66it/s]\n",
      "Val Loss: 6.2088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  3 | Train Loss: 5.0298 | Train F1: 0.0837 | Val Loss: 4.8058 | Val F1: 0.1404 | LR: 1.31e-04\n",
      "ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! F1: 0.1404\n",
      "\n",
      "ğŸ“ˆ Epoch 4/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.5508, Mixup: False, Cutout: True, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.64it/s]\n",
      "Val Loss: 5.8374: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  4 | Train Loss: 4.8693 | Train F1: 0.0790 | Val Loss: 4.6522 | Val F1: 0.1316 | LR: 1.29e-04\n",
      "\n",
      "ğŸ“ˆ Epoch 5/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.6504, Mixup: False, Cutout: False, RandomCrop: True: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.69it/s]\n",
      "Val Loss: 5.1404: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  5 | Train Loss: 4.5866 | Train F1: 0.1068 | Val Loss: 4.3914 | Val F1: 0.1477 | LR: 1.27e-04\n",
      "ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! F1: 0.1477\n",
      "\n",
      "ğŸ“ˆ Epoch 6/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.1328, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.70it/s]\n",
      "Val Loss: 5.1456: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  6 | Train Loss: 4.4928 | Train F1: 0.1131 | Val Loss: 4.3188 | Val F1: 0.1324 | LR: 1.24e-04\n",
      "\n",
      "ğŸ“ˆ Epoch 7/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.0547, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.69it/s]\n",
      "Val Loss: 4.6002: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  7 | Train Loss: 4.3326 | Train F1: 0.1251 | Val Loss: 4.0417 | Val F1: 0.1553 | LR: 1.20e-04\n",
      "ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! F1: 0.1553\n",
      "\n",
      "ğŸ“ˆ Epoch 8/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.8828, Mixup: False, Cutout: False, RandomCrop: True: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.67it/s]\n",
      "Val Loss: 4.5807: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  8 | Train Loss: 4.2297 | Train F1: 0.1406 | Val Loss: 4.2853 | Val F1: 0.1144 | LR: 1.15e-04\n",
      "\n",
      "ğŸ“ˆ Epoch 9/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.4297, Mixup: True, Cutout: False, RandomCrop: False: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:13<00:00,  5.70it/s]\n",
      "Val Loss: 4.1792: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  9 | Train Loss: 4.2773 | Train F1: 0.1368 | Val Loss: 3.7489 | Val F1: 0.1984 | LR: 1.10e-04\n",
      "ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! F1: 0.1984\n",
      "\n",
      "ğŸ“ˆ Epoch 10/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.8477, Mixup: True, Cutout: False, RandomCrop: False:   4%|â–         | 3/79 [00:06<02:50,  2.25s/it]\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ“ˆ Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m train_ret \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrn_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m    108\u001b[0m val_ret \u001b[38;5;241m=\u001b[39m validate_one_epoch(\n\u001b[1;32m    109\u001b[0m     val_loader, model, loss_fn, device, \n\u001b[1;32m    110\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch, fold\u001b[38;5;241m=\u001b[39mfold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    111\u001b[0m     log_confusion\u001b[38;5;241m=\u001b[39m(epoch \u001b[38;5;241m==\u001b[39m EPOCHS\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# ë§ˆì§€ë§‰ epochì—ë§Œ confusion matrix\u001b[39;00m\n\u001b[1;32m    112\u001b[0m )\n",
      "Cell \u001b[0;32mIn[24], line 129\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, optimizer, loss_fn, device, epoch, fold)\u001b[0m\n\u001b[1;32m    127\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    128\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    132\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:315\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 315\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:565\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    563\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    568\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 10. K-Fold Cross Validation Loop with WandB\n",
    "# =============================================================================\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\" FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # ê° foldë³„ child run ìƒì„±\n",
    "    fold_run = wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        entity=ENTITY,\n",
    "        name=f\"fold-{fold+1}-{model_name}-{datetime.now().strftime('%H%M')}\",\n",
    "        config=config,\n",
    "        tags=[\"fold\", f\"fold-{fold+1}\", model_name, \"child-run\"],\n",
    "        group=\"k-fold-experiment\",\n",
    "        job_type=f\"fold-{fold+1}\",\n",
    "        reinit=True  # ìƒˆë¡œìš´ run ì‹œì‘ í—ˆìš©\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Fold {fold+1} Dashboard: {fold_run.url}\")\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ train/validation ë°ì´í„° ë¶„í• \n",
    "    train_fold_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_fold_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # ë°ì´í„° ë¶„í•  ì •ë³´ ë¡œê¹…\n",
    "    wandb.log({\n",
    "        \"fold_info/fold_number\": fold + 1,\n",
    "        \"fold_info/train_samples\": len(train_fold_df),\n",
    "        \"fold_info/val_samples\": len(val_fold_df),\n",
    "        \"fold_info/train_ratio\": len(train_fold_df) / len(train_df),\n",
    "        \"fold_info/val_ratio\": len(val_fold_df) / len(train_df)\n",
    "    })\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ Dataset ìƒì„±\n",
    "    trn_dataset = ImageDataset(\n",
    "        train_fold_df,\n",
    "        \"../data/train/\",\n",
    "        transform=trn_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = ImageDataset(\n",
    "        val_fold_df,\n",
    "        \"../data/train/\",\n",
    "        transform=tst_transform  # ê²€ì¦ì—ëŠ” ì¦ê°• ì ìš© ì•ˆí•¨\n",
    "    )\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ DataLoader ìƒì„±\n",
    "    trn_loader = DataLoader(\n",
    "        trn_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(trn_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™” (ê° foldë§ˆë‹¤ ìƒˆë¡œìš´ ëª¨ë¸)\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "    \n",
    "    # Focal Loss: í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜\n",
    "    # alpha: í¬ê·€ í´ë˜ìŠ¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ (Optunaë¡œ ìµœì í™”ë¨)\n",
    "    # gamma: ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ëŠ” ì •ë„ (Optunaë¡œ ìµœì í™”ë¨)\n",
    "    loss_fn = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)  # ìµœì í™”ëœ Focal Loss ì ìš©\n",
    "    optimizer = Adam(model.parameters(), lr=LR, weight_decay=1e-4)  # weight_decay ì¶”ê°€\n",
    "    \n",
    "    # Learning Rate Scheduler ì¶”ê°€ (ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR * 0.01)\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ ìµœê³  ì„±ëŠ¥ ì¶”ì \n",
    "    best_val_f1 = 0.0\n",
    "    best_model = None\n",
    "    patience = 0\n",
    "    max_patience = 7\n",
    "    \n",
    "    print(f\" ëª¨ë¸ í•™ìŠµ ì‹œì‘ - Fold {fold+1}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 11. Training Loop for Current Fold\n",
    "    # =============================================================================\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nğŸ“ˆ Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        # Training\n",
    "        train_ret = train_one_epoch(\n",
    "            trn_loader, model, optimizer, loss_fn, device, \n",
    "            epoch=epoch, fold=fold+1\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_ret = validate_one_epoch(\n",
    "            val_loader, model, loss_fn, device, \n",
    "            epoch=epoch, fold=fold+1,\n",
    "            log_confusion=(epoch == EPOCHS-1)  # ë§ˆì§€ë§‰ epochì—ë§Œ confusion matrix\n",
    "        )\n",
    "        \n",
    "        # Learning rate ë¡œê¹…\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # WandBì— metrics ë¡œê¹…\n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"fold\": fold + 1,\n",
    "            \"train/loss\": train_ret['train_loss'],\n",
    "            \"train/accuracy\": train_ret['train_acc'], \n",
    "            \"train/f1\": train_ret['train_f1'],\n",
    "            \"val/loss\": val_ret['val_loss'],\n",
    "            \"val/accuracy\": val_ret['val_acc'],\n",
    "            \"val/f1\": val_ret['val_f1'],\n",
    "            \"learning_rate\": current_lr,\n",
    "            \"optimizer/lr\": current_lr\n",
    "        }\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated(0) / 1e9\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            log_data.update({\n",
    "                \"system/gpu_memory_used_gb\": gpu_memory_used,\n",
    "                \"system/gpu_memory_total_gb\": gpu_memory_total,\n",
    "                \"system/gpu_utilization_pct\": (gpu_memory_used / gpu_memory_total) * 100\n",
    "            })\n",
    "        \n",
    "        wandb.log(log_data)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\" Epoch {epoch+1:2d} | \"\n",
    "              f\"Train Loss: {train_ret['train_loss']:.4f} | \"\n",
    "              f\"Train F1: {train_ret['train_f1']:.4f} | \"\n",
    "              f\"Val Loss: {val_ret['val_loss']:.4f} | \"\n",
    "              f\"Val F1: {val_ret['val_f1']:.4f} | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "        if val_ret['val_f1'] > best_val_f1:\n",
    "            best_val_f1 = val_ret['val_f1']\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            patience = 0\n",
    "            \n",
    "            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥\n",
    "            model_path = f'best_model_fold_{fold+1}.pth'\n",
    "            torch.save(best_model, model_path)\n",
    "            wandb.save(model_path, policy=\"now\")\n",
    "            \n",
    "            # ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ë¡œê¹…\n",
    "            wandb.log({\n",
    "                f\"best_performance/epoch\": epoch + 1,\n",
    "                f\"best_performance/val_f1\": best_val_f1,\n",
    "                f\"best_performance/val_acc\": val_ret['val_acc'],\n",
    "                f\"best_performance/val_loss\": val_ret['val_loss'],\n",
    "            })\n",
    "            \n",
    "            print(f\"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            \n",
    "        # Early stopping (ì„ íƒì )\n",
    "        if patience >= max_patience and epoch > EPOCHS // 2:\n",
    "            print(f\"â¸ï¸ Early stopping at epoch {epoch+1} (patience: {patience})\")\n",
    "            wandb.log({\"early_stopping/epoch\": epoch + 1})\n",
    "            break\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 12. Fold Results Summary\n",
    "    # =============================================================================\n",
    "    \n",
    "    # í˜„ì¬ fold ê²°ê³¼ ì €ì¥\n",
    "    fold_result = {\n",
    "        'fold': fold + 1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'final_train_f1': train_ret['train_f1'],\n",
    "        'train_samples': len(trn_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'epochs_trained': epoch + 1,\n",
    "        'early_stopped': patience >= max_patience\n",
    "    }\n",
    "    \n",
    "    fold_results.append(fold_result)\n",
    "    fold_models.append(best_model)\n",
    "    \n",
    "    # Fold ìµœì¢… ìš”ì•½ ë¡œê¹…\n",
    "    wandb.log({\n",
    "        \"fold_summary/best_val_f1\": best_val_f1,\n",
    "        \"fold_summary/final_train_f1\": train_ret['train_f1'],\n",
    "        \"fold_summary/epochs_trained\": epoch + 1,\n",
    "        \"fold_summary/improvement\": best_val_f1 - val_ret['val_f1'],\n",
    "        \"fold_summary/early_stopped\": patience >= max_patience\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n Fold {fold + 1} ì™„ë£Œ!\")\n",
    "    print(f\" ìµœê³  Validation F1: {best_val_f1:.4f}\")\n",
    "    print(f\" í•™ìŠµëœ ì—í­: {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Fold run ì¢…ë£Œ\n",
    "    wandb.finish()\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del model, optimizer, scheduler, trn_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb258fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7-3. Optuna êµ¬í˜„ í…ŒìŠ¤íŠ¸\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ§ª Optuna êµ¬í˜„ í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Optuna ì„¤ì • í™•ì¸\n",
    "print(f\"âœ… USE_OPTUNA: {USE_OPTUNA}\")\n",
    "print(f\"âœ… OPTUNA_TRIALS: {OPTUNA_TRIALS}\")\n",
    "print(f\"âœ… OPTUNA_TIMEOUT: {OPTUNA_TIMEOUT}ì´ˆ\")\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ìˆ˜ í™•ì¸\n",
    "print(f\"\\nğŸ“‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ìˆ˜ ìƒíƒœ:\")\n",
    "print(f\"   LR: {LR}\")\n",
    "print(f\"   BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   EPOCHS: {EPOCHS}\")\n",
    "print(f\"   img_size: {img_size}\")\n",
    "print(f\"   focal_alpha: {focal_alpha}\")\n",
    "print(f\"   focal_gamma: {focal_gamma}\")\n",
    "print(f\"   mixup_alpha: {mixup_alpha}\")\n",
    "print(f\"   mixup_prob: {mixup_prob}\")\n",
    "print(f\"   gradient_clipping: {gradient_clipping}\")\n",
    "\n",
    "# Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "try:\n",
    "    import optuna\n",
    "    print(f\"\\nâœ… Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ: v{optuna.__version__}\")\n",
    "    \n",
    "    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ study ìƒì„±\n",
    "    def simple_objective(trial):\n",
    "        x = trial.suggest_float('x', -10, 10)\n",
    "        return (x - 2) ** 2\n",
    "    \n",
    "    test_study = optuna.create_study(direction='minimize')\n",
    "    test_study.optimize(simple_objective, n_trials=3)\n",
    "    \n",
    "    print(f\"âœ… Optuna ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ\")\n",
    "    print(f\"   í…ŒìŠ¤íŠ¸ ìµœì ê°’: {test_study.best_value:.4f}\")\n",
    "    print(f\"   í…ŒìŠ¤íŠ¸ ìµœì  íŒŒë¼ë¯¸í„°: {test_study.best_params}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Optuna í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# FocalLoss í…ŒìŠ¤íŠ¸\n",
    "try:\n",
    "    test_logits = torch.randn(4, 17)\n",
    "    test_targets = torch.randint(0, 17, (4,))\n",
    "    \n",
    "    focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
    "    loss_value = focal_loss(test_logits, test_targets)\n",
    "    \n",
    "    print(f\"\\nâœ… FocalLoss í…ŒìŠ¤íŠ¸ ì„±ê³µ\")\n",
    "    print(f\"   alpha: {focal_alpha}\")\n",
    "    print(f\"   gamma: {focal_gamma}\")\n",
    "    print(f\"   loss_value: {loss_value.item():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ FocalLoss í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ í™•ì¸\n",
    "try:\n",
    "    import os\n",
    "    data_exists = os.path.exists(data_path)\n",
    "    train_csv_exists = os.path.exists(os.path.join(data_path, 'train.csv'))\n",
    "    train_dir_exists = os.path.exists(os.path.join(data_path, 'train'))\n",
    "    \n",
    "    print(f\"\\nğŸ“ ë°ì´í„° ê²½ë¡œ í™•ì¸:\")\n",
    "    print(f\"   data_path: {data_path}\")\n",
    "    print(f\"   ê²½ë¡œ ì¡´ì¬: {data_exists}\")\n",
    "    print(f\"   train.csv ì¡´ì¬: {train_csv_exists}\")\n",
    "    print(f\"   train/ ë””ë ‰í† ë¦¬ ì¡´ì¬: {train_dir_exists}\")\n",
    "    \n",
    "    if data_exists and train_csv_exists and train_dir_exists:\n",
    "        print(f\"âœ… ë°ì´í„° ê²½ë¡œ ì •ìƒ\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ ë°ì´í„° ê²½ë¡œì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Optuna êµ¬í˜„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"   ì´ì œ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"   USE_OPTUNA=Trueë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ ìë™ìœ¼ë¡œ ìµœì í™”ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " K-FOLD CROSS VALIDATION ìµœì¢… ê²°ê³¼\n",
      "============================================================\n",
      " í™œì„±í™”ëœ runì´ ì—†ì–´ ìƒˆë¡œìš´ summary runì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/computervisioncompetition-cv-1/experiments/wandb/run-20250906_142625-bh62ochz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/bh62ochz' target=\"_blank\">SUMMARY-efficientnet-b3-baseline-0906-1426</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/bh62ochz' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/bh62ochz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CV ê²°ê³¼ ë¡œê¹… ì™„ë£Œ!\n",
      "Fold 1: 0.9143 (28 epochs)  Early Stopped\n",
      "Fold 2: 0.9087 (27 epochs)  Early Stopped\n",
      "Fold 3: 0.9319 (36 epochs)  Early Stopped\n",
      "Fold 4: 0.9083 (33 epochs)  Early Stopped\n",
      "Fold 5: 0.9321 (49 epochs)  Early Stopped\n",
      "\n",
      " í‰ê·  CV F1: 0.9191 Â± 0.0108\n",
      " ìµœê³  Fold: 0.9321\n",
      " ìµœì•… Fold: 0.9083\n",
      " ì„±ëŠ¥ ë²”ìœ„: 0.0238\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 13. K-Fold Cross Validation Results Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" K-FOLD CROSS VALIDATION ìµœì¢… ê²°ê³¼\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "val_f1_scores = [result['best_val_f1'] for result in fold_results]\n",
    "mean_f1 = np.mean(val_f1_scores)\n",
    "std_f1 = np.std(val_f1_scores)\n",
    "\n",
    "try:\n",
    "    # wandb.runì´ í˜„ì¬ í™œì„±í™”ëœ runì„ ê°€ë¦¬í‚´\n",
    "    if wandb.run is None:\n",
    "        print(\" í™œì„±í™”ëœ runì´ ì—†ì–´ ìƒˆë¡œìš´ summary runì„ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        active_run = wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            name=f\"SUMMARY-{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "            config=config,\n",
    "            tags=[\"summary\", \"cv-results\", model_name],\n",
    "            group=\"k-fold-experiment\",\n",
    "            job_type=\"summary\",\n",
    "            reinit=True\n",
    "        )\n",
    "    else:\n",
    "        print(\" ê¸°ì¡´ runì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        active_run = wandb.run\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Run ìƒíƒœ í™•ì¸ ì¤‘ ì—ëŸ¬: {e}\")\n",
    "    # ìƒˆë¡œìš´ run ìƒì„±\n",
    "    active_run = wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        name=f\"SUMMARY-{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "        config=config,\n",
    "        tags=[\"summary\", \"cv-results\", model_name],\n",
    "        group=\"k-fold-experiment\",\n",
    "        job_type=\"summary\",\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "# CV ìš”ì•½ í…Œì´ë¸” ìƒì„±\n",
    "fold_table = wandb.Table(columns=[\n",
    "    \"Fold\", \"Best_Val_F1\", \"Final_Train_F1\", \"Train_Samples\", \n",
    "    \"Val_Samples\", \"Epochs_Trained\", \"Early_Stopped\"\n",
    "])\n",
    "\n",
    "for result in fold_results:\n",
    "    fold_table.add_data(\n",
    "        result['fold'], \n",
    "        result['best_val_f1'], \n",
    "        result['final_train_f1'],\n",
    "        result['train_samples'], \n",
    "        result['val_samples'],\n",
    "        result['epochs_trained'],\n",
    "        result['early_stopped']\n",
    "    )\n",
    "\n",
    "# ì•ˆì „í•œ ë¡œê¹…\n",
    "try:\n",
    "    active_run.log({\n",
    "        \"cv_results/mean_f1\": mean_f1,\n",
    "        \"cv_results/std_f1\": std_f1,\n",
    "        \"cv_results/best_fold_f1\": max(val_f1_scores),\n",
    "        \"cv_results/worst_fold_f1\": min(val_f1_scores),\n",
    "        \"cv_results/f1_range\": max(val_f1_scores) - min(val_f1_scores),\n",
    "        \"cv_results/fold_results_table\": fold_table,\n",
    "        \"cv_results/n_folds\": N_FOLDS,\n",
    "        \"cv_results/total_epochs\": sum([r['epochs_trained'] for r in fold_results]),\n",
    "        \"cv_results/avg_epochs_per_fold\": np.mean([r['epochs_trained'] for r in fold_results]),\n",
    "        \"cv_results/early_stopped_folds\": sum([r['early_stopped'] for r in fold_results])\n",
    "    })\n",
    "    \n",
    "    # Foldë³„ ì„±ëŠ¥ ë°”ì°¨íŠ¸ ìƒì„±\n",
    "    fold_performance_data = [[f\"Fold {i+1}\", score] for i, score in enumerate(val_f1_scores)]\n",
    "    active_run.log({\n",
    "        \"cv_results/fold_performance_chart\": wandb.plot.bar(\n",
    "            wandb.Table(data=fold_performance_data, columns=[\"Fold\", \"F1_Score\"]),\n",
    "            \"Fold\", \"F1_Score\", \n",
    "            title=\"K-Fold Cross Validation Performance\"\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    print(\" CV ê²°ê³¼ ë¡œê¹… ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" WandB ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "    print(\" ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤:\")\n",
    "\n",
    "# ì–´ë–¤ ê²½ìš°ë“  ì½˜ì†”ì—ëŠ” ê²°ê³¼ ì¶œë ¥\n",
    "for result in fold_results:\n",
    "    status = \" Early Stopped\" if result['early_stopped'] else \" Completed\"\n",
    "    print(f\"Fold {result['fold']}: {result['best_val_f1']:.4f} \"\n",
    "          f\"({result['epochs_trained']} epochs) {status}\")\n",
    "\n",
    "print(f\"\\n í‰ê·  CV F1: {mean_f1:.4f} Â± {std_f1:.4f}\")\n",
    "print(f\" ìµœê³  Fold: {max(val_f1_scores):.4f}\")\n",
    "print(f\" ìµœì•… Fold: {min(val_f1_scores):.4f}\")\n",
    "print(f\" ì„±ëŠ¥ ë²”ìœ„: {max(val_f1_scores) - min(val_f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ ì•™ìƒë¸” ëª¨ë¸ ì¤€ë¹„ ì¤‘...\n",
      "Fold 1 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "Fold 2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "Fold 3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "Fold 4 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "Fold 5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      " ì´ 5ê°œ ëª¨ë¸ë¡œ ì•™ìƒë¸” êµ¬ì„±\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 14. Ensemble Models Preparation\n",
    "# =============================================================================\n",
    "\n",
    "# 5-Fold ì•™ìƒë¸” ëª¨ë¸ ì¤€ë¹„\n",
    "ensemble_models = []\n",
    "print(f\"\\nğŸ”§ ì•™ìƒë¸” ëª¨ë¸ ì¤€ë¹„ ì¤‘...\")\n",
    "\n",
    "for i, state_dict in enumerate(fold_models):\n",
    "    fold_model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    fold_model.load_state_dict(state_dict)\n",
    "    fold_model.eval()\n",
    "    ensemble_models.append(fold_model)\n",
    "    print(f\"Fold {i+1} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "print(f\" ì´ {len(ensemble_models)}ê°œ ëª¨ë¸ë¡œ ì•™ìƒë¸” êµ¬ì„±\")\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"ensemble/num_models\": len(ensemble_models),\n",
    "            \"ensemble/model_architecture\": model_name,\n",
    "            \"ensemble/ensemble_type\": \"simple_average\"\n",
    "        })\n",
    "    else:\n",
    "        print(\"ğŸ“Š ì•™ìƒë¸” ì •ë³´:\")\n",
    "        print(f\"  - ëª¨ë¸ ê°œìˆ˜: {len(ensemble_models)}\")\n",
    "        print(f\"  - ì•„í‚¤í…ì²˜: {model_name}\")\n",
    "        print(f\"  - ì•™ìƒë¸” íƒ€ì…: simple_average\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ì•™ìƒë¸” ì •ë³´ ë¡œê¹… ì‹¤íŒ¨: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98e388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TTA (Test Time Augmentation) ì„¤ì •...\n",
      "TTA ë³€í™˜ 5ê°œ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 15. TTA (Test Time Augmentation) Setup\n",
    "# =============================================================================\n",
    "\n",
    "# Temperature Scaling í´ë˜ìŠ¤ ì •ì˜\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self, temperature=1.5):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * temperature)\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "print(f\"\\n TTA (Test Time Augmentation) ì„¤ì •...\")\n",
    "\n",
    "# Essential TTA transforms\n",
    "essential_tta_transforms = [\n",
    "    # ì›ë³¸\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 90ë„ íšŒì „ë“¤\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[90, 90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[180, 180], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[-90, -90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # ë°ê¸° ê°œì„ \n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[0.3, 0.3], contrast_limit=[0.3, 0.3], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "print(f\"TTA ë³€í™˜ {len(essential_tta_transforms)}ê°œ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"tta/num_transforms\": len(essential_tta_transforms),\n",
    "            \"tta/transforms_used\": [\"original\", \"rot_90\", \"rot_180\", \"rot_270\", \"brightness\"],\n",
    "            \"tta/batch_size\": 64  # TTAìš© ë°°ì¹˜ í¬ê¸°\n",
    "        })\n",
    "    else:\n",
    "        print(\"ğŸ“Š TTA ì„¤ì • ì •ë³´:\")\n",
    "        print(f\"  - ë³€í˜• ê°œìˆ˜: {len(essential_tta_transforms)}\")\n",
    "        print(f\"  - ë³€í˜• ì¢…ë¥˜: original, rot_90, rot_180, rot_270, brightness\")\n",
    "        print(f\"  - ë°°ì¹˜ í¬ê¸°: 64\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ TTA ì„¤ì • ë¡œê¹… ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ“Š TTA ì„¤ì • ì •ë³´:\")\n",
    "    print(f\"  - ë³€í˜• ê°œìˆ˜: {len(essential_tta_transforms)}\")\n",
    "    print(f\"  - ë°°ì¹˜ í¬ê¸°: 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TTA Dataset: 3140ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 16. TTA Dataset and DataLoader\n",
    "# =============================================================================\n",
    "\n",
    "class TTAImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms  # ì—¬ëŸ¬ transformì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ìŒ\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        \n",
    "        # ëª¨ë“  transformì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            aug_img = transform(image=img)['image']\n",
    "            augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target\n",
    "\n",
    "# TTA Dataset ìƒì„±\n",
    "tta_dataset = TTAImageDataset(\n",
    "    \"../data/sample_submission.csv\",\n",
    "    \"../data/test/\",\n",
    "    essential_tta_transforms\n",
    ")\n",
    "\n",
    "# TTA DataLoader (ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "tta_loader = DataLoader(\n",
    "    tta_dataset,\n",
    "    batch_size=64,  # TTAëŠ” ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì¤„ì„\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\" TTA Dataset: {len(tta_dataset)}ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45382398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ìµœì¢… ì¶”ë¡  - ì•™ìƒë¸” + TTA\n",
      "============================================================\n",
      "ì•™ìƒë¸” TTA ì¶”ë¡  ì‹œì‘...\n",
      "5ê°œ ëª¨ë¸ Ã— 5ê°œ TTA ë³€í˜• = 25ê°œ ì˜ˆì¸¡ í‰ê· \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble TTA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:31<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ì•™ìƒë¸” TTA ì¶”ë¡  ì™„ë£Œ!\n",
      "ì´ ì†Œìš”ì‹œê°„: 2.5ë¶„\n",
      " í‰ê·  ì‹ ë¢°ë„: 0.6706 Â± 0.1795\n",
      " ê³ ì‹ ë¢°ë„ ìƒ˜í”Œ: 77/3140 (2.5%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 17. Ensemble + TTA Inference with WandB Logging\n",
    "# =============================================================================\n",
    "\n",
    "def ensemble_tta_inference_with_logging(models, loader, transforms, confidence_threshold=0.9):\n",
    "    \"\"\"5-Fold ëª¨ë¸ ì•™ìƒë¸” + TTA ì¶”ë¡  with WandB ë¡œê¹…\"\"\"\n",
    "    all_predictions = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    # TTA ì§„í–‰ìƒí™© ë¡œê¹…ì„ ìœ„í•œ í…Œì´ë¸”\n",
    "    tta_progress = wandb.Table(columns=[\"Batch\", \"Avg_Confidence\", \"Low_Conf_Count\", \"High_Conf_Count\"])\n",
    "    \n",
    "    # Temperature scaling ì´ˆê¸°í™”\n",
    "    temp_scaling = TemperatureScaling().to(device)\n",
    "    \n",
    "    print(f\"ì•™ìƒë¸” TTA ì¶”ë¡  ì‹œì‘...\")\n",
    "    print(f\"{len(models)}ê°œ ëª¨ë¸ Ã— {len(transforms)}ê°œ TTA ë³€í˜• = {len(models) * len(transforms)}ê°œ ì˜ˆì¸¡ í‰ê· \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Ensemble TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        ensemble_probs = torch.zeros(batch_size, 17).to(device)\n",
    "        \n",
    "        # ê° fold ëª¨ë¸ë³„ ì˜ˆì¸¡\n",
    "        for model_idx, model in enumerate(models):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # ê° TTA ë³€í˜•ë³„ ì˜ˆì¸¡\n",
    "                for tta_idx, images in enumerate(images_list):\n",
    "                    images = images.to(device)\n",
    "                    preds = model(images)\n",
    "                    \n",
    "                    # Temperature scaling ì ìš©\n",
    "                    preds = temp_scaling(preds)\n",
    "                    probs = torch.softmax(preds, dim=1)\n",
    "                    \n",
    "                    # ì•™ìƒë¸” í™•ë¥ ì— ëˆ„ì  (í‰ê· )\n",
    "                    ensemble_probs += probs / (len(models) * len(images_list))\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ê³„ì‚°\n",
    "        max_probs = torch.max(ensemble_probs, dim=1)[0]\n",
    "        batch_confidences = max_probs.cpu().numpy()\n",
    "        all_confidences.extend(batch_confidences)\n",
    "        \n",
    "        final_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "        all_predictions.extend(final_preds.cpu().numpy())\n",
    "        \n",
    "        # ë°°ì¹˜ë³„ ì‹ ë¢°ë„ ë¶„ì„\n",
    "        high_conf_count = np.sum(batch_confidences >= confidence_threshold)\n",
    "        low_conf_count = batch_size - high_conf_count\n",
    "        avg_confidence = np.mean(batch_confidences)\n",
    "        \n",
    "        # ì§„í–‰ìƒí™© í…Œì´ë¸”ì— ì¶”ê°€\n",
    "        tta_progress.add_data(batch_idx, avg_confidence, low_conf_count, high_conf_count)\n",
    "        \n",
    "        # ë°°ì¹˜ë³„ ìƒì„¸ ë¡œê¹… (20ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if batch_idx % 20 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            estimated_total = elapsed_time * len(loader) / (batch_idx + 1)\n",
    "            remaining_time = estimated_total - elapsed_time\n",
    "            \n",
    "            wandb.log({\n",
    "                \"tta_progress/batch\": batch_idx,\n",
    "                \"tta_progress/avg_confidence\": avg_confidence,\n",
    "                \"tta_progress/high_confidence_ratio\": high_conf_count / batch_size,\n",
    "                \"tta_progress/low_confidence_count\": low_conf_count,\n",
    "                \"tta_progress/elapsed_time_min\": elapsed_time / 60,\n",
    "                \"tta_progress/estimated_remaining_min\": remaining_time / 60,\n",
    "                \"tta_progress/samples_processed\": (batch_idx + 1) * batch_size,\n",
    "            })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # TTA ìµœì¢… ê²°ê³¼ ë¡œê¹…\n",
    "    final_avg_confidence = np.mean(all_confidences)\n",
    "    confidence_std = np.std(all_confidences)\n",
    "    high_conf_samples = np.sum(np.array(all_confidences) >= confidence_threshold)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"tta_results/total_time_min\": total_time / 60,\n",
    "        \"tta_results/samples_per_second\": len(all_predictions) / total_time,\n",
    "        \"tta_results/final_avg_confidence\": final_avg_confidence,\n",
    "        \"tta_results/confidence_std\": confidence_std,\n",
    "        \"tta_results/high_confidence_samples\": high_conf_samples,\n",
    "        \"tta_results/high_confidence_ratio\": high_conf_samples / len(all_predictions),\n",
    "        \"tta_results/total_predictions\": len(all_predictions),\n",
    "        \"tta_results/confidence_histogram\": wandb.Histogram(all_confidences),\n",
    "        \"tta_results/progress_table\": tta_progress\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n ì•™ìƒë¸” TTA ì¶”ë¡  ì™„ë£Œ!\")\n",
    "    print(f\"ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
    "    print(f\" í‰ê·  ì‹ ë¢°ë„: {final_avg_confidence:.4f} Â± {confidence_std:.4f}\")\n",
    "    print(f\" ê³ ì‹ ë¢°ë„ ìƒ˜í”Œ: {high_conf_samples}/{len(all_predictions)} ({high_conf_samples/len(all_predictions)*100:.1f}%)\")\n",
    "    \n",
    "    return all_predictions, all_confidences\n",
    "\n",
    "# ì•™ìƒë¸” TTA ì‹¤í–‰\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" ìµœì¢… ì¶”ë¡  - ì•™ìƒë¸” + TTA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "tta_predictions, confidences = ensemble_tta_inference_with_logging(\n",
    "    models=ensemble_models, \n",
    "    loader=tta_loader, \n",
    "    transforms=essential_tta_transforms,\n",
    "    confidence_threshold=0.9\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9072c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ìµœì¢… ê²°ê³¼ ì •ë¦¬ ì¤‘...\n",
      "\n",
      "ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\n",
      "Class  0:  200 (  6.4%)\n",
      "Class  1:   90 (  2.9%)\n",
      "Class  2:  200 (  6.4%)\n",
      "Class  3:  166 (  5.3%)\n",
      "Class  4:  206 (  6.6%)\n",
      "Class  5:  200 (  6.4%)\n",
      "Class  6:  205 (  6.5%)\n",
      "Class  7:  246 (  7.8%)\n",
      "Class  8:  200 (  6.4%)\n",
      "Class  9:  200 (  6.4%)\n",
      "Class 10:  204 (  6.5%)\n",
      "Class 11:  191 (  6.1%)\n",
      "Class 12:  199 (  6.3%)\n",
      "Class 13:  157 (  5.0%)\n",
      "Class 14:   76 (  2.4%)\n",
      "Class 15:  200 (  6.4%)\n",
      "Class 16:  200 (  6.4%)\n",
      "ìµœì¢… ê²°ê³¼ WandB ë¡œê¹… ì™„ë£Œ!\n",
      "ì´ ì˜ˆì¸¡ ìˆ˜: 3140\n",
      "ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ìˆ˜: 17\n",
      "í‰ê·  ì‹ ë¢°ë„: 0.6706\n",
      "ì‹ ë¢°ë„ ë²”ìœ„: 0.1487 ~ 0.9539\n",
      "ì˜ˆì¸¡ ë¶„í¬ ì°¨íŠ¸ ë¡œê¹… ì™„ë£Œ!\n",
      "ì‹¤í—˜ ìš”ì•½ ë¡œê¹… ì™„ë£Œ!\n",
      "\n",
      " ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\n",
      " íŒŒì¼ ìœ„ì¹˜: ../output/choice8.csv\n",
      " ì´ ì˜ˆì¸¡ ìˆ˜: 3140\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 18. Final Results and Submission\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n ìµœì¢… ê²°ê³¼ ì •ë¦¬ ì¤‘...\")\n",
    "\n",
    "# TTA ê²°ê³¼ë¡œ submission íŒŒì¼ ìƒì„±\n",
    "tta_pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n",
    "tta_pred_df['target'] = tta_predictions\n",
    "\n",
    "# ê¸°ì¡´ submissionê³¼ ë™ì¼í•œ ìˆœì„œì¸ì§€ í™•ì¸\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == tta_pred_df['ID']).all(), \"ID ìˆœì„œ ë¶ˆì¼ì¹˜!\"\n",
    "\n",
    "# ì˜ˆì¸¡ ë¶„í¬ ë¶„ì„\n",
    "pred_distribution = tta_pred_df['target'].value_counts().sort_index()\n",
    "pred_table = wandb.Table(columns=[\"Class\", \"Count\", \"Percentage\"])\n",
    "\n",
    "print(f\"\\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\")\n",
    "for class_id in range(17):\n",
    "    count = pred_distribution.get(class_id, 0)\n",
    "    percentage = count / len(tta_pred_df) * 100\n",
    "    pred_table.add_data(class_id, count, percentage)\n",
    "    print(f\"Class {class_id:2d}: {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "# ì‹ ë¢°ë„ ë¶„ì„\n",
    "confidence_bins = [0.5, 0.7, 0.8, 0.9, 0.95, 1.0]\n",
    "confidence_analysis = {}\n",
    "for i, threshold in enumerate(confidence_bins):\n",
    "    if i == 0:\n",
    "        count = np.sum(np.array(confidences) >= threshold)\n",
    "    else:\n",
    "        prev_threshold = confidence_bins[i-1]\n",
    "        count = np.sum((np.array(confidences) >= prev_threshold) & (np.array(confidences) < threshold))\n",
    "    confidence_analysis[f\"conf_{threshold}\"] = count\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ë¡œê¹…\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"final_results/total_predictions\": len(tta_predictions),\n",
    "            \"final_results/unique_classes_predicted\": len(np.unique(tta_predictions)),\n",
    "            \"final_results/prediction_distribution_table\": pred_table,\n",
    "            \"final_results/avg_confidence\": np.mean(confidences),\n",
    "            \"final_results/median_confidence\": np.median(confidences),\n",
    "            \"final_results/min_confidence\": np.min(confidences),\n",
    "            \"final_results/max_confidence\": np.max(confidences),\n",
    "            \"final_results/confidence_distribution\": wandb.Histogram(confidences),\n",
    "            **confidence_analysis\n",
    "        })\n",
    "        print(\"ìµœì¢… ê²°ê³¼ WandB ë¡œê¹… ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"í™œì„±í™”ëœ runì´ ì—†ì–´ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"WandB ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "# ì½˜ì†” ì¶œë ¥ì€ í•­ìƒ ì‹¤í–‰\n",
    "print(f\"ì´ ì˜ˆì¸¡ ìˆ˜: {len(tta_predictions)}\")\n",
    "print(f\"ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ìˆ˜: {len(np.unique(tta_predictions))}\")\n",
    "print(f\"í‰ê·  ì‹ ë¢°ë„: {np.mean(confidences):.4f}\")\n",
    "print(f\"ì‹ ë¢°ë„ ë²”ìœ„: {np.min(confidences):.4f} ~ {np.max(confidences):.4f}\")\n",
    "\n",
    "\n",
    "# ì˜ˆì¸¡ ë¶„í¬ ë°”ì°¨íŠ¸\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        pred_dist_data = [[f\"Class_{i}\", pred_distribution.get(i, 0)] for i in range(17)]\n",
    "        wandb.run.log({\n",
    "            \"final_results/prediction_distribution_chart\": wandb.plot.bar(\n",
    "                wandb.Table(data=pred_dist_data, columns=[\"Class\", \"Count\"]),\n",
    "                \"Class\", \"Count\", \n",
    "                title=\"Final Prediction Distribution\"\n",
    "            )\n",
    "        })\n",
    "        print(\"ì˜ˆì¸¡ ë¶„í¬ ì°¨íŠ¸ ë¡œê¹… ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"ì°¨íŠ¸ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"ì°¨íŠ¸ ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "output_path = \"../output/choice9.csv\"\n",
    "tta_pred_df.to_csv(output_path, index=False)\n",
    "\n",
    "# ê²°ê³¼ íŒŒì¼ì„ WandB ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"final_predictions\",\n",
    "    type=\"predictions\",\n",
    "    description=f\"Final ensemble predictions with {N_FOLDS}-fold CV + TTA\"\n",
    ")\n",
    "artifact.add_file(output_path)\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log_artifact(artifact)\n",
    "        print(\"ì‹¤í—˜ ìš”ì•½ ë¡œê¹… ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"í™œì„±í™”ëœ runì´ ì—†ì–´ ì‹¤í—˜ ìš”ì•½ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"ì‹¤í—˜ ìš”ì•½ ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\n ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\" íŒŒì¼ ìœ„ì¹˜: {output_path}\")\n",
    "print(f\" ì´ ì˜ˆì¸¡ ìˆ˜: {len(tta_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30990203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‹¤í—˜ ìš”ì•½ ë¡œê¹… ì™„ë£Œ!\n",
      "ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\n",
      "\n",
      "ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: 2025-09-06 14:29:43\n",
      "\n",
      "============================================================\n",
      "ì‹¤í—˜ ì™„ë£Œ!\n",
      "============================================================\n",
      " K-Fold CV ê²°ê³¼: 0.9191 Â± 0.0108\n",
      " ìµœê³  ì„±ëŠ¥ Fold: 0.9321\n",
      " ì•™ìƒë¸” ëª¨ë¸: 5ê°œ\n",
      " TTA ë³€í˜•: 5ê°œ\n",
      " í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: 0.6706\n",
      " WandB ëŒ€ì‹œë³´ë“œ: https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/b1avqwn7\n",
      "\n",
      " ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ:\n",
      "                     ID  target\n",
      "0  0008fdb22ddce0ce.jpg       2\n",
      "1  00091bffdffd83de.jpg      12\n",
      "2  00396fbc1f6cc21d.jpg       5\n",
      "3  00471f8038d9c4b6.jpg      12\n",
      "4  00901f504008d884.jpg       2\n",
      "5  009b22decbc7220c.jpg      15\n",
      "6  00b33e0ee6d59427.jpg       0\n",
      "7  00bbdcfbbdb3e131.jpg       8\n",
      "8  00c03047e0fbef40.jpg      15\n",
      "9  00c0dabb63ca7a16.jpg      11\n",
      "\n",
      " ëª¨ë“  ì‘ì—… ì™„ë£Œ!\n",
      " ê²°ê³¼ íŒŒì¼: ../output/choice8.csv\n",
      " WandBì—ì„œ ì „ì²´ ì‹¤í—˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 19. Experiment Summary and Cleanup\n",
    "# =============================================================================\n",
    "\n",
    "# ì‹¤í—˜ ìš”ì•½ ìƒì„±\n",
    "experiment_summary = {\n",
    "    \"experiment_name\": main_run.name,\n",
    "    \"model_architecture\": model_name,\n",
    "    \"image_size\": img_size,\n",
    "    \"cv_strategy\": f\"{N_FOLDS}-Fold StratifiedKFold\",\n",
    "    \"cv_mean_f1\": mean_f1,\n",
    "    \"cv_std_f1\": std_f1,\n",
    "    \"cv_best_fold\": max(val_f1_scores),\n",
    "    \"ensemble_models\": len(ensemble_models),\n",
    "    \"tta_transforms\": len(essential_tta_transforms),\n",
    "    \"total_training_time_min\": sum([r['epochs_trained'] for r in fold_results]) * 2,  # ì¶”ì •ì¹˜\n",
    "    \"avg_prediction_confidence\": np.mean(confidences),\n",
    "    \"high_confidence_predictions\": np.sum(np.array(confidences) >= 0.9),\n",
    "    \"experiment_tags\": [\"baseline\", \"efficientnet-b3\", \"k-fold-cv\", \"tta\", \"ensemble\"]\n",
    "}\n",
    "\n",
    "# ì‹¤í—˜ ìš”ì•½\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\"experiment_summary\": experiment_summary})\n",
    "        print(\"ì‹¤í—˜ ìš”ì•½ ë¡œê¹… ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"í™œì„±í™”ëœ runì´ ì—†ì–´ ì‹¤í—˜ ìš”ì•½ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"ì‹¤í—˜ ìš”ì•½ ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "\n",
    "# ë§ˆì§€ë§‰ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"status\": \"completed\",\n",
    "            \"completion_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"total_runtime_hours\": 0  # start_time ì†ì„± ë¬¸ì œë¡œ ì¼ë‹¨ 0ìœ¼ë¡œ ì„¤ì •\n",
    "        })\n",
    "        print(\"ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"í™œì„±í™”ëœ runì´ ì—†ì–´ ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "print(f\"\\nì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ì‹¤í—˜ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\" K-Fold CV ê²°ê³¼: {mean_f1:.4f} Â± {std_f1:.4f}\")\n",
    "print(f\" ìµœê³  ì„±ëŠ¥ Fold: {max(val_f1_scores):.4f}\")\n",
    "print(f\" ì•™ìƒë¸” ëª¨ë¸: {len(ensemble_models)}ê°œ\")\n",
    "print(f\" TTA ë³€í˜•: {len(essential_tta_transforms)}ê°œ\")\n",
    "print(f\" í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: {np.mean(confidences):.4f}\")\n",
    "print(f\" WandB ëŒ€ì‹œë³´ë“œ: {main_run.url}\")\n",
    "\n",
    "# Sample predictions ì¶œë ¥\n",
    "print(f\"\\n ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ:\")\n",
    "print(tta_pred_df.head(10))\n",
    "\n",
    "# ë©”ì¸ run ì¢…ë£Œ\n",
    "main_run.finish()\n",
    "\n",
    "print(f\"\\n ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
    "print(f\" ê²°ê³¼ íŒŒì¼: {output_path}\")\n",
    "print(f\" WandBì—ì„œ ì „ì²´ ì‹¤í—˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del ensemble_models\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4db3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Focal Loss í…ŒìŠ¤íŠ¸\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ” Focal Loss êµ¬í˜„ í…ŒìŠ¤íŠ¸...\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
    "batch_size = 4\n",
    "num_classes = 17\n",
    "test_logits = torch.randn(batch_size, num_classes)\n",
    "test_targets = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# Focal Lossì™€ CrossEntropyLoss ë¹„êµ\n",
    "focal_loss = FocalLoss(alpha=1.0, gamma=2.0)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "focal_loss_value = focal_loss(test_logits, test_targets)\n",
    "ce_loss_value = ce_loss(test_logits, test_targets)\n",
    "\n",
    "print(f\"âœ… Focal Loss í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"   Focal Loss: {focal_loss_value.item():.4f}\")\n",
    "print(f\"   CrossEntropy Loss: {ce_loss_value.item():.4f}\")\n",
    "print(f\"   Focal Lossê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\nğŸ“Š í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸...\")\n",
    "\n",
    "# ë¶ˆê· í˜•í•œ íƒ€ê²Ÿ ìƒì„± (ëŒ€ë¶€ë¶„ì´ í´ë˜ìŠ¤ 0)\n",
    "imbalanced_targets = torch.tensor([0, 0, 0, 1])  # 3ê°œëŠ” í´ë˜ìŠ¤ 0, 1ê°œëŠ” í´ë˜ìŠ¤ 1\n",
    "\n",
    "# ëª¨ë¸ì´ í´ë˜ìŠ¤ 0ì„ ì˜ ì˜ˆì¸¡í•˜ë„ë¡ ì„¤ì • (ë†’ì€ í™•ë¥ )\n",
    "easy_logits = torch.tensor([\n",
    "    [3.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # í´ë˜ìŠ¤ 0 ì˜ˆì¸¡\n",
    "    [2.5, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],  # í´ë˜ìŠ¤ 0 ì˜ˆì¸¡\n",
    "    [2.0, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],  # í´ë˜ìŠ¤ 0 ì˜ˆì¸¡\n",
    "    [0.1, 2.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],  # í´ë˜ìŠ¤ 1 ì˜ˆì¸¡ (ì–´ë ¤ìš´ ìƒ˜í”Œ)\n",
    "])\n",
    "\n",
    "focal_imbalanced = focal_loss(easy_logits, imbalanced_targets)\n",
    "ce_imbalanced = ce_loss(easy_logits, imbalanced_targets)\n",
    "\n",
    "print(f\"   ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ Focal Loss: {focal_imbalanced.item():.4f}\")\n",
    "print(f\"   ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ CrossEntropy Loss: {ce_imbalanced.item():.4f}\")\n",
    "print(f\"   Focal Lossê°€ ì–´ë ¤ìš´ ìƒ˜í”Œ(í´ë˜ìŠ¤ 1)ì— ë” ì§‘ì¤‘í•©ë‹ˆë‹¤!\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Focal Loss êµ¬í˜„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"   - alpha=1.0: ëª¨ë“  í´ë˜ìŠ¤ì— ê· ë“±í•œ ê°€ì¤‘ì¹˜\")\n",
    "print(f\"   - gamma=2.0: ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ëŠ” ì •ë„\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°ì— íš¨ê³¼ì ì…ë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07f9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
