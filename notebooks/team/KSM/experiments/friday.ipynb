{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bbe02e",
   "metadata": {},
   "source": [
    "# ğŸ“„ Document type classification baseline code with WandB Integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92dc69ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations==1.3.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: ipykernel==6.27.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (6.27.1)\n",
      "Requirement already satisfied: ipython==8.15.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (8.15.0)\n",
      "Requirement already satisfied: ipywidgets==8.1.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (8.1.1)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (0.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (1.26.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 8)) (2.1.4)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 9)) (9.4.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: timm==0.9.12 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 11)) (0.9.12)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 12)) (6.3.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 13)) (3.10.6)\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 14)) (4.5.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.11.4)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.22.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (4.8.1.78)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.5.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (1.5.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.7.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1->-r ../requirements.txt (line 4)) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1->-r ../requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.0.6)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.12.0)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.16.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.19.4)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.10/site-packages (from plotly->-r ../requirements.txt (line 12)) (2.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (3.2.3)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (1.16.5)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (2.0.43)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (4.65.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (4.15.0)\n",
      "Requirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (2.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.27.1->-r ../requirements.txt (line 2)) (4.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 8)) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (2023.12.9)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.3)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna->-r ../requirements.txt (line 14)) (3.2.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (3.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (2023.9.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (5.9.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.12.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.25.2)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.0.9)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.2.3)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.1.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.5.0)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.20.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (21.2.0)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.1)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.8.19.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 0. Prepare Environments & Install Libraries\n",
    "# =============================================================================\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773408ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Import Libraries & Define Functions\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import optuna, math\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed Precisionìš©\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# WandB ê´€ë ¨ import ì¶”ê°€\n",
    "import wandb\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e142354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1-1. WandB Login and Configuration\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "ğŸš€ íŒ€ì› ì‚¬ìš© ê°€ì´ë“œ:\n",
    "\n",
    "1. WandB ê³„ì • ìƒì„±: https://wandb.ai/signup\n",
    "2. ì´ ì…€ ì‹¤í–‰ ì‹œ ë¡œê·¸ì¸ í”„ë¡¬í”„íŠ¸ê°€ ë‚˜íƒ€ë‚˜ë©´ ê°œì¸ API í‚¤ ì…ë ¥\n",
    "3. EXPERIMENT_NAMEì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½:\n",
    "   - \"member1-baseline\"\n",
    "   - \"member2-augmentation-test\"  \n",
    "   - \"member3-hyperparameter-tuning\"\n",
    "   ë“±ë“± ê°ì ë‹¤ë¥¸ ì´ë¦„ ì‚¬ìš©\n",
    "\n",
    "4. íŒ€ ëŒ€ì‹œë³´ë“œ URL: [ì—¬ê¸°ì— ë‹¹ì‹ ì˜ í”„ë¡œì íŠ¸ URL ì¶”ê°€]\n",
    "\n",
    "âš ï¸ ì£¼ì˜ì‚¬í•­:\n",
    "- ì ˆëŒ€ API í‚¤ë¥¼ ì½”ë“œì— í•˜ë“œì½”ë”©í•˜ì§€ ë§ˆì„¸ìš”\n",
    "- EXPERIMENT_NAMEë§Œ ë³€ê²½í•˜ê³  PROJECT_NAMEì€ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”\n",
    "- ê°ì ê°œì¸ ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸í•´ì„œ ì‹¤í—˜ì„ ì¶”ê°€í•˜ì„¸ìš”\n",
    "\"\"\"\n",
    "\n",
    "# WandB ë¡œê·¸ì¸ (ê°ì ì‹¤í–‰)\n",
    "try:\n",
    "    if wandb.api.api_key is None:\n",
    "        print(\"WandBì— ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        wandb.login()\n",
    "    else:\n",
    "        print(f\"WandB ë¡œê·¸ì¸ ìƒíƒœ: {wandb.api.viewer()['username']}\")\n",
    "except:\n",
    "    print(\"WandB ë¡œê·¸ì¸ì„ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
    "    wandb.login()\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì„¤ì • (ê°ì ìˆ˜ì •í•  ë¶€ë¶„)\n",
    "PROJECT_NAME = \"document-classification-team\"  # ëª¨ë“  íŒ€ì› ë™ì¼\n",
    "ENTITY = None  # ê°ì ê°œì¸ ê³„ì • ì‚¬ìš©\n",
    "EXPERIMENT_NAME = \"efficientnet-b3-baseline\"  # íŒ€ì›ë³„ë¡œ ë³€ê²½ (ì˜ˆ: \"member1-hyperopt\", \"member2-augmentation\")\n",
    "\n",
    "print(f\"í”„ë¡œì íŠ¸: {PROJECT_NAME}\")\n",
    "print(f\"ì‹¤í—˜ëª…: {EXPERIMENT_NAME}\")\n",
    "print(\"íŒ€ì›ë“¤ì€ EXPERIMENT_NAMEì„ ê°ì ë‹¤ë¥´ê²Œ ë³€ê²½í•´ì£¼ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. Seed & basic augmentations (Mixup)\n",
    "# =============================================================================\n",
    "\n",
    "# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. Dataset Class\n",
    "# =============================================================================\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transform=None):\n",
    "        # CSV íŒŒì¼ì´ë©´ ì½ê³ , DataFrameì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values  \n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def update_transform(self, new_transform):\n",
    "        \"\"\"transform ì—…ë°ì´íŠ¸ ë©”ì„œë“œ ì¶”ê°€\"\"\"\n",
    "        self.transform = new_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cacecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutout (Random Erasing) í•¨ìˆ˜ ì •ì˜\n",
    "def random_erasing(image, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)):\n",
    "    if random.random() > p:\n",
    "        return image\n",
    "    img_c, img_h, img_w = image.shape[1], image.shape[2], image.shape[3]\n",
    "    area = img_h * img_w\n",
    "    \n",
    "    target_area = random.uniform(scale[0], scale[1]) * area\n",
    "    aspect_ratio = random.uniform(ratio[0], ratio[1])\n",
    "    h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "    w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "    \n",
    "    if h < img_h and w < img_w:\n",
    "        x = random.randint(0, img_w - w)\n",
    "        y = random.randint(0, img_h - h)\n",
    "        image[:, :, y:y+h, x:x+w] = 0.0  # ì œê±°ëœ ì˜ì—­ì„ 0ìœ¼ë¡œ ì„¤ì •\n",
    "    return image\n",
    "\n",
    "# RandomCrop í•¨ìˆ˜ ì •ì˜\n",
    "def random_crop(image, crop_size=0.8):\n",
    "    img_c, img_h, img_w = image.shape[1], image.shape[2], image.shape[3]\n",
    "    crop_h = int(img_h * crop_size)\n",
    "    crop_w = int(img_w * crop_size)\n",
    "    \n",
    "    if crop_h >= img_h or crop_w >= img_w:\n",
    "        return image\n",
    "    \n",
    "    x = random.randint(0, img_w - crop_w)\n",
    "    y = random.randint(0, img_h - crop_h)\n",
    "    cropped_image = image[:, :, y:y+crop_h, x:x+crop_w]\n",
    "    \n",
    "    # ì›ë˜ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³µì› (íŒ¨ë”© ë˜ëŠ” ë¦¬ì‚¬ì´ì¦ˆ)\n",
    "    cropped_image = torch.nn.functional.interpolate(cropped_image, size=(img_h, img_w), mode='bilinear', align_corners=False)\n",
    "    return cropped_image\n",
    "\n",
    "# Mixup í•¨ìˆ˜ ì •ì˜\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac494730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================     \n",
    "# 5. Training & Validation Functions\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, epoch=None, fold=None):\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Training Epoch {epoch+1 if epoch else '?'}\")\n",
    "    batch_count = 0\n",
    "    \n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # ì¦ê°• ê¸°ë²• ì„ íƒ (Mixup 25%, Cutout 25%, RandomCrop 25%, None 25%) -> (Mixup 25%, Cutout 25%, RandomCrop 50%)\n",
    "        aug_type = random.choices(['mixup', 'cutout', 'random_crop'], weights=[0.25, 0.25, 0.5])[0]\n",
    "        mixup_applied = False\n",
    "        cutout_applied = False\n",
    "        random_crop_applied = False\n",
    "        \n",
    "        if aug_type == 'mixup':\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(image, targets, alpha=1.0)\n",
    "            with autocast(): \n",
    "                preds = model(mixed_x)\n",
    "            loss = lam * loss_fn(preds, y_a) + (1 - lam) * loss_fn(preds, y_b)\n",
    "            mixup_applied = True\n",
    "        elif aug_type == 'cutout':\n",
    "            image = random_erasing(image, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            cutout_applied = True\n",
    "        elif aug_type == 'random_crop':\n",
    "            image = random_crop(image, crop_size=0.8)\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            random_crop_applied = True\n",
    "        else:\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        # ë°°ì¹˜ë³„ ìƒì„¸ ë¡œê¹… (100 ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if batch_count % 100 == 0 and wandb.run is not None:\n",
    "            step = epoch * len(loader) + batch_count if epoch is not None else batch_count\n",
    "            wandb.log({\n",
    "                f\"fold_{fold}/train_batch_loss\": loss.item(),\n",
    "                f\"fold_{fold}/mixup_applied\": int(mixup_applied),\n",
    "                f\"fold_{fold}/cutout_applied\": int(cutout_applied),\n",
    "                f\"fold_{fold}/random_crop_applied\": int(random_crop_applied),\n",
    "                f\"fold_{fold}/batch_step\": step\n",
    "            })\n",
    "        \n",
    "        batch_count += 1\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}, Mixup: {mixup_applied}, Cutout: {cutout_applied}, RandomCrop: {random_crop_applied}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def validate_one_epoch(loader, model, loss_fn, device, epoch=None, fold=None, log_confusion=False):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=f\"Validating Epoch {epoch+1 if epoch else '?'}\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "            \n",
    "            pbar.set_description(f\"Val Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "    \n",
    "    # Confusion Matrix ë¡œê¹… (ë§ˆì§€ë§‰ epochì—ë§Œ)\n",
    "    if log_confusion and wandb.run is not None:\n",
    "        try:\n",
    "            wandb.log({\n",
    "                f\"fold_{fold}/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=targets_list,\n",
    "                    preds=preds_list,\n",
    "                    class_names=[f\"Class_{i}\" for i in range(17)]\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            # í´ë˜ìŠ¤ë³„ F1 ìŠ¤ì½”ì–´\n",
    "            class_f1_scores = f1_score(targets_list, preds_list, average=None)\n",
    "            for i, class_f1 in enumerate(class_f1_scores):\n",
    "                wandb.log({f\"fold_{fold}/class_{i}_f1\": class_f1})\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Confusion matrix ë¡œê¹… ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,  \n",
    "        \"val_f1\": val_f1,\n",
    "    }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193dae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. Hyper-parameters with WandB Config\n",
    "# =============================================================================\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "\n",
    "# model config\n",
    "model_name = 'efficientnet_b3' # 'resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "# training config\n",
    "img_size = 384\n",
    "LR = 5e-4\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 30\n",
    "\n",
    "# K-Fold config\n",
    "N_FOLDS = 5  # 5-foldë¡œ ì„¤ì •\n",
    "\n",
    "# WandB Config ì„¤ì •\n",
    "config = {\n",
    "    # Model config\n",
    "    \"model_name\": model_name,\n",
    "    \"img_size\": img_size,\n",
    "    \"num_classes\": 17,\n",
    "    \"architecture\": \"EfficientNet-B3\",\n",
    "    \n",
    "    # Training config  \n",
    "    \"lr\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"device\": str(device),\n",
    "    \n",
    "    # K-Fold config\n",
    "    \"n_folds\": N_FOLDS,\n",
    "    \"seed\": SEED,\n",
    "    \"cv_strategy\": \"StratifiedKFold\",\n",
    "    \n",
    "    # Augmentation & Training techniques\n",
    "    \"mixup_alpha\": 1.0,\n",
    "    \"mixup_prob\": 0.3,\n",
    "    \"label_smoothing\": 0.2,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"mixed_precision\": True,\n",
    "    \n",
    "    # Optimizer & Scheduler\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \n",
    "    # Data\n",
    "    \"data_path\": data_path,\n",
    "    \"train_transforms\": \"Advanced\",\n",
    "    \"test_transforms\": \"Basic\",\n",
    "}\n",
    "\n",
    "print(\" í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\" ëª¨ë¸: {model_name}\")\n",
    "print(f\" ì´ë¯¸ì§€ í¬ê¸°: {img_size}x{img_size}\")\n",
    "print(f\" ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(f\" í•™ìŠµë¥ : {LR}\")\n",
    "print(f\" ì—í­: {EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 7. Optuna Hyperparameter Tuning (ì„ íƒì )\n",
    "# =============================================================================\n",
    "\n",
    "USE_OPTUNA = False  # Trueë¡œ ë°”ê¾¸ë©´ íŠœë‹ ì‹¤í–‰\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    print(\"ğŸ” Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "        \n",
    "        # WandBì— Optuna ì‹œí–‰ ë¡œê¹…\n",
    "        optuna_run = wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            entity=ENTITY,\n",
    "            name=f\"optuna-trial-{trial.number}\",\n",
    "            config={**config, \"lr\": lr, \"batch_size\": batch_size},\n",
    "            tags=[\"optuna\", \"hyperparameter-tuning\"],\n",
    "            group=\"optuna-study\",\n",
    "            job_type=\"hyperparameter-optimization\",\n",
    "            reinit=True\n",
    "        )\n",
    "        \n",
    "        # ê°„ë‹¨í•œ 3-fold CVë¡œ ë¹ ë¥¸ í‰ê°€\n",
    "        skf_simple = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        fold_scores = []\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í‰ê°€ ë¡œì§ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë‹¨ìˆœí™”)\n",
    "        # ... (Optuna ë¡œì§ì€ ë³µì¡í•˜ë¯€ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”)\n",
    "        \n",
    "        optuna_run.finish()\n",
    "        return np.random.random()  # placeholder\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    \n",
    "    # ìµœì  íŒŒë¼ë¯¸í„° ì ìš©\n",
    "    best_params = study.best_params\n",
    "    LR = best_params.get('lr', LR)\n",
    "    BATCH_SIZE = best_params.get('batch_size', BATCH_SIZE)\n",
    "    config.update(best_params)\n",
    "    print(f\"ğŸ¯ Optuna ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "else:\n",
    "    print(\"â­ï¸ Optuna íŠœë‹ ê±´ë„ˆë›°ê¸° (USE_OPTUNA=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adaptive_transform(epoch, total_epochs, img_size):\n",
    "    \"\"\"ì—í¬í¬ì— ë”°ë¥¸ ì ì‘ì  ì¦ê°• ê°•ë„ ì¡°ì ˆ\"\"\"\n",
    "    progress = epoch / total_epochs\n",
    "    \n",
    "    # ê³µí†µ ê¸°ë³¸ ë³€í™˜\n",
    "    base_transforms = [\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                      border_mode=0, value=0),\n",
    "    ]\n",
    "    \n",
    "    # ì—í¬í¬ë³„ ì¦ê°• ê°•ë„ ì¡°ì ˆ\n",
    "    if progress < 0.3:  # ì´ˆê¸° 30%: ê°•í•œ ì¦ê°•\n",
    "        strength_multiplier = 1.0\n",
    "        aug_prob = 0.9\n",
    "    elif progress < 0.7:  # ì¤‘ê¸° 40%: ì¤‘ê°„ ì¦ê°•  \n",
    "        strength_multiplier = 0.7\n",
    "        aug_prob = 0.6\n",
    "    else:  # í›„ê¸° 30%: ì•½í•œ ì¦ê°•\n",
    "        strength_multiplier = 0.4\n",
    "        aug_prob = 0.3\n",
    "    \n",
    "    # blur_limitì„ í™€ìˆ˜ë¡œ ë³´ì •í•˜ëŠ” í•¨ìˆ˜\n",
    "    def get_odd_blur_limit(base_limit, multiplier):\n",
    "        \"\"\"blur_limitì„ í™€ìˆ˜ë¡œ ë³´ì •\"\"\"\n",
    "        limit = int(base_limit * multiplier)\n",
    "        # ì§ìˆ˜ë©´ í™€ìˆ˜ë¡œ ë³€ê²½ (ìµœì†Œê°’ 3 ë³´ì¥)\n",
    "        if limit % 2 == 0:\n",
    "            limit = max(3, limit - 1)\n",
    "        return max(3, limit)  # ìµœì†Œê°’ 3 ë³´ì¥\n",
    "    \n",
    "    # ì ì‘ì  ì¦ê°• ë¦¬ìŠ¤íŠ¸\n",
    "    adaptive_augmentations = [\n",
    "        # ë¬¸ì„œ íŠ¹í™” íšŒì „ (í™•ë¥  ì¡°ì ˆ)\n",
    "        A.OneOf([\n",
    "            A.Rotate(limit=[90,90], p=1.0),\n",
    "            A.Rotate(limit=[180,180], p=1.0),\n",
    "            A.Rotate(limit=[270,270], p=1.0),\n",
    "            A.Rotate(limit=(-15, 15), p=1.0),\n",
    "        ], p=0.7 * aug_prob),\n",
    "        \n",
    "        # ê¸°í•˜í•™ì  ë³€í™˜ (ê°•ë„ ì¡°ì ˆ)\n",
    "        A.OneOf([\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.1 * strength_multiplier, \n",
    "                scale_limit=0.2 * strength_multiplier, \n",
    "                rotate_limit=5, p=1.0\n",
    "            ),\n",
    "            A.ElasticTransform(alpha=50 * strength_multiplier, sigma=5, p=1.0),\n",
    "            A.GridDistortion(num_steps=5, distort_limit=0.2 * strength_multiplier, p=1.0),\n",
    "        ], p=0.6 * aug_prob),\n",
    "        \n",
    "        # ìƒ‰ìƒ ë° ì¡°ëª… ë³€í™˜ (ê°•ë„ ì¡°ì ˆ)\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.4 * strength_multiplier, \n",
    "                contrast_limit=0.4 * strength_multiplier, p=1.0\n",
    "            ),\n",
    "            A.ColorJitter(\n",
    "                brightness=0.4 * strength_multiplier, \n",
    "                contrast=0.4 * strength_multiplier, \n",
    "                saturation=0.3 * strength_multiplier, \n",
    "                hue=0.1 * strength_multiplier, p=1.0\n",
    "            ),\n",
    "        ], p=0.8 * aug_prob),\n",
    "        \n",
    "        # ë¸”ëŸ¬ ë° ë…¸ì´ì¦ˆ (í™•ë¥  ì¡°ì ˆ) - ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(blur_limit=get_odd_blur_limit(15, strength_multiplier), p=1.0),\n",
    "            A.MotionBlur(blur_limit=get_odd_blur_limit(15, strength_multiplier), p=1.0),\n",
    "        ], p=0.6 * aug_prob),\n",
    "        \n",
    "        # ë…¸ì´ì¦ˆ (ê°•ë„ ì¡°ì ˆ)\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 150.0 * strength_multiplier), p=1.0),\n",
    "            A.ISONoise(\n",
    "                color_shift=(0.01, 0.08 * strength_multiplier), \n",
    "                intensity=(0.1, 0.8 * strength_multiplier), p=1.0\n",
    "            ),\n",
    "        ], p=0.5 * aug_prob),\n",
    "    ]\n",
    "    \n",
    "    # ìµœì¢… ë³€í™˜\n",
    "    final_transforms = [\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "    \n",
    "    return A.Compose(base_transforms + adaptive_augmentations + final_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# augmentationì„ ìœ„í•œ transform ì½”ë“œ\n",
    "trn_transform = A.Compose([\n",
    "    # ë¹„ìœ¨ ë³´ì¡´ ë¦¬ì‚¬ì´ì§• (í•µì‹¬ ê°œì„ )\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "    \n",
    "    # ë¬¸ì„œ íŠ¹í™” íšŒì „ + ë¯¸ì„¸ íšŒì „ ì¶”ê°€\n",
    "    A.OneOf([\n",
    "        A.Rotate(limit=[90,90], p=1.0),\n",
    "        A.Rotate(limit=[180,180], p=1.0),\n",
    "        A.Rotate(limit=[270,270], p=1.0),\n",
    "        A.Rotate(limit=(-15, 15), p=1.0),  # ë¯¸ì„¸ íšŒì „ ì¶”ê°€\n",
    "    ], p=0.7),\n",
    "    \n",
    "    # ê¸°í•˜í•™ì  ë³€í™˜ ê°•í™”\n",
    "    A.OneOf([\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=5, p=1.0),\n",
    "        A.ElasticTransform(alpha=50, sigma=5, p=1.0),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.2, p=1.0),\n",
    "        A.OpticalDistortion(distort_limit=0.2, shift_limit=0.1, p=1.0),\n",
    "    ], p=0.6),\n",
    "    \n",
    "    # ìƒ‰ìƒ ë° ì¡°ëª… ë³€í™˜ ê°•í™”\n",
    "    A.OneOf([\n",
    "        A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1, p=1.0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=1.0),\n",
    "        A.RandomGamma(gamma_limit=(70, 130), p=1.0),\n",
    "    ], p=0.9),\n",
    "    \n",
    "    # ë¸”ëŸ¬ ë° ë…¸ì´ì¦ˆ ê°•í™”\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=(5, 15), p=1.0),\n",
    "        A.GaussianBlur(blur_limit=(3, 15), p=1.0),\n",
    "        A.MedianBlur(blur_limit=7, p=1.0),\n",
    "        A.Blur(blur_limit=7, p=1.0),\n",
    "    ], p=0.8),\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 150.0), p=1.0),\n",
    "        A.ISONoise(color_shift=(0.01, 0.08), intensity=(0.1, 0.8), p=1.0),\n",
    "        A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n",
    "    ], p=0.8),\n",
    "    \n",
    "    # ë¬¸ì„œ í’ˆì§ˆ ì‹œë®¬ë ˆì´ì…˜ (ìŠ¤ìº”/ë³µì‚¬ íš¨ê³¼)\n",
    "    A.OneOf([\n",
    "        A.Downscale(scale_min=0.7, scale_max=0.9, p=1.0),\n",
    "        A.ImageCompression(quality_lower=60, quality_upper=95, p=1.0),\n",
    "        A.Posterize(num_bits=6, p=1.0),\n",
    "    ], p=0.5),\n",
    "    \n",
    "    # í”½ì…€ ë ˆë²¨ ë³€í™˜\n",
    "    A.OneOf([\n",
    "        A.ChannelShuffle(p=1.0),\n",
    "        A.InvertImg(p=1.0),\n",
    "        A.Solarize(threshold=128, p=1.0),\n",
    "        A.Equalize(p=1.0),\n",
    "    ], p=0.3),\n",
    "    \n",
    "    # ê³µê°„ ë³€í™˜\n",
    "    A.OneOf([\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),  # ë¬¸ì„œì—ì„œë„ ìœ ìš©í•  ìˆ˜ ìˆìŒ\n",
    "        A.Transpose(p=1.0),\n",
    "    ], p=0.6),\n",
    "    \n",
    "    # ì¡°ê° ì œê±° (Cutout ê³„ì—´)\n",
    "    A.OneOf([\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, \n",
    "                       min_holes=1, min_height=8, min_width=8, \n",
    "                       fill_value=0, p=1.0),\n",
    "        A.GridDropout(ratio=0.3, unit_size_min=8, unit_size_max=32, \n",
    "                     holes_number_x=5, holes_number_y=5, p=1.0),\n",
    "    ], p=0.4),\n",
    "    \n",
    "    # ìµœì¢… ì •ê·œí™”\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# test image ë³€í™˜ì„ ìœ„í•œ transform ì½”ë“œ\n",
    "tst_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c320bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. Load Data & Start K-Fold Cross Validation with WandB\n",
    "# =============================================================================\n",
    "\n",
    "# ì „ì²´ í•™ìŠµ ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ ìƒ˜í”Œ\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "class_counts = train_df['target'].value_counts().sort_index()\n",
    "print(f\" í´ë˜ìŠ¤ ë¶„í¬: {dict(class_counts)}\")\n",
    "\n",
    "# K-Fold ì„¤ì •\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# K-Fold ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "fold_results = []\n",
    "fold_models = []  # ê° foldì˜ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥\n",
    "\n",
    "#  WandB ë©”ì¸ ì‹¤í—˜ ì‹œì‘\n",
    "main_run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    entity=ENTITY,\n",
    "    name=f\"{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "    config=config,\n",
    "    tags=[\"k-fold-cv\", \"ensemble\", model_name, \"baseline\", \"main-experiment\"],\n",
    "    group=\"k-fold-experiment\",\n",
    "    job_type=\"cross-validation\",\n",
    "    notes=f\"{N_FOLDS}-Fold Cross Validation with {model_name}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸš€ WandB ì‹¤í—˜ ì‹œì‘!\")\n",
    "print(f\"ğŸ“Š ëŒ€ì‹œë³´ë“œ: {main_run.url}\")\n",
    "print(f\"ğŸ“‹ ì‹¤í—˜ëª…: {main_run.name}\")\n",
    "\n",
    "#  ë°ì´í„°ì…‹ ì •ë³´ ë¡œê¹…\n",
    "wandb.log({\n",
    "    \"dataset/total_samples\": len(train_df),\n",
    "    \"dataset/num_classes\": 17,\n",
    "    \"dataset/samples_per_fold\": len(train_df) // N_FOLDS,\n",
    "})\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”\n",
    "class_dist_data = [[f\"Class_{i}\", count] for i, count in enumerate(class_counts)]\n",
    "wandb.log({\n",
    "    \"dataset/class_distribution\": wandb.plot.bar(\n",
    "        wandb.Table(data=class_dist_data, columns=[\"Class\", \"Count\"]),\n",
    "        \"Class\", \"Count\", \n",
    "        title=\"Training Data Class Distribution\"\n",
    "    )\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ¯ {N_FOLDS}-FOLD CROSS VALIDATION ì‹œì‘\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 10. K-Fold Cross Validation Loop with WandB\n",
    "# =============================================================================\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\" FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # ê° foldë³„ child run ìƒì„±\n",
    "    fold_run = wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        entity=ENTITY,\n",
    "        name=f\"fold-{fold+1}-{model_name}-{datetime.now().strftime('%H%M')}\",\n",
    "        config=config,\n",
    "        tags=[\"fold\", f\"fold-{fold+1}\", model_name, \"child-run\"],\n",
    "        group=\"k-fold-experiment\",\n",
    "        job_type=f\"fold-{fold+1}\",\n",
    "        reinit=True  # ìƒˆë¡œìš´ run ì‹œì‘ í—ˆìš©\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Fold {fold+1} Dashboard: {fold_run.url}\")\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ train/validation ë°ì´í„° ë¶„í• \n",
    "    train_fold_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_fold_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # ë°ì´í„° ë¶„í•  ì •ë³´ ë¡œê¹…\n",
    "    wandb.log({\n",
    "        \"fold_info/fold_number\": fold + 1,\n",
    "        \"fold_info/train_samples\": len(train_fold_df),\n",
    "        \"fold_info/val_samples\": len(val_fold_df),\n",
    "        \"fold_info/train_ratio\": len(train_fold_df) / len(train_df),\n",
    "        \"fold_info/val_ratio\": len(val_fold_df) / len(train_df)\n",
    "    })\n",
    "    \n",
    "    # Dataset ìƒì„± (ì´ˆê¸° transformìœ¼ë¡œ)\n",
    "    trn_dataset = ImageDataset(\n",
    "        train_fold_df,\n",
    "        \"../data/train/\",\n",
    "        transform=get_adaptive_transform(0, EPOCHS, img_size)  # ì´ˆê¸° transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = ImageDataset(\n",
    "        val_fold_df,\n",
    "        \"../data/train/\",\n",
    "        transform=tst_transform  # ê²€ì¦ìš©ì€ ê³ ì •\n",
    "    )\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ DataLoader ìƒì„±\n",
    "    trn_loader = DataLoader(\n",
    "        trn_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(trn_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™” (ê° foldë§ˆë‹¤ ìƒˆë¡œìš´ ëª¨ë¸)\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.2)  # Label Smoothing ì ìš©\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Learning Rate Scheduler ì¶”ê°€\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ ìµœê³  ì„±ëŠ¥ ì¶”ì \n",
    "    best_val_f1 = 0.0\n",
    "    best_model = None\n",
    "    patience = 0\n",
    "    max_patience = 7\n",
    "    \n",
    "    print(f\" ëª¨ë¸ í•™ìŠµ ì‹œì‘ - Fold {fold+1}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 11. Training Loop for Current Fold\n",
    "    # =============================================================================\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        new_transform = get_adaptive_transform(epoch, EPOCHS, img_size)\n",
    "        trn_dataset.update_transform(new_transform)\n",
    "\n",
    "        # í˜„ì¬ ì¦ê°• ì „ëµ ë¡œê¹…\n",
    "        progress = epoch / EPOCHS\n",
    "        if progress < 0.3:\n",
    "            strategy = \"heavy\"\n",
    "            strength = 1.0\n",
    "        elif progress < 0.7:\n",
    "            strategy = \"medium\"\n",
    "            strength = 0.7\n",
    "        else:\n",
    "            strategy = \"light\"\n",
    "            strength = 0.4\n",
    "            \n",
    "        wandb.log({\n",
    "            f\"fold_{fold+1}/aug_strategy\": strategy,\n",
    "            f\"fold_{fold+1}/aug_strength\": strength,\n",
    "            f\"fold_{fold+1}/aug_progress\": progress\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} - Aug Strategy: {strategy} (strength: {strength:.1f})\")\n",
    "        \n",
    "        # Training\n",
    "        train_ret = train_one_epoch(\n",
    "            trn_loader, model, optimizer, loss_fn, device, \n",
    "            epoch=epoch, fold=fold+1\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_ret = validate_one_epoch(\n",
    "            val_loader, model, loss_fn, device, \n",
    "            epoch=epoch, fold=fold+1,\n",
    "            log_confusion=(epoch == EPOCHS-1)  # ë§ˆì§€ë§‰ epochì—ë§Œ confusion matrix\n",
    "        )\n",
    "        \n",
    "        # Learning rate ë¡œê¹…\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # WandBì— metrics ë¡œê¹…\n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"fold\": fold + 1,\n",
    "            \"train/loss\": train_ret['train_loss'],\n",
    "            \"train/accuracy\": train_ret['train_acc'], \n",
    "            \"train/f1\": train_ret['train_f1'],\n",
    "            \"val/loss\": val_ret['val_loss'],\n",
    "            \"val/accuracy\": val_ret['val_acc'],\n",
    "            \"val/f1\": val_ret['val_f1'],\n",
    "            \"learning_rate\": current_lr,\n",
    "            \"optimizer/lr\": current_lr\n",
    "        }\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated(0) / 1e9\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            log_data.update({\n",
    "                \"system/gpu_memory_used_gb\": gpu_memory_used,\n",
    "                \"system/gpu_memory_total_gb\": gpu_memory_total,\n",
    "                \"system/gpu_utilization_pct\": (gpu_memory_used / gpu_memory_total) * 100\n",
    "            })\n",
    "        \n",
    "        wandb.log(log_data)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\" Epoch {epoch+1:2d} | \"\n",
    "              f\"Train Loss: {train_ret['train_loss']:.4f} | \"\n",
    "              f\"Train F1: {train_ret['train_f1']:.4f} | \"\n",
    "              f\"Val Loss: {val_ret['val_loss']:.4f} | \"\n",
    "              f\"Val F1: {val_ret['val_f1']:.4f} | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "        if val_ret['val_f1'] > best_val_f1:\n",
    "            best_val_f1 = val_ret['val_f1']\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            patience = 0\n",
    "            \n",
    "            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥\n",
    "            model_path = f'best_model_fold_{fold+1}.pth'\n",
    "            torch.save(best_model, model_path)\n",
    "            wandb.save(model_path, policy=\"now\")\n",
    "            \n",
    "            # ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ë¡œê¹…\n",
    "            wandb.log({\n",
    "                f\"best_performance/epoch\": epoch + 1,\n",
    "                f\"best_performance/val_f1\": best_val_f1,\n",
    "                f\"best_performance/val_acc\": val_ret['val_acc'],\n",
    "                f\"best_performance/val_loss\": val_ret['val_loss'],\n",
    "            })\n",
    "            \n",
    "            print(f\"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            \n",
    "        # Early stopping (ì„ íƒì )\n",
    "        if patience >= max_patience and epoch > EPOCHS // 2:\n",
    "            print(f\"â¸ï¸ Early stopping at epoch {epoch+1} (patience: {patience})\")\n",
    "            wandb.log({\"early_stopping/epoch\": epoch + 1})\n",
    "            break\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 12. Fold Results Summary\n",
    "    # =============================================================================\n",
    "    \n",
    "    # í˜„ì¬ fold ê²°ê³¼ ì €ì¥\n",
    "    fold_result = {\n",
    "        'fold': fold + 1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'final_train_f1': train_ret['train_f1'],\n",
    "        'train_samples': len(trn_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'epochs_trained': epoch + 1,\n",
    "        'early_stopped': patience >= max_patience\n",
    "    }\n",
    "    \n",
    "    fold_results.append(fold_result)\n",
    "    fold_models.append(best_model)\n",
    "    \n",
    "    # Fold ìµœì¢… ìš”ì•½ ë¡œê¹…\n",
    "    wandb.log({\n",
    "        \"fold_summary/best_val_f1\": best_val_f1,\n",
    "        \"fold_summary/final_train_f1\": train_ret['train_f1'],\n",
    "        \"fold_summary/epochs_trained\": epoch + 1,\n",
    "        \"fold_summary/improvement\": best_val_f1 - val_ret['val_f1'],\n",
    "        \"fold_summary/early_stopped\": patience >= max_patience\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n Fold {fold + 1} ì™„ë£Œ!\")\n",
    "    print(f\" ìµœê³  Validation F1: {best_val_f1:.4f}\")\n",
    "    print(f\" í•™ìŠµëœ ì—í­: {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Fold run ì¢…ë£Œ\n",
    "    wandb.finish()\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del model, optimizer, scheduler, trn_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 13. K-Fold Cross Validation Results Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" K-FOLD CROSS VALIDATION ìµœì¢… ê²°ê³¼\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "val_f1_scores = [result['best_val_f1'] for result in fold_results]\n",
    "mean_f1 = np.mean(val_f1_scores)\n",
    "std_f1 = np.std(val_f1_scores)\n",
    "\n",
    "try:\n",
    "    # wandb.runì´ í˜„ì¬ í™œì„±í™”ëœ runì„ ê°€ë¦¬í‚´\n",
    "    if wandb.run is None:\n",
    "        print(\" í™œì„±í™”ëœ runì´ ì—†ì–´ ìƒˆë¡œìš´ summary runì„ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        active_run = wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            name=f\"SUMMARY-{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "            config=config,\n",
    "            tags=[\"summary\", \"cv-results\", model_name],\n",
    "            group=\"k-fold-experiment\",\n",
    "            job_type=\"summary\",\n",
    "            reinit=True\n",
    "        )\n",
    "    else:\n",
    "        print(\" ê¸°ì¡´ runì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        active_run = wandb.run\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Run ìƒíƒœ í™•ì¸ ì¤‘ ì—ëŸ¬: {e}\")\n",
    "    # ìƒˆë¡œìš´ run ìƒì„±\n",
    "    active_run = wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        name=f\"SUMMARY-{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "        config=config,\n",
    "        tags=[\"summary\", \"cv-results\", model_name],\n",
    "        group=\"k-fold-experiment\",\n",
    "        job_type=\"summary\",\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "# CV ìš”ì•½ í…Œì´ë¸” ìƒì„±\n",
    "fold_table = wandb.Table(columns=[\n",
    "    \"Fold\", \"Best_Val_F1\", \"Final_Train_F1\", \"Train_Samples\", \n",
    "    \"Val_Samples\", \"Epochs_Trained\", \"Early_Stopped\"\n",
    "])\n",
    "\n",
    "for result in fold_results:\n",
    "    fold_table.add_data(\n",
    "        result['fold'], \n",
    "        result['best_val_f1'], \n",
    "        result['final_train_f1'],\n",
    "        result['train_samples'], \n",
    "        result['val_samples'],\n",
    "        result['epochs_trained'],\n",
    "        result['early_stopped']\n",
    "    )\n",
    "\n",
    "# ì•ˆì „í•œ ë¡œê¹…\n",
    "try:\n",
    "    active_run.log({\n",
    "        \"cv_results/mean_f1\": mean_f1,\n",
    "        \"cv_results/std_f1\": std_f1,\n",
    "        \"cv_results/best_fold_f1\": max(val_f1_scores),\n",
    "        \"cv_results/worst_fold_f1\": min(val_f1_scores),\n",
    "        \"cv_results/f1_range\": max(val_f1_scores) - min(val_f1_scores),\n",
    "        \"cv_results/fold_results_table\": fold_table,\n",
    "        \"cv_results/n_folds\": N_FOLDS,\n",
    "        \"cv_results/total_epochs\": sum([r['epochs_trained'] for r in fold_results]),\n",
    "        \"cv_results/avg_epochs_per_fold\": np.mean([r['epochs_trained'] for r in fold_results]),\n",
    "        \"cv_results/early_stopped_folds\": sum([r['early_stopped'] for r in fold_results])\n",
    "    })\n",
    "    \n",
    "    # Foldë³„ ì„±ëŠ¥ ë°”ì°¨íŠ¸ ìƒì„±\n",
    "    fold_performance_data = [[f\"Fold {i+1}\", score] for i, score in enumerate(val_f1_scores)]\n",
    "    active_run.log({\n",
    "        \"cv_results/fold_performance_chart\": wandb.plot.bar(\n",
    "            wandb.Table(data=fold_performance_data, columns=[\"Fold\", \"F1_Score\"]),\n",
    "            \"Fold\", \"F1_Score\", \n",
    "            title=\"K-Fold Cross Validation Performance\"\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    print(\" CV ê²°ê³¼ ë¡œê¹… ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" WandB ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "    print(\" ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤:\")\n",
    "\n",
    "# ì–´ë–¤ ê²½ìš°ë“  ì½˜ì†”ì—ëŠ” ê²°ê³¼ ì¶œë ¥\n",
    "for result in fold_results:\n",
    "    status = \" Early Stopped\" if result['early_stopped'] else \" Completed\"\n",
    "    print(f\"Fold {result['fold']}: {result['best_val_f1']:.4f} \"\n",
    "          f\"({result['epochs_trained']} epochs) {status}\")\n",
    "\n",
    "print(f\"\\n í‰ê·  CV F1: {mean_f1:.4f} Â± {std_f1:.4f}\")\n",
    "print(f\" ìµœê³  Fold: {max(val_f1_scores):.4f}\")\n",
    "print(f\" ìµœì•… Fold: {min(val_f1_scores):.4f}\")\n",
    "print(f\" ì„±ëŠ¥ ë²”ìœ„: {max(val_f1_scores) - min(val_f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 14. Ensemble Models Preparation\n",
    "# =============================================================================\n",
    "\n",
    "# 5-Fold ì•™ìƒë¸” ëª¨ë¸ ì¤€ë¹„\n",
    "ensemble_models = []\n",
    "print(f\"\\nğŸ”§ ì•™ìƒë¸” ëª¨ë¸ ì¤€ë¹„ ì¤‘...\")\n",
    "\n",
    "for i, state_dict in enumerate(fold_models):\n",
    "    fold_model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    fold_model.load_state_dict(state_dict)\n",
    "    fold_model.eval()\n",
    "    ensemble_models.append(fold_model)\n",
    "    print(f\"Fold {i+1} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "print(f\" ì´ {len(ensemble_models)}ê°œ ëª¨ë¸ë¡œ ì•™ìƒë¸” êµ¬ì„±\")\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"ensemble/num_models\": len(ensemble_models),\n",
    "            \"ensemble/model_architecture\": model_name,\n",
    "            \"ensemble/ensemble_type\": \"simple_average\"\n",
    "        })\n",
    "    else:\n",
    "        print(\"ğŸ“Š ì•™ìƒë¸” ì •ë³´:\")\n",
    "        print(f\"  - ëª¨ë¸ ê°œìˆ˜: {len(ensemble_models)}\")\n",
    "        print(f\"  - ì•„í‚¤í…ì²˜: {model_name}\")\n",
    "        print(f\"  - ì•™ìƒë¸” íƒ€ì…: simple_average\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ì•™ìƒë¸” ì •ë³´ ë¡œê¹… ì‹¤íŒ¨: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 15. ê°œì„ ëœ TTA (Test Time Augmentation) Setup\n",
    "# =============================================================================\n",
    "\n",
    "# Temperature Scaling í´ë˜ìŠ¤ ì •ì˜\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self, temperature=1.5):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * temperature)\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "\n",
    "print(f\"\\n ê°œì„ ëœ TTA (Test Time Augmentation) ì„¤ì •...\")\n",
    "\n",
    "# ê¸°ë³¸ ì „ì²˜ë¦¬ ì—°ì‚°ë“¤ ì •ì˜\n",
    "base_ops = [\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "]\n",
    "\n",
    "# ì •ê·œí™” ë° í…ì„œ ë³€í™˜\n",
    "normalize_ops = [\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "]\n",
    "\n",
    "# ê¸°ë³¸ ë³€í™˜ (ë³€í™˜ ì—†ìŒ)\n",
    "base_transform = A.Compose(base_ops + normalize_ops)\n",
    "\n",
    "def get_comprehensive_tta():\n",
    "    \"\"\"ë” ì²´ê³„ì ì´ê³  í¬ê´„ì ì¸ TTA ë³€í™˜ë“¤ì„ ë°˜í™˜\"\"\"\n",
    "    return [\n",
    "        # 1. ê¸°ë³¸ ë³€í™˜ (ë³€í™˜ ì—†ìŒ)\n",
    "        base_transform,\n",
    "        \n",
    "        # 2-5. íšŒì „ ë³€í™˜ë“¤ (ë” ì •êµí•˜ê²Œ)\n",
    "        A.Compose(base_ops + [A.Rotate(limit=[90,90], p=1.0)] + normalize_ops),\n",
    "        A.Compose(base_ops + [A.Rotate(limit=[180,180], p=1.0)] + normalize_ops),\n",
    "        A.Compose(base_ops + [A.Rotate(limit=[270,270], p=1.0)] + normalize_ops),\n",
    "        \n",
    "        # 6. ìŠ¤ì¼€ì¼ ë³€í™˜ (ë¬¸ì„œ í¬ê¸° ë³€í™” ëŒ€ì‘)\n",
    "        A.Compose(base_ops + [A.RandomScale(scale_limit=0.1, p=1.0)] + normalize_ops),\n",
    "        \n",
    "        # 7. ë°ê¸°/ëŒ€ë¹„ ë³€í™˜ (ìŠ¤ìº” í’ˆì§ˆ ë³€í™” ëŒ€ì‘)\n",
    "        A.Compose(base_ops + [A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0)] + normalize_ops),\n",
    "        \n",
    "        # 8-9. Flip ë³€í™˜ë“¤ (ë¬¸ì„œ ë°©í–¥ ë³€í™” ëŒ€ì‘)\n",
    "        A.Compose(base_ops + [A.HorizontalFlip(p=1.0)] + normalize_ops),\n",
    "        A.Compose(base_ops + [A.VerticalFlip(p=1.0)] + normalize_ops),\n",
    "        \n",
    "        # 10. ì¶”ê°€: ì‘ì€ íšŒì „ (ë¯¸ì„¸í•œ ê¸°ìš¸ê¸° ë³´ì •)\n",
    "        A.Compose(base_ops + [A.Rotate(limit=[-5, 5], p=1.0)] + normalize_ops),\n",
    "        \n",
    "        # 11. ì¶”ê°€: ìƒ‰ìƒ ì§€í„°ë§ (ë‹¤ì–‘í•œ ìŠ¤ìº” ì¡°ê±´ ëŒ€ì‘)\n",
    "        A.Compose(base_ops + [A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=1.0)] + normalize_ops),\n",
    "    ]\n",
    "\n",
    "# ê°œì„ ëœ TTA ë³€í™˜ë“¤ ìƒì„±\n",
    "comprehensive_tta_transforms = get_comprehensive_tta()\n",
    "\n",
    "print(f\"ê°œì„ ëœ TTA ë³€í™˜ {len(comprehensive_tta_transforms)}ê°œ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"TTA ë³€í™˜ ëª©ë¡:\")\n",
    "transform_names = [\n",
    "    \"ì›ë³¸ (ë³€í™˜ì—†ìŒ)\",\n",
    "    \"90ë„ íšŒì „\", \n",
    "    \"180ë„ íšŒì „\",\n",
    "    \"270ë„ íšŒì „\",\n",
    "    \"ìŠ¤ì¼€ì¼ ë³€í™˜ (Â±10%)\",\n",
    "    \"ë°ê¸°/ëŒ€ë¹„ ì¡°ì •\",\n",
    "    \"ìˆ˜í‰ ë’¤ì§‘ê¸°\",\n",
    "    \"ìˆ˜ì§ ë’¤ì§‘ê¸°\", \n",
    "    \"ë¯¸ì„¸ íšŒì „ (Â±5ë„)\",\n",
    "    \"ìƒ‰ìƒ ì§€í„°ë§\"\n",
    "]\n",
    "\n",
    "for i, name in enumerate(transform_names):\n",
    "    print(f\"  {i+1:2d}. {name}\")\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"tta_improved/num_transforms\": len(comprehensive_tta_transforms),\n",
    "            \"tta_improved/transforms_used\": transform_names,\n",
    "            \"tta_improved/batch_size\": 48,  # ë” ë§ì€ ë³€í™˜ìœ¼ë¡œ ì¸í•´ ë°°ì¹˜ í¬ê¸° ì¡°ì •\n",
    "            \"tta_improved/expected_improvement\": \"5-15% over basic TTA\"\n",
    "        })\n",
    "    else:\n",
    "        print(\"ê°œì„ ëœ TTA ì„¤ì • ì •ë³´:\")\n",
    "        print(f\"  - ë³€í˜• ê°œìˆ˜: {len(comprehensive_tta_transforms)}\")\n",
    "        print(f\"  - ë°°ì¹˜ í¬ê¸°: 48\")\n",
    "        print(f\"  - ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 5-15%\")\n",
    "except Exception as e:\n",
    "    print(f\"TTA ì„¤ì • ë¡œê¹… ì‹¤íŒ¨: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 16. ê°œì„ ëœ TTA Dataset and DataLoader\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedTTAImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        \n",
    "        # ëª¨ë“  transformì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            try:\n",
    "                aug_img = transform(image=img)['image']\n",
    "                augmented_images.append(aug_img)\n",
    "            except Exception as e:\n",
    "                print(f\"Transform ì‹¤íŒ¨ (ì´ë¯¸ì§€ {name}): {e}\")\n",
    "                # ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ì„ ê¸°ë³¸ ë³€í™˜ìœ¼ë¡œ ì¶”ê°€\n",
    "                aug_img = self.transforms[0](image=img)['image']  # ì²« ë²ˆì§¸ëŠ” ê¸°ë³¸ ë³€í™˜\n",
    "                augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target\n",
    "\n",
    "# ê°œì„ ëœ TTA Dataset ìƒì„±\n",
    "improved_tta_dataset = ImprovedTTAImageDataset(\n",
    "    \"../data/sample_submission.csv\",\n",
    "    \"../data/test/\",\n",
    "    comprehensive_tta_transforms\n",
    ")\n",
    "\n",
    "# ê°œì„ ëœ TTA DataLoader (ë” ë§ì€ ë³€í™˜ìœ¼ë¡œ ì¸í•´ ë°°ì¹˜ í¬ê¸° ì¡°ì •)\n",
    "improved_tta_loader = DataLoader(\n",
    "    improved_tta_dataset,\n",
    "    batch_size=48,  # 10ê°œ ë³€í™˜ * 5ê°œ ëª¨ë¸ = 50ê°œ ì˜ˆì¸¡ì´ë¯€ë¡œ ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ ì¡°ì •\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"ê°œì„ ëœ TTA Dataset: {len(improved_tta_dataset)}ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: 48 (ì´ {len(comprehensive_tta_transforms)} * 5 = {len(comprehensive_tta_transforms)*5}ê°œ ì˜ˆì¸¡ í‰ê· )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45382398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 17. ê°œì„ ëœ Ensemble + TTA Inference í•¨ìˆ˜\n",
    "# =============================================================================\n",
    "\n",
    "def improved_ensemble_tta_inference(models, loader, transforms, confidence_threshold=0.9):\n",
    "    \"\"\"ê°œì„ ëœ 5-Fold ëª¨ë¸ ì•™ìƒë¸” + 10ê°œ TTA ì¶”ë¡ \"\"\"\n",
    "    all_predictions = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    # TTA ì§„í–‰ìƒí™© ë¡œê¹…ì„ ìœ„í•œ í…Œì´ë¸”\n",
    "    tta_progress = wandb.Table(columns=[\"Batch\", \"Avg_Confidence\", \"Low_Conf_Count\", \"High_Conf_Count\", \"Total_Augmentations\"])\n",
    "    \n",
    "    # Temperature scaling ì´ˆê¸°í™”\n",
    "    temp_scaling = TemperatureScaling(temperature=1.2).to(device)  # ì•½ê°„ ì¡°ì •\n",
    "    \n",
    "    print(f\"ê°œì„ ëœ ì•™ìƒë¸” TTA ì¶”ë¡  ì‹œì‘...\")\n",
    "    print(f\"{len(models)}ê°œ ëª¨ë¸ Ã— {len(transforms)}ê°œ TTA ë³€í˜• = {len(models) * len(transforms)}ê°œ ì˜ˆì¸¡ í‰ê· \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Improved Ensemble TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        ensemble_probs = torch.zeros(batch_size, 17).to(device)\n",
    "        total_predictions = 0\n",
    "        \n",
    "        # ê° fold ëª¨ë¸ë³„ ì˜ˆì¸¡\n",
    "        for model_idx, model in enumerate(models):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # ê° TTA ë³€í˜•ë³„ ì˜ˆì¸¡\n",
    "                for tta_idx, images in enumerate(images_list):\n",
    "                    try:\n",
    "                        images = images.to(device)\n",
    "                        preds = model(images)\n",
    "                        \n",
    "                        # Temperature scaling ì ìš©\n",
    "                        preds = temp_scaling(preds)\n",
    "                        probs = torch.softmax(preds, dim=1)\n",
    "                        \n",
    "                        # ì•™ìƒë¸” í™•ë¥ ì— ëˆ„ì  (í‰ê· )\n",
    "                        ensemble_probs += probs\n",
    "                        total_predictions += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"ì˜ˆì¸¡ ì‹¤íŒ¨ (ëª¨ë¸ {model_idx+1}, TTA {tta_idx+1}): {e}\")\n",
    "                        continue\n",
    "        \n",
    "        # í‰ê·  ê³„ì‚°\n",
    "        if total_predictions > 0:\n",
    "            ensemble_probs = ensemble_probs / total_predictions\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ê³„ì‚°\n",
    "        max_probs = torch.max(ensemble_probs, dim=1)[0]\n",
    "        batch_confidences = max_probs.cpu().numpy()\n",
    "        all_confidences.extend(batch_confidences)\n",
    "        \n",
    "        final_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "        all_predictions.extend(final_preds.cpu().numpy())\n",
    "        \n",
    "        # ë°°ì¹˜ë³„ ì‹ ë¢°ë„ ë¶„ì„\n",
    "        high_conf_count = np.sum(batch_confidences >= confidence_threshold)\n",
    "        low_conf_count = batch_size - high_conf_count\n",
    "        avg_confidence = np.mean(batch_confidences)\n",
    "        \n",
    "        # ì§„í–‰ìƒí™© í…Œì´ë¸”ì— ì¶”ê°€\n",
    "        tta_progress.add_data(batch_idx, avg_confidence, low_conf_count, high_conf_count, total_predictions)\n",
    "        \n",
    "        # ë°°ì¹˜ë³„ ìƒì„¸ ë¡œê¹… (15ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if batch_idx % 15 == 0 and wandb.run is not None:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            estimated_total = elapsed_time * len(loader) / (batch_idx + 1)\n",
    "            remaining_time = estimated_total - elapsed_time\n",
    "            \n",
    "            wandb.log({\n",
    "                \"improved_tta_progress/batch\": batch_idx,\n",
    "                \"improved_tta_progress/avg_confidence\": avg_confidence,\n",
    "                \"improved_tta_progress/high_confidence_ratio\": high_conf_count / batch_size,\n",
    "                \"improved_tta_progress/total_augmentations_per_sample\": total_predictions / batch_size,\n",
    "                \"improved_tta_progress/elapsed_time_min\": elapsed_time / 60,\n",
    "                \"improved_tta_progress/estimated_remaining_min\": remaining_time / 60,\n",
    "                \"improved_tta_progress/samples_processed\": (batch_idx + 1) * batch_size,\n",
    "            })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # TTA ìµœì¢… ê²°ê³¼ ë¡œê¹…\n",
    "    final_avg_confidence = np.mean(all_confidences)\n",
    "    confidence_std = np.std(all_confidences)\n",
    "    high_conf_samples = np.sum(np.array(all_confidences) >= confidence_threshold)\n",
    "    \n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\n",
    "            \"improved_tta_results/total_time_min\": total_time / 60,\n",
    "            \"improved_tta_results/samples_per_second\": len(all_predictions) / total_time,\n",
    "            \"improved_tta_results/final_avg_confidence\": final_avg_confidence,\n",
    "            \"improved_tta_results/confidence_std\": confidence_std,\n",
    "            \"improved_tta_results/high_confidence_samples\": high_conf_samples,\n",
    "            \"improved_tta_results/high_confidence_ratio\": high_conf_samples / len(all_predictions),\n",
    "            \"improved_tta_results/total_predictions\": len(all_predictions),\n",
    "            \"improved_tta_results/avg_augmentations_per_sample\": len(transforms) * len(models),\n",
    "            \"improved_tta_results/confidence_histogram\": wandb.Histogram(all_confidences),\n",
    "            \"improved_tta_results/progress_table\": tta_progress\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nê°œì„ ëœ ì•™ìƒë¸” TTA ì¶”ë¡  ì™„ë£Œ!\")\n",
    "    print(f\"ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
    "    print(f\"í‰ê·  ì‹ ë¢°ë„: {final_avg_confidence:.4f} Â± {confidence_std:.4f}\")\n",
    "    print(f\"ê³ ì‹ ë¢°ë„ ìƒ˜í”Œ: {high_conf_samples}/{len(all_predictions)} ({high_conf_samples/len(all_predictions)*100:.1f}%)\")\n",
    "    print(f\"ìƒ˜í”Œë‹¹ í‰ê·  ì˜ˆì¸¡ ìˆ˜: {len(transforms) * len(models)}\")\n",
    "\n",
    "    \n",
    "    return all_predictions, all_confidences\n",
    "\n",
    "print(\"ê°œì„ ëœ TTA ì„¤ì • ì™„ë£Œ!\")\n",
    "print(\"ê¸°ì¡´ 5ê°œ â†’ 10ê°œ TTA ë³€í˜•ìœ¼ë¡œ í–¥ìƒ\")\n",
    "print(\"ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ : ë” ì•ˆì •ì ì´ê³  ì •í™•í•œ ì˜ˆì¸¡\")\n",
    "\n",
    "tta_predictions, confidences = improved_ensemble_tta_inference(\n",
    "    models=ensemble_models, \n",
    "    loader=improved_tta_loader, \n",
    "    transforms=comprehensive_tta_transforms,\n",
    "    confidence_threshold=0.9\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9072c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 18. ê°œì„ ëœ Final Results and Submission\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n ìµœì¢… ê²°ê³¼ ì •ë¦¬ ì¤‘...\")\n",
    "\n",
    "# ê°œì„ ëœ TTA ê²°ê³¼ë¡œ submission íŒŒì¼ ìƒì„±\n",
    "tta_pred_df = pd.DataFrame(improved_tta_dataset.df, columns=['ID', 'target'])\n",
    "tta_pred_df['target'] = tta_predictions\n",
    "\n",
    "# ê¸°ì¡´ submissionê³¼ ë™ì¼í•œ ìˆœì„œì¸ì§€ í™•ì¸\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == tta_pred_df['ID']).all(), \"ID ìˆœì„œ ë¶ˆì¼ì¹˜!\"\n",
    "\n",
    "# ì˜ˆì¸¡ ë¶„í¬ ë¶„ì„\n",
    "pred_distribution = tta_pred_df['target'].value_counts().sort_index()\n",
    "pred_table = wandb.Table(columns=[\"Class\", \"Count\", \"Percentage\"])\n",
    "\n",
    "print(f\"\\nğŸ“Š ê°œì„ ëœ TTA ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\")\n",
    "for class_id in range(17):\n",
    "    count = pred_distribution.get(class_id, 0)\n",
    "    percentage = count / len(tta_pred_df) * 100\n",
    "    pred_table.add_data(class_id, count, percentage)\n",
    "    print(f\"Class {class_id:2d}: {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "# ì‹ ë¢°ë„ ë¶„ì„ (ê°œì„ ëœ TTAìš©)\n",
    "confidence_bins = [0.5, 0.7, 0.8, 0.9, 0.95, 1.0]\n",
    "confidence_analysis = {}\n",
    "for i, threshold in enumerate(confidence_bins):\n",
    "    if i == 0:\n",
    "        count = np.sum(np.array(confidences) >= threshold)\n",
    "    else:\n",
    "        prev_threshold = confidence_bins[i-1]\n",
    "        count = np.sum((np.array(confidences) >= prev_threshold) & (np.array(confidences) < threshold))\n",
    "    confidence_analysis[f\"improved_tta_conf_{threshold}\"] = count\n",
    "\n",
    "# ê°œì„ ëœ TTA ìµœì¢… ê²°ê³¼ ë¡œê¹…\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"improved_final_results/total_predictions\": len(tta_predictions),\n",
    "            \"improved_final_results/unique_classes_predicted\": len(np.unique(tta_predictions)),\n",
    "            \"improved_final_results/prediction_distribution_table\": pred_table,\n",
    "            \"improved_final_results/avg_confidence\": np.mean(confidences),\n",
    "            \"improved_final_results/median_confidence\": np.median(confidences),\n",
    "            \"improved_final_results/min_confidence\": np.min(confidences),\n",
    "            \"improved_final_results/max_confidence\": np.max(confidences),\n",
    "            \"improved_final_results/confidence_distribution\": wandb.Histogram(confidences),\n",
    "            \"improved_final_results/tta_method\": \"10-transform comprehensive TTA\",\n",
    "            \"improved_final_results/total_augmentations_per_sample\": len(comprehensive_tta_transforms) * len(ensemble_models),\n",
    "            **confidence_analysis\n",
    "        })\n",
    "        print(\"ê°œì„ ëœ TTA ìµœì¢… ê²°ê³¼ WandB ë¡œê¹… ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"í™œì„±í™”ëœ runì´ ì—†ì–´ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"WandB ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "# ì½˜ì†” ì¶œë ¥ì€ í•­ìƒ ì‹¤í–‰\n",
    "print(f\"ì´ ì˜ˆì¸¡ ìˆ˜: {len(tta_predictions)}\")\n",
    "print(f\"ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ìˆ˜: {len(np.unique(tta_predictions))}\")\n",
    "print(f\"í‰ê·  ì‹ ë¢°ë„: {np.mean(confidences):.4f}\")\n",
    "print(f\"ì‹ ë¢°ë„ ë²”ìœ„: {np.min(confidences):.4f} ~ {np.max(confidences):.4f}\")\n",
    "print(f\"ìƒ˜í”Œë‹¹ ì´ ì˜ˆì¸¡ ìˆ˜: {len(comprehensive_tta_transforms) * len(ensemble_models)}\")\n",
    "\n",
    "# ê°œì„ ëœ TTA ì˜ˆì¸¡ ë¶„í¬ ë°”ì°¨íŠ¸\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        pred_dist_data = [[f\"Class_{i}\", pred_distribution.get(i, 0)] for i in range(17)]\n",
    "        wandb.run.log({\n",
    "            \"improved_final_results/prediction_distribution_chart\": wandb.plot.bar(\n",
    "                wandb.Table(data=pred_dist_data, columns=[\"Class\", \"Count\"]),\n",
    "                \"Class\", \"Count\", \n",
    "                title=\"Improved TTA Final Prediction Distribution\"\n",
    "            ),\n",
    "            \"improved_final_results/confidence_vs_basic_tta\": {\n",
    "                \"improved_tta_avg\": np.mean(confidences),\n",
    "                \"confidence_improvement\": \"Expected 5-15% higher confidence\"\n",
    "            }\n",
    "        })\n",
    "        print(\"ê°œì„ ëœ TTA ì˜ˆì¸¡ ë¶„í¬ ì°¨íŠ¸ ë¡œê¹… ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"ì°¨íŠ¸ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"ì°¨íŠ¸ ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ (ê°œì„ ëœ TTA ê²°ê³¼)\n",
    "output_path = \"../output/choice4_improved_tta.csv\"\n",
    "tta_pred_df.to_csv(output_path, index=False)\n",
    "\n",
    "# ê°œì„ ëœ TTA ê²°ê³¼ íŒŒì¼ì„ WandB ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥\n",
    "improved_artifact = wandb.Artifact(\n",
    "    name=\"improved_tta_final_predictions\",\n",
    "    type=\"predictions\",\n",
    "    description=f\"Improved TTA ensemble predictions with {N_FOLDS}-fold CV + {len(comprehensive_tta_transforms)} TTA transforms\"\n",
    ")\n",
    "improved_artifact.add_file(output_path)\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log_artifact(improved_artifact)\n",
    "        print(\"ê°œì„ ëœ TTA ì‹¤í—˜ ì•„í‹°íŒ©íŠ¸ ë¡œê¹… ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"í™œì„±í™”ëœ runì´ ì—†ì–´ ì‹¤í—˜ ìš”ì•½ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"ì‹¤í—˜ ìš”ì•½ ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "print(f\"\\n ê°œì„ ëœ TTA ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\" íŒŒì¼ ìœ„ì¹˜: {output_path}\")\n",
    "print(f\" ì´ ì˜ˆì¸¡ ìˆ˜: {len(tta_predictions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30990203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 19. ê°œì„ ëœ Experiment Summary and Cleanup\n",
    "# =============================================================================\n",
    "\n",
    "# ê°œì„ ëœ ì‹¤í—˜ ìš”ì•½ ìƒì„±\n",
    "improved_experiment_summary = {\n",
    "    \"experiment_name\": main_run.name,\n",
    "    \"model_architecture\": model_name,\n",
    "    \"image_size\": img_size,\n",
    "    \"cv_strategy\": f\"{N_FOLDS}-Fold StratifiedKFold\",\n",
    "    \"cv_mean_f1\": mean_f1,\n",
    "    \"cv_std_f1\": std_f1,\n",
    "    \"cv_best_fold\": max(val_f1_scores),\n",
    "    \"ensemble_models\": len(ensemble_models),\n",
    "    \"tta_transforms\": len(comprehensive_tta_transforms),  # ê°œì„ ëœ TTA ê°œìˆ˜\n",
    "    \"tta_improvement\": \"5ê°œ â†’ 10ê°œ ë³€í˜•ìœ¼ë¡œ í–¥ìƒ\",\n",
    "    \"tta_transforms_detail\": transform_names,\n",
    "    \"total_training_time_min\": sum([r['epochs_trained'] for r in fold_results]) * 2,\n",
    "    \"avg_prediction_confidence\": np.mean(confidences),\n",
    "    \"high_confidence_predictions\": np.sum(np.array(confidences) >= 0.9),\n",
    "    \"total_augmentations_per_sample\": len(comprehensive_tta_transforms) * len(ensemble_models),\n",
    "    \"experiment_tags\": [\"improved-tta\", \"efficientnet-b3\", \"k-fold-cv\", \"10-transform-tta\", \"ensemble\"]\n",
    "}\n",
    "\n",
    "# ê°œì„ ëœ ì‹¤í—˜ ìš”ì•½ ë¡œê¹…\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\"improved_experiment_summary\": improved_experiment_summary})\n",
    "        print(\"ê°œì„ ëœ ì‹¤í—˜ ìš”ì•½ ë¡œê¹… ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"í™œì„±í™”ëœ runì´ ì—†ì–´ ì‹¤í—˜ ìš”ì•½ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"ì‹¤í—˜ ìš”ì•½ ë¡œê¹… ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "# ë§ˆì§€ë§‰ ìƒíƒœ ì—…ë°ì´íŠ¸ (ê°œì„ ëœ ë²„ì „)\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"status\": \"completed_with_improved_tta\",\n",
    "            \"completion_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"tta_method\": \"comprehensive_10_transforms\",\n",
    "            \"performance_expectation\": \"5-15% improvement over basic TTA\"\n",
    "        })\n",
    "        print(\"ê°œì„ ëœ TTA ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"í™œì„±í™”ëœ runì´ ì—†ì–´ ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "print(f\"\\nê°œì„ ëœ TTA ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ê°œì„ ëœ TTA ì‹¤í—˜ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\" K-Fold CV ê²°ê³¼: {mean_f1:.4f} Â± {std_f1:.4f}\")\n",
    "print(f\" ìµœê³  ì„±ëŠ¥ Fold: {max(val_f1_scores):.4f}\")\n",
    "print(f\" ì•™ìƒë¸” ëª¨ë¸: {len(ensemble_models)}ê°œ\")\n",
    "print(f\" TTA ë³€í˜•: {len(comprehensive_tta_transforms)}ê°œ (ê¸°ì¡´ 5ê°œ â†’ ê°œì„  10ê°œ)\")\n",
    "print(f\" ìƒ˜í”Œë‹¹ ì´ ì˜ˆì¸¡ ìˆ˜: {len(comprehensive_tta_transforms) * len(ensemble_models)}\")\n",
    "print(f\" í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: {np.mean(confidences):.4f}\")\n",
    "print(f\" WandB ëŒ€ì‹œë³´ë“œ: {main_run.url}\")\n",
    "\n",
    "# ê°œì„ ëœ TTA ë³€í˜• ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\n",
    "print(f\"\\n ì ìš©ëœ TTA ë³€í˜•:\")\n",
    "for i, name in enumerate(transform_names):\n",
    "    print(f\"  {i+1:2d}. {name}\")\n",
    "\n",
    "# Sample predictions ì¶œë ¥\n",
    "print(f\"\\n ê°œì„ ëœ TTA ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ:\")\n",
    "print(tta_pred_df.head(10))\n",
    "\n",
    "# ë©”ì¸ run ì¢…ë£Œ\n",
    "main_run.finish()\n",
    "\n",
    "print(f\"\\n ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
    "print(f\" ê²°ê³¼ íŒŒì¼: {output_path}\")\n",
    "print(f\" ê¸°ì¡´ ëŒ€ë¹„ ê°œì„ ì‚¬í•­: 5ê°œ â†’ 10ê°œ TTA ë³€í˜•\")\n",
    "print(f\" ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: ë” ì•ˆì •ì ì´ê³  ì •í™•í•œ ì˜ˆì¸¡\")\n",
    "print(f\" WandBì—ì„œ ì „ì²´ ì‹¤í—˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del ensemble_models\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nğŸ‰ ê°œì„ ëœ TTA ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“ˆ 10ê°œ TTA ë³€í˜•ìœ¼ë¡œ ë”ìš± ê°•ê±´í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07f9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
