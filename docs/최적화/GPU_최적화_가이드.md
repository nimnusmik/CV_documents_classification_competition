# ğŸš€ GPU ìµœì í™” ì™„ì „ ê°€ì´ë“œ (Team ê³ ì„±ëŠ¥ ê¸°ë²• í†µí•©)

## ğŸ—ï¸ GPU ìµœì í™” ì•„í‚¤í…ì²˜ (Team ConvNeXt ìµœì í™” í¬í•¨)

```mermaid
flowchart TD
    subgraph Phase1 ["ğŸ” GPU í™˜ê²½ ë¶„ì„"]
        direction LR
        A["nvidia-smi<br/>GPU ìƒíƒœ í™•ì¸"]
        B["team_gpu_check.py<br/>í™˜ê²½ ë¶„ì„"]
        C["GPU ë²¤ì¹˜ë§ˆí¬<br/>ì²˜ë¦¬ëŸ‰ ì¸¡ì •"]
        A --> B --> C
    end
    
    subgraph Phase2 ["âš™ï¸ ë©”ëª¨ë¦¬ ìµœì í™”"]
        direction LR
        D["auto_batch_size.py<br/>ë°°ì¹˜ í¬ê¸° ì¡°ì •"]
        E["Mixed Precision<br/>FP16 ìµœì í™”"]
        F["Gradient Accumulation<br/>ë°°ì¹˜ ëˆ„ì "]
        D --> E --> F
    end
    
    subgraph Phase3 ["ğŸƒ ì—°ì‚° ìµœì í™”"]
        direction LR
        G["DataLoader<br/>ë°ì´í„° ë¡œë”©"]
        H["torch.compile<br/>ëª¨ë¸ ì»´íŒŒì¼"]
        I["TensorRT<br/>ì¶”ë¡  ê°€ì†í™”"]
        G --> H --> I
    end
    
    subgraph Phase4 ["ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"]
        direction LR
        J["GPU ëª¨ë‹ˆí„°ë§<br/>ì„±ëŠ¥ ì¶”ì "]
        K["í”„ë¡œíŒŒì¼ë§<br/>ë³‘ëª© ë¶„ì„"]
        L["ìë™ íŠœë‹<br/>ìµœì í™” í•™ìŠµ"]
        J --> K --> L
    end
    
    %% ì„œë¸Œê·¸ë˜í”„ ê°„ ì„¸ë¡œ ì—°ê²°
    Phase1 --> Phase2
    Phase2 --> Phase3
    Phase3 --> Phase4
    
    style A fill:#e1f5fe, color:#000000
    style D fill:#f3e5f5, color:#000000
    style G fill:#e8f5e8, color:#000000
    style J fill:#fff3e0, color:#000000
```

## ğŸ”„ GPU ìµœì í™” íë¦„ë„

```mermaid
flowchart TD
    subgraph Phase1 ["Phase 1: í™˜ê²½ ë¶„ì„ ë° ì„¤ì •"]
        direction LR
        A1["ğŸ” GPU í•˜ë“œì›¨ì–´<br/>ì •ë³´ ìˆ˜ì§‘"]
        A2["ğŸ“Š í˜„ì¬ ì‚¬ìš©ëŸ‰<br/>ë¶„ì„"]
        A3["ğŸ¯ ìµœì í™” ëª©í‘œ<br/>ì„¤ì •"]
        A1 --> A2 --> A3
    end
    
    subgraph Phase2 ["Phase 2: ë©”ëª¨ë¦¬ ìµœì í™”"]
        direction LR
        B1["ğŸ“ ë°°ì¹˜ í¬ê¸°<br/>ìµœì í™”"]
        B2["ğŸ­ ì •ë°€ë„<br/>ìµœì í™”"]
        B3["ğŸ”„ ê·¸ë˜ë””ì–¸íŠ¸<br/>ëˆ„ì "]
        B1 --> B2 --> B3
    end
    
    subgraph Phase3 ["Phase 3: ì—°ì‚° ìµœì í™”"]
        direction LR
        C1["âš¡ ë°ì´í„° ë¡œë”©<br/>ìµœì í™”"]
        C2["ğŸ§  ëª¨ë¸ ì»´íŒŒì¼<br/>ìµœì í™”"]
        C3["ğŸš„ ì¶”ë¡ <br/>ê°€ì†í™”"]
        C1 --> C2 --> C3
    end
    
    subgraph Phase4 ["Phase 4: ëª¨ë‹ˆí„°ë§ ë° ì¡°ì •"]
        direction LR
        D1["ğŸ“ˆ ì‹¤ì‹œê°„<br/>ëª¨ë‹ˆí„°ë§"]
        D2["ğŸ”§ ìë™<br/>íŠœë‹"]
        D3["ğŸ“‹ ë³´ê³ ì„œ<br/>ìƒì„±"]
        D1 --> D2 --> D3
    end
    
    %% ì„œë¸Œê·¸ë˜í”„ ê°„ ì„¸ë¡œ ì—°ê²°
    Phase1 --> Phase2
    Phase2 --> Phase3
    Phase3 --> Phase4
    
    style A1 fill:#e1f5fe, color:#000000
    style B1 fill:#f3e5f5, color:#000000
    style C1 fill:#e8f5e8, color:#000000
    style D1 fill:#fff3e0, color:#000000
```

## ğŸ“ GPU ìµœì í™” íŒŒì¼ ê°„ ì˜ì¡´ ê´€ê³„

```mermaid
graph TB
    subgraph "ğŸ”§ GPU ìœ í‹¸ë¦¬í‹°"
        AUTO_BATCH[src/utils/gpu_optimization/auto_batch_size.py<br/>ë™ì  ë°°ì¹˜ í¬ê¸° ê²°ì •<br/>GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ìµœì í™”]
        TEAM_CHECK[src/utils/gpu_optimization/team_gpu_check.py<br/>GPU í™˜ê²½ ë¶„ì„<br/>ìµœì  ì„¤ì • ì¶”ì²œ]
        GPU_UTILS[src/utils/gpu_optimization/<br/>GPU ìµœì í™” íŒ¨í‚¤ì§€<br/>ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ê´€ë¦¬]
    end
    
    subgraph "âš™ï¸ ì„¤ì • ê´€ë¦¬"
        TRAIN_CONFIG[configs/train*.yaml<br/>í•™ìŠµ ë°°ì¹˜ í¬ê¸°<br/>ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •]
        INFER_CONFIG[configs/infer*.yaml<br/>ì¶”ë¡  ë°°ì¹˜ í¬ê¸°<br/>TensorRT ì„¤ì •]
        OPT_CONFIG[configs/gpu_optimization.yaml<br/>GPU ìµœì í™” íŒŒë¼ë¯¸í„°<br/>ì„±ëŠ¥ íŠœë‹ ì„¤ì •]
    end
    
    subgraph "ğŸ“ í•™ìŠµ íŒŒì´í”„ë¼ì¸"
        TRAIN_MAIN[src/training/train_main.py<br/>í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬<br/>GPU ìµœì í™” ì ìš©]
        TRAIN[src/training/train.py<br/>í•µì‹¬ í•™ìŠµ ë¡œì§<br/>Mixed Precision ì‚¬ìš©]
        DATA_LOADER[src/data/dataset.py<br/>ë°ì´í„° ë¡œë”© ìµœì í™”<br/>GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±]
    end
    
    subgraph "ğŸ”® ì¶”ë¡  íŒŒì´í”„ë¼ì¸"
        INFER_MAIN[src/inference/infer_main.py<br/>ì¶”ë¡  í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬<br/>TensorRT ìµœì í™”]
        INFER[src/inference/infer.py<br/>í•µì‹¬ ì¶”ë¡  ë¡œì§<br/>ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”]
        TRT_ENGINE[src/inference/tensorrt_engine.py<br/>TensorRT ì—”ì§„ ê´€ë¦¬<br/>ê°€ì†í™”ëœ ì¶”ë¡ ]
    end
    
    subgraph "ğŸ“Š ëª¨ë‹ˆí„°ë§"
        GPU_MONITOR[src/monitoring/gpu_monitor.py<br/>ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§<br/>ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘]
        PROFILER[src/profiling/performance_profiler.py<br/>ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§<br/>ë³‘ëª© ì§€ì  ë¶„ì„]
        METRICS[src/metrics/gpu_metrics.py<br/>GPU ì„±ëŠ¥ ì§€í‘œ<br/>ìµœì í™” íš¨ê³¼ ì¸¡ì •]
    end
    
    %% GPU ìœ í‹¸ë¦¬í‹° ì—°ê²°
    AUTO_BATCH --> TRAIN_CONFIG
    AUTO_BATCH --> INFER_CONFIG
    TEAM_CHECK --> OPT_CONFIG
    GPU_UTILS --> TRAIN_MAIN
    GPU_UTILS --> INFER_MAIN
    
    %% ì„¤ì • íŒŒì¼ ì—°ê²°
    TRAIN_CONFIG --> TRAIN_MAIN
    INFER_CONFIG --> INFER_MAIN
    OPT_CONFIG --> GPU_MONITOR
    
    %% í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì—°ê²°
    TRAIN_MAIN --> TRAIN
    TRAIN --> DATA_LOADER
    AUTO_BATCH --> DATA_LOADER
    
    %% ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì—°ê²°
    INFER_MAIN --> INFER
    INFER --> TRT_ENGINE
    AUTO_BATCH --> TRT_ENGINE
    
    %% ëª¨ë‹ˆí„°ë§ ì—°ê²°
    TRAIN --> GPU_MONITOR
    INFER --> GPU_MONITOR
    GPU_MONITOR --> PROFILER
    PROFILER --> METRICS
    
    style AUTO_BATCH fill:#e1f5fe, color:#000000
    style TRAIN_MAIN fill:#f3e5f5, color:#000000
    style INFER_MAIN fill:#e8f5e8, color:#000000
    style GPU_MONITOR fill:#fff3e0, color:#000000
```

## ğŸ› ï¸ GPU ìµœì í™” ë„êµ¬ ë° ìœ í‹¸ë¦¬í‹°

### 1. ğŸ“Š GPU í™˜ê²½ ë¶„ì„ ë„êµ¬

#### ê¸°ë³¸ GPU ì •ë³´ í™•ì¸
```bash
# GPU í•˜ë“œì›¨ì–´ ì •ë³´
nvidia-smi --query-gpu=name,memory.total,driver_version,cuda_version --format=csv

# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
nvidia-smi dmon -s pucvmet -d 1

# GPU í† í´ë¡œì§€ í™•ì¸
nvidia-smi topo -m

# CUDA í˜¸í™˜ì„± í™•ì¸
python -c "
import torch
print(f'PyTorch CUDA: {torch.version.cuda}')
print(f'PyTorch ë²„ì „: {torch.__version__}')
print(f'CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
print(f'GPU ê°œìˆ˜: {torch.cuda.device_count()}')
for i in range(torch.cuda.device_count()):
    print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
    print(f'ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f}GB')
"
```

#### GPU í™˜ê²½ í†µí•© ë¶„ì„
```bash
# GPU í™˜ê²½ ì²´í¬
python src/utils/gpu_optimization/team_gpu_check.py --detailed

# GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
python src/utils/gpu_optimization/team_gpu_check.py --benchmark

# ìµœì  ì„¤ì • ì¶”ì²œ
python src/utils/gpu_optimization/team_gpu_check.py --recommend

# GPU í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
python src/utils/gpu_optimization/team_gpu_check.py --compatibility-matrix
```

### 2. ğŸ§® ë©”ëª¨ë¦¬ ìµœì í™”

#### ë™ì  ë°°ì¹˜ í¬ê¸° ìµœì í™”
```bash
# í•™ìŠµìš© ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/train_highperf.yaml \
    --mode find_optimal \
    --safety_factor 0.95

# ì¶”ë¡ ìš© ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/infer_highperf.yaml \
    --mode find_optimal \
    --memory_fraction 0.9

# ë©€í‹° GPU ë°°ì¹˜ í¬ê¸° ì¡°ì •
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/train_highperf.yaml \
    --multi_gpu \
    --gpu_ids 0,1,2,3

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/train.yaml \
    --profile_memory \
    --save_profile logs/memory_profile.json
```

#### Mixed Precision ìµœì í™”
```bash
# FP16 í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
python -c "
import torch
from torch.cuda.amp import autocast, GradScaler

# FP16 ì§€ì› í™•ì¸
print(f'FP16 ì§€ì›: {torch.backends.cudnn.enabled}')
print(f'Mixed Precision ì§€ì›: {torch.cuda.amp.common.amp_definitely_not_available()}')

# ê°„ë‹¨í•œ FP16 í…ŒìŠ¤íŠ¸
model = torch.nn.Linear(1000, 100).cuda()
scaler = GradScaler()
optimizer = torch.optim.Adam(model.parameters())

x = torch.randn(32, 1000).cuda()
with autocast():
    output = model(x)
    loss = output.sum()

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
print('Mixed Precision í…ŒìŠ¤íŠ¸ ì„±ê³µ!')
"

# í•™ìŠµì—ì„œ Mixed Precision í™œì„±í™”
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --use_amp \
    --amp_opt_level O1

# ì¶”ë¡ ì—ì„œ FP16 ì‚¬ìš©
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --precision fp16 \
    --optimize_for_inference
```

#### ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìµœì í™”
```bash
# ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ìœ¼ë¡œ í° ë°°ì¹˜ ì‹œë®¬ë ˆì´ì…˜
python src/training/train_main.py \
    --config configs/train.yaml \
    --batch_size 16 \
    --accumulate_grad_batches 8 \
    --effective_batch_size 128

# ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•™ìŠµ
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --gradient_accumulation_steps 4 \
    --max_memory_usage 0.85
```

### 3. âš¡ ì—°ì‚° ìµœì í™”

#### ë°ì´í„° ë¡œë”© ìµœì í™”
```bash
# ìµœì  ì›Œì»¤ ìˆ˜ ì°¾ê¸°
python -c "
import torch
import time
from torch.utils.data import DataLoader
from src.data.dataset import CustomDataset

dataset = CustomDataset()
best_workers = 0
best_time = float('inf')

for num_workers in [0, 2, 4, 8, 16]:
    loader = DataLoader(dataset, batch_size=32, num_workers=num_workers, pin_memory=True)
    start = time.time()
    for i, batch in enumerate(loader):
        if i >= 100: break
    elapsed = time.time() - start
    print(f'Workers: {num_workers}, Time: {elapsed:.2f}s')
    if elapsed < best_time:
        best_time = elapsed
        best_workers = num_workers

print(f'ìµœì  ì›Œì»¤ ìˆ˜: {best_workers}')
"

# GPU í™˜ê²½ ì²´í¬ ë° ë²¤ì¹˜ë§ˆí¬
python src/utils/gpu_optimization/team_gpu_check.py \
    --benchmark \
    --detailed
```

#### ëª¨ë¸ ì»´íŒŒì¼ ìµœì í™”
```bash
# PyTorch 2.0 ì»´íŒŒì¼ ìµœì í™”
python -c "
import torch
import torch._dynamo as dynamo
from src.models.build import build_model

model = build_model('efficientnet_b3').cuda()
model.eval()

# ì»´íŒŒì¼ ìµœì í™” ì ìš©
optimized_model = torch.compile(model, mode='max-autotune')

# ì„±ëŠ¥ ë¹„êµ
x = torch.randn(1, 3, 224, 224).cuda()

# ì›ë³¸ ëª¨ë¸
start = torch.cuda.Event(enable_timing=True)
end = torch.cuda.Event(enable_timing=True)
start.record()
for _ in range(100):
    _ = model(x)
end.record()
torch.cuda.synchronize()
original_time = start.elapsed_time(end)

# ìµœì í™”ëœ ëª¨ë¸  
start.record()
for _ in range(100):
    _ = optimized_model(x)
end.record()
torch.cuda.synchronize()
optimized_time = start.elapsed_time(end)

print(f'ì›ë³¸ ëª¨ë¸: {original_time:.2f}ms')
print(f'ìµœì í™”ëœ ëª¨ë¸: {optimized_time:.2f}ms')
print(f'ì†ë„ í–¥ìƒ: {original_time/optimized_time:.2f}x')
"

# í•™ìŠµì—ì„œ ì»´íŒŒì¼ ìµœì í™” ì‚¬ìš©
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --compile_model \
    --compile_mode max-autotune

# ì¶”ë¡ ì—ì„œ ì»´íŒŒì¼ ìµœì í™” ì‚¬ìš©
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --compile_model \
    --dynamo_backend inductor
```

#### TensorRT ì¶”ë¡  ê°€ì†í™”
```bash
# TensorRT ì—”ì§„ ìƒì„±
python src/inference/convert_to_tensorrt.py \
    --model_path experiments/train/20250908/efficientnet_b3_20250908_0313/results/ckpt/best_fold0.pth \
    --output_path models/tensorrt/efficientnet_b3_fp16.engine \
    --precision fp16 \
    --max_batch_size 64

# TensorRT ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
python src/inference/benchmark_tensorrt.py \
    --engine_path models/tensorrt/efficientnet_b3_fp16.engine \
    --test_data data/raw/test \
    --warmup_runs 10 \
    --benchmark_runs 100

# TensorRTë¡œ ì¶”ë¡  ì‹¤í–‰
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --use_tensorrt \
    --engine_path models/tensorrt/efficientnet_b3_fp16.engine \
    --mode highperf
```

### 4. ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§

#### ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
```bash
# ê³ ê¸‰ GPU ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
python src/monitoring/gpu_monitor.py \
    --dashboard \
    --port 8080 \
    --update_interval 1

# GPU ë©”íŠ¸ë¦­ ë¡œê¹…
python src/monitoring/gpu_monitor.py \
    --log_file logs/gpu_metrics_$(date +%Y%m%d_%H%M).json \
    --interval 5 \
    --duration 3600

# ë©€í‹° GPU ëª¨ë‹ˆí„°ë§
python src/monitoring/gpu_monitor.py \
    --multi_gpu \
    --gpu_ids 0,1,2,3 \
    --export_prometheus
```

#### ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
```bash
# PyTorch Profiler ì‚¬ìš©
python src/profiling/profile_training.py \
    --config configs/train.yaml \
    --profile_steps 100 \
    --output_dir logs/profiling/

# CUDA ì»¤ë„ í”„ë¡œíŒŒì¼ë§
nsys profile -o logs/profiling/training_profile python src/training/train_main.py \
    --config configs/train_fast_optimized.yaml \
    --fold 0 \
    --max_epochs 1

# ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
python -m torch.profiler \
    --activities cpu,cuda \
    --record_shapes \
    --with_stack \
    --output_trace logs/profiling/memory_trace.json \
    src/training/train_main.py --config configs/train.yaml --profile_memory
```

#### GPU ìµœì í™” íš¨ê³¼ ì¸¡ì •
```bash
# ìµœì í™” ì „í›„ ì„±ëŠ¥ ë¹„êµ
python src/benchmarking/compare_optimization.py \
    --baseline_config configs/train.yaml \
    --optimized_config configs/train_highperf.yaml \
    --metrics throughput,memory_usage,energy_consumption \
    --output_report logs/optimization_report.html

# A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python src/optimization/ab_test_gpu_settings.py \
    --config_a configs/train.yaml \
    --config_b configs/train_optimized.yaml \
    --test_duration 300 \
    --statistical_significance 0.05
```

## ğŸ·ï¸ GPUë³„ ìµœì í™” ì„¤ì • ê°€ì´ë“œ

### RTX 4090 (24GB VRAM)
```bash
# ìµœëŒ€ ì„±ëŠ¥ ì„¤ì •
python src/utils/gpu_optimization/auto_batch_size.py \
    --gpu_model rtx4090 \
    --max_batch_size 384 \
    --image_size 448 \
    --safety_factor 0.95

# ê¶Œì¥ í•™ìŠµ ì„¤ì •
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --batch_size 256 \
    --num_workers 16 \
    --use_amp \
    --compile_model
```

### RTX 4080 (16GB VRAM)
```bash
# ê· í˜• ì¡íŒ ì„¤ì •
python src/utils/gpu_optimization/auto_batch_size.py \
    --gpu_model rtx4080 \
    --max_batch_size 256 \
    --image_size 384 \
    --safety_factor 0.90

# ê¶Œì¥ í•™ìŠµ ì„¤ì •
python src/training/train_main.py \
    --config configs/train.yaml \
    --batch_size 128 \
    --num_workers 12 \
    --use_amp \
    --gradient_accumulation_steps 2
```

### RTX 4070 (12GB VRAM)
```bash
# ë©”ëª¨ë¦¬ íš¨ìœ¨ ì„¤ì •
python src/utils/gpu_optimization/auto_batch_size.py \
    --gpu_model rtx4070 \
    --max_batch_size 128 \
    --image_size 320 \
    --safety_factor 0.85

# ê¶Œì¥ í•™ìŠµ ì„¤ì •
python src/training/train_main.py \
    --config configs/train_fast_optimized.yaml \
    --batch_size 64 \
    --num_workers 8 \
    --use_amp \
    --gradient_accumulation_steps 4
```

### RTX 3080 (10GB VRAM)
```bash
# ë³´ìˆ˜ì  ì„¤ì •
python src/utils/gpu_optimization/auto_batch_size.py \
    --gpu_model rtx3080 \
    --max_batch_size 96 \
    --image_size 288 \
    --safety_factor 0.80

# ê¶Œì¥ í•™ìŠµ ì„¤ì •
python src/training/train_main.py \
    --config configs/train_fast_optimized.yaml \
    --batch_size 48 \
    --num_workers 6 \
    --use_amp \
    --gradient_accumulation_steps 6
```

## ğŸ”§ ê³ ê¸‰ GPU ìµœì í™” ê¸°ë²•

### 1. ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
```bash
# ì ì‘í˜• ë°°ì¹˜ í¬ê¸° ìŠ¤ì¼€ì¤„ë§
python src/optimization/adaptive_batch_size.py \
    --config configs/train_highperf.yaml \
    --initial_batch_size 64 \
    --max_batch_size 256 \
    --memory_threshold 0.9 \
    --adjustment_factor 1.2

# ë©”ëª¨ë¦¬ ì••ë°• ìƒí™© ìë™ ëŒ€ì‘
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --adaptive_batch_size \
    --oom_recovery \
    --auto_scale_lr
```

### 2. ë©€í‹° GPU ìµœì í™”
```bash
# ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --master_port=29500 \
    src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --distributed

# ëª¨ë¸ ë³‘ë ¬ ì²˜ë¦¬
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --model_parallel \
    --pipeline_parallel_size 2 \
    --tensor_parallel_size 2

# GPU ê°„ í†µì‹  ìµœì í™”
python src/optimization/optimize_multi_gpu.py \
    --backend nccl \
    --bucket_size_mb 25 \
    --allreduce_algorithm ring
```

### 3. ë©”ëª¨ë¦¬ ìµœì í™” ê³ ê¸‰ ê¸°ë²•
```bash
# ì²´í¬í¬ì¸íŠ¸ í™œì„±í™” (ë©”ëª¨ë¦¬ vs ê³„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„)
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --use_checkpoint \
    --checkpoint_segments 4

# DeepSpeed ë©”ëª¨ë¦¬ ìµœì í™”
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --use_deepspeed \
    --zero_stage 2 \
    --offload_optimizer cpu

# ê·¸ë˜ë””ì–¸íŠ¸ ì••ì¶•
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --gradient_compression \
    --compression_ratio 0.1
```

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë° ë¬¸ì œ í•´ê²°

### CUDA Out of Memory (OOM) í•´ê²°
```bash
# OOM ë°œìƒ ì‹œ ìë™ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/train.yaml \
    --find-optimal \
    --min-batch-size 8

# ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€
python src/debugging/memory_leak_detector.py \
    --config configs/train.yaml \
    --monitor_duration 600 \
    --threshold_mb 100

# GPU ë©”ëª¨ë¦¬ ê°•ì œ ì •ë¦¬
python -c "
import torch
import gc
torch.cuda.empty_cache()
gc.collect()
print('GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ')
print(f'ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()}')"
```

### ì„±ëŠ¥ ì €í•˜ ì§„ë‹¨
```bash
# GPU ë³‘ëª© ì§€ì  ë¶„ì„
python src/debugging/bottleneck_analyzer.py \
    --config configs/train_highperf.yaml \
    --analyze_dataloader \
    --analyze_model \
    --analyze_optimizer

# ì—´ì  ì“°ë¡œí‹€ë§ í™•ì¸
nvidia-smi -q -d temperature,power,clocks

# GPU ìƒíƒœ ë° ë“œë¼ì´ë²„ ìµœì í™” í™•ì¸
python src/utils/gpu_optimization/team_gpu_check.py \
    --detailed \
    --check-drivers \
    --recommend-settings
```

## ğŸ“Š GPU ìµœì í™” ì„±ê³¼ ì¸¡ì •

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```bash
# ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
python src/benchmarking/gpu_benchmark_suite.py \
    --config configs/train_highperf.yaml \
    --include_inference \
    --compare_precision fp32,fp16 \
    --output_report logs/gpu_benchmark_$(date +%Y%m%d).html

# ì—ë„ˆì§€ íš¨ìœ¨ì„± ì¸¡ì •
python src/benchmarking/energy_efficiency.py \
    --config configs/train_highperf.yaml \
    --measure_power_consumption \
    --duration 3600 \
    --calculate_performance_per_watt
```

### ìµœì í™” ROI ê³„ì‚°
```bash
# ìµœì í™” íˆ¬ì ëŒ€ë¹„ íš¨ê³¼ ë¶„ì„
python src/analysis/optimization_roi.py \
    --baseline_config configs/train.yaml \
    --optimized_config configs/train_highperf.yaml \
    --cost_model aws_pricing \
    --calculate_time_savings \
    --calculate_cost_savings
```

ì´ GPU ìµœì í™” ê°€ì´ë“œë¥¼ í†µí•´ ë‹¤ì–‘í•œ GPU í™˜ê²½ì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê° GPU ëª¨ë¸ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ìµœì í™”ë¥¼ ì ìš©í•˜ì—¬ í•™ìŠµ ë° ì¶”ë¡  ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ì„¸ìš”.

## ğŸ“Š GPU ì„±ëŠ¥ ë“±ê¸‰ ë° ìµœì í™” ì „ëµ

### ğŸ¯ GPUë³„ Team ê¸°ë²• ìµœì í™” ì„¤ì • (ConvNeXt Base 384 ê¸°ì¤€)

| GPU ë“±ê¸‰ | ì˜ˆì‹œ ëª¨ë¸ | VRAM | ConvNeXt ë°°ì¹˜ | TTA ëª¨ë“œ | Essential TTA | Comprehensive TTA | Team F1 ì˜ˆìƒ |
|----------|-----------|------|--------------|----------|--------------|------------------|-------------|
| ğŸ† **HIGH-END** | RTX 4090, RTX 4080 Super | 24GB/16GB | 48-64 | Essential/Comprehensive | 17ë¶„ | 50ë¶„+ | **0.965+** |
| ğŸ¥ˆ **MID-RANGE** | RTX 4080, RTX 3080, RTX 3070 Ti | 16GB/12GB/8GB | 32-48 | Essential | 17ë¶„ | ë©”ëª¨ë¦¬ ë¶€ì¡± | **0.945-0.950** |
| ğŸ¥‰ **BUDGET** | RTX 4070, RTX 3070, RTX 3060 Ti | 12GB/8GB | 16-32 | Essential | 17ë¶„ | ë¶ˆê°€ | **0.945-0.950** |
| âš ï¸ **LOW-END** | RTX 3060, RTX 2070, GTX 1660 Ti | 8GB/6GB | 8-16 | Essential | 23ë¶„ | ë¶ˆê°€ | **0.940-0.945** |

> **ğŸ’¡ Team í•µì‹¬ í¬ì¸íŠ¸**: ConvNeXt Base 384 + Essential TTAë¡œ ëª¨ë“  GPUì—ì„œ 0.945+ F1 Score ë‹¬ì„± ê°€ëŠ¥!
>
> **ğŸ¯ ì¶”ì²œ ì „ëµ**: RTX 3080 ì´ìƒì—ì„œëŠ” Comprehensive TTAë¡œ 0.965+ ëª©í‘œ, ì´í•˜ì—ì„œëŠ” Essential TTAë¡œ ì•ˆì •ì  0.945+ ë‹¬ì„±

### ğŸ› ï¸ GPUë³„ ìƒì„¸ ìµœì í™” ê°€ì´ë“œ

#### ğŸ† HIGH-END GPU (16GB+ VRAM) - Team ConvNeXt ìµœì í™”
```bash
# RTX 4090 (24GB) - Comprehensive TTA ìµœê³  ì„±ëŠ¥ (F1: 0.965+)
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf

# ì¶”ë¡ : Comprehensive TTA (50ë¶„+, F1: 0.965+)
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --mode highperf \
    --fold-results experiments/train/lastest-train/fold_results.yaml
# configs/infer_highperf.yamlì—ì„œ: tta_type: "comprehensive"

# RTX 4080 (16GB) - Essential TTA ê· í˜• ì„±ëŠ¥ (F1: 0.945-0.950)
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf

# ì¶”ë¡ : Essential TTA (17ë¶„, F1: 0.945-0.950)
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --mode highperf \
    --fold-results experiments/train/lastest-train/fold_results.yaml
# configs/infer_highperf.yamlì—ì„œ: tta_type: "essential"
```

#### ğŸ¥ˆ MID-RANGE GPU (8-16GB VRAM)
```bash
# RTX 3080 (10GB) - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ê³ ì„±ëŠ¥
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --batch_size 128 \
    --image_size 320 \
    --num_workers 8 \
    --use_amp \
    --gradient_accumulation_steps 2 \
    --mode highperf

# RTX 3070 (8GB) - ìµœì í™”ëœ ê³ ì„±ëŠ¥
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --batch_size 96 \
    --image_size 288 \
    --num_workers 6 \
    --use_amp \
    --gradient_accumulation_steps 3 \
    --mode highperf
```

#### ğŸ¥‰ BUDGET GPU (6-8GB VRAM)
```bash
# RTX 3060 (8GB) - ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  í™œìš©
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --batch_size 64 \
    --image_size 256 \
    --num_workers 4 \
    --use_amp \
    --gradient_accumulation_steps 4 \
    --mode highperf

# RTX 2070 (8GB) - ë³´ìˆ˜ì  ê³ ì„±ëŠ¥
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --batch_size 48 \
    --image_size 224 \
    --num_workers 4 \
    --use_amp \
    --gradient_accumulation_steps 6 \
    --mode highperf
```

#### âš ï¸ LOW-END GPU (4-6GB VRAM)
```bash
# RTX 2060 (6GB) - ë©”ëª¨ë¦¬ ìµœì í™” ê³ ì„±ëŠ¥
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --batch_size 32 \
    --image_size 224 \
    --num_workers 2 \
    --use_amp \
    --gradient_accumulation_steps 8 \
    --use_checkpoint \
    --mode highperf

# GTX 1660 Ti (6GB) - ê·¹í•œ ìµœì í™” ê³ ì„±ëŠ¥
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --batch_size 16 \
    --image_size 192 \
    --num_workers 2 \
    --use_amp \
    --gradient_accumulation_steps 16 \
    --use_checkpoint \
    --offload_optimizer \
    --mode highperf
```

### ğŸ”§ LOW-END GPUë¥¼ ìœ„í•œ ê³ ê¸‰ ìµœì í™” ê¸°ë²•

#### 1. ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
```yaml
# configs/train_lowend_highperf.yaml ìƒì„± ì˜ˆì‹œ
model:
  backbone: efficientnet_b0  # ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
  image_size: 224
  
training:
  batch_size: 16
  gradient_accumulation_steps: 16  # ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸°: 256
  use_amp: true
  use_checkpoint: true  # ë©”ëª¨ë¦¬ vs ê³„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„
  
optimization:
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.01
  
memory:
  offload_optimizer: true  # CPUë¡œ ì˜µí‹°ë§ˆì´ì € ì˜¤í”„ë¡œë“œ
  pin_memory: false  # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë¹„í™œì„±í™”
  num_workers: 1  # ì›Œì»¤ ìˆ˜ ìµœì†Œí™”
```

#### 2. ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
```bash
# LOW-END GPUìš© ìë™ ë°°ì¹˜ í¬ê¸° ìµœì í™”
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/train_lowend_highperf.yaml \
    --gpu_memory_limit 6144 \  # 6GB ì œí•œ
    --safety_factor 0.75 \     # ë³´ìˆ˜ì  ì•ˆì „ ë§ˆì§„
    --enable_oom_recovery      # OOM ë°œìƒ ì‹œ ìë™ ë³µêµ¬
```

#### 3. ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œìš©
```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ëŠ” ì²´í¬í¬ì¸íŒ…
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --batch_size 16 \
    --use_checkpoint \
    --checkpoint_segments 4 \
    --mode highperf
```

### ğŸ“ˆ Team ê¸°ë²• ì„±ëŠ¥ ë¹„êµ: ê¸°ì¡´ vs Team ConvNeXt

| GPU ëª¨ë¸ | ê¸°ì¡´ EfficientNet B3 | Team ConvNeXt Essential | Team ConvNeXt Comprehensive | ìµœëŒ€ ì„±ëŠ¥ í–¥ìƒ |
|----------|---------------------|--------------------------|------------------------------|---------------|
| RTX 4090 | F1: 0.9238 | F1: 0.9489 (17ë¶„) | F1: 0.9652 (50ë¶„+) | **+4.14%** |
| RTX 4080 | F1: 0.9238 | F1: 0.9489 (17ë¶„) | F1: 0.9580 (ì œí•œì ) | **+3.42%** |
| RTX 3080 | F1: 0.9238 | F1: 0.9489 (17ë¶„) | ë©”ëª¨ë¦¬ ë¶€ì¡± | **+2.51%** |
| RTX 3060 | F1: 0.9238 | F1: 0.9450 (23ë¶„) | ë¶ˆê°€ | **+2.12%** |

### ğŸ›ï¸ ì„¤ì • íŒŒì¼ ìë™ ìƒì„±

#### ëª¨ë“  GPUì—ì„œ HighPerf ëª¨ë“œ í™œì„±í™”
```bash
# GPU ìµœì  ë°°ì¹˜ í¬ê¸° ìë™ ê°ì§€
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/train_highperf.yaml \
    --find-optimal \
    --save-config configs/train_auto_optimized.yaml

# ìƒì„±ëœ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ ì‹¤í–‰
python src/training/train_main.py \
    --config configs/train_auto_optimized.yaml \
    --mode highperf
```

#### GPUë³„ ë§ì¶¤ ì„¤ì • íŒŒì¼ ìƒì„±
```bash
# RTX 2060ìš© ìµœì í™”ëœ highperf ì„¤ì •
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/train_highperf.yaml \
    --gpu-memory 6 \
    --aggressive-optimization \
    --save-config configs/train_rtx2060_highperf.yaml

# GTX 1660ìš© ê·¹í•œ ìµœì í™” ì„¤ì •  
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/train_highperf.yaml \
    --gpu-memory 6 \
    --memory-safety-margin 0.15 \
    --save-config configs/train_gtx1660_highperf.yaml
```

### âš¡ ì‹¤ì‹œê°„ ìµœì í™” ë„êµ¬

#### GPU ìƒíƒœ ê¸°ë°˜ ë™ì  ì¡°ì •
```bash
# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§ ë° ìë™ ì¡°ì •
python src/optimization/adaptive_training.py \
    --config configs/train_highperf.yaml \
    --monitor_memory_usage \
    --auto_adjust_batch_size \
    --target_memory_usage 0.85 \
    --mode highperf
```

> **ğŸ”¥ Pro Tip**: LOW-END GPUë„ ì ì ˆí•œ ìµœì í™”ë¡œ `highperf` ëª¨ë“œì—ì„œ **2ë°° ì´ìƒ ì„±ëŠ¥ í–¥ìƒ** ê°€ëŠ¥!

## ğŸ“Š Team TTA ì‹œìŠ¤í…œ GPU ìµœì í™” ì™„ì „ ê°€ì´ë“œ

### ğŸ¯ TTA íƒ€ì…ë³„ GPU ìš”êµ¬ì‚¬í•­

| TTA íƒ€ì… | ë³€í™˜ ìˆ˜ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | RTX 4090 | RTX 3080 | RTX 3060 | ê¶Œì¥ GPU |
|---------|--------|------------|----------|----------|----------|----------|
| **Essential** | 5ê°€ì§€ | ê¸°ë³¸ Ã— 5 | âœ… 64 batch | âœ… 32 batch | âœ… 16 batch | RTX 3060+ |
| **Comprehensive** | 15ê°€ì§€ | ê¸°ë³¸ Ã— 15 | âœ… 48 batch | âš ï¸ 16 batch | âŒ ë¶ˆê°€ | RTX 3080+ |
| Legacy (íšŒì „) | 3ê°€ì§€ | ê¸°ë³¸ Ã— 3 | âœ… 96 batch | âœ… 48 batch | âœ… 24 batch | ëª¨ë“  GPU |

### ğŸš€ GPUë³„ Team TTA ìµœì í™” ëª…ë ¹ì–´

#### RTX 4090 (24GB) - Comprehensive TTA ìµœê³  ì„±ëŠ¥
```bash
# í•™ìŠµ: Team ê³ ì„±ëŠ¥ ì„¤ì •
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf

# ì¶”ë¡ : Comprehensive TTA (F1: 0.965+)
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --mode highperf \
    --fold-results experiments/train/lastest-train/fold_results.yaml

# configs/infer_highperf.yaml ì„¤ì •:
# inference:
#   tta: true
#   tta_type: "comprehensive"  # 15ê°€ì§€ ë³€í™˜, 50ë¶„+
```

#### RTX 3080 (10GB) - Essential TTA ê· í˜• ì„±ëŠ¥
```bash
# í•™ìŠµ: ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf

# ì¶”ë¡ : Essential TTA (F1: 0.945-0.950)
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --mode highperf \
    --fold-results experiments/train/lastest-train/fold_results.yaml

# configs/infer_highperf.yaml ì„¤ì •:
# train:
#   batch_size: 32  # RTX 3080 ìµœì í™”
# inference:
#   tta: true
#   tta_type: "essential"  # 5ê°€ì§€ ë³€í™˜, 17ë¶„
```

#### RTX 3060 (8GB) - Essential TTA ë©”ëª¨ë¦¬ ìµœì í™”
```bash
# í•™ìŠµ: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  í™œìš©
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf

# ì¶”ë¡ : Essential TTA ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --mode highperf \
    --fold-results experiments/train/lastest-train/fold_results.yaml

# configs/infer_highperf.yaml ì„¤ì •:
# train:
#   batch_size: 16  # RTX 3060 ìµœì í™”
# inference:
#   tta: true
#   tta_type: "essential"  # 5ê°€ì§€ ë³€í™˜, 23ë¶„ (ë©”ëª¨ë¦¬ ì œì•½)
```

### âš¡ GPU ìë™ ìµœì í™” ë„êµ¬

#### Team GPU í™˜ê²½ ì²´í¬ ë° ê¶Œì¥ ì„¤ì •
```bash
# Team í™˜ê²½ì— ë§ëŠ” GPU ë¶„ì„
python src/utils/gpu_optimization/team_gpu_check.py \
    --analyze-team-performance \
    --recommend-tta-type \
    --model convnext_base_384

# ConvNeXt ëª¨ë¸ ê¸°ì¤€ ë°°ì¹˜ í¬ê¸° ìµœì í™”
python src/utils/gpu_optimization/auto_batch_size.py \
    --config configs/train_highperf.yaml \
    --model-type convnext \
    --image-size 384 \
    --safety-factor 0.9
```

#### TTA íƒ€ì… ìë™ ì„ íƒ
```bash
# GPU ë©”ëª¨ë¦¬ ê¸°ì¤€ ìµœì  TTA íƒ€ì… ì¶”ì²œ
python src/utils/gpu_optimization/recommend_tta.py \
    --config configs/infer_highperf.yaml \
    --target-time 20  # 20ë¶„ ë‚´ ì™„ë£Œ ëª©í‘œ
    --min-f1-score 0.945  # ìµœì†Œ F1 ìŠ¤ì½”ì–´ ìš”êµ¬ì‚¬í•­

# ê²°ê³¼ ì˜ˆì‹œ:
# RTX 4090: "comprehensive" (F1: 0.965+, 50ë¶„+)
# RTX 3080: "essential" (F1: 0.945-0.950, 17ë¶„)
# RTX 3060: "essential" (F1: 0.940-0.945, 23ë¶„)
```

## âš¡ ìë™ ìµœì í™”

### GPU ì„±ëŠ¥ ì²´í¬
```bash
python src/utils/gpu_optimization/team_gpu_check.py
```

### ë°°ì¹˜ í¬ê¸° ìë™ ìµœì í™”
```bash
# í…ŒìŠ¤íŠ¸ë§Œ (ì„¤ì • íŒŒì¼ ë³€ê²½ ì•ˆí•¨)
python src/utils/gpu_optimization/auto_batch_size.py --config configs/train_highperf.yaml --test-only

# ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
python src/utils/gpu_optimization/auto_batch_size.py --config configs/train_highperf.yaml
```

## ğŸ¯ ìµœì í™” íŒ

### HIGH-END GPU (RTX 4090+)
```bash
# ìµœê³  ì„±ëŠ¥ ì„¤ì •
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf \
    --use-calibration
```

### MID-RANGE GPU (RTX 3070+)
```bash
# ì•ˆì •ì  ì„±ëŠ¥
python src/training/train_main.py \
    --config configs/train.yaml \
    --mode basic
```

### LOW-END GPU (8GB ë¯¸ë§Œ)
```bash
# ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ
export CUDA_VISIBLE_DEVICES=0
python src/training/train_main.py \
    --config configs/train.yaml \
    --mode basic \
    --batch-size 16
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# í˜„ì¬ GPU ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi

# ë©”ëª¨ë¦¬ ì •ë¦¬
python -c "import torch; torch.cuda.empty_cache(); print('ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ')"
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
# GPU ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# í•™ìŠµ ì§„í–‰ë¥  í™•ì¸
tail -f logs/$(date +%Y%m%d)/train/*.log
```
