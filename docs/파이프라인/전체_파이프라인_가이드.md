# ğŸŒŸ ì „ì²´ Team ConvNeXt íŒŒì´í”„ë¼ì¸ ì™„ì „ ê°€ì´ë“œ (F1: 0.9652)

## ğŸ—ï¸ Team ConvNeXt 0.9652 ë‹¬ì„± ì•„í‚¤í…ì²˜

```mermaid
graph TD
    subgraph "ğŸ“Š ë°ì´í„° ë‹¨ê³„"
        A[data/raw/<br/>ì›ë³¸ ë°ì´í„°<br/>ì´ë¯¸ì§€ + ë©”íƒ€ë°ì´í„°]
        B[src/data/dataset.py<br/>ë°ì´í„° ë¡œë”<br/>ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸]
    end
    
    subgraph "ğŸ† Team ConvNeXt í•™ìŠµ"
        C[src/training/train_main.py<br/>--mode highperf<br/>ConvNeXt Base 384 + Hard Aug]
        D[configs/train_highperf.yaml<br/>Team ìµœì  ì„¤ì •<br/>Temperature Scaling]
        E[ConvNeXt Base 384<br/>ImageNet-22k ì‚¬ì „í•™ìŠµ<br/>5-Fold CV]
    end
    
    subgraph "ğŸ† Team TTA ì¶”ë¡ "
        F[src/inference/infer_main.py<br/>--mode highperf<br/>Essential/Comprehensive TTA]
        G[configs/infer_highperf.yaml<br/>tta_type: essential/comprehensive<br/>5ê°€ì§€/15ê°€ì§€ ë³€í™˜]
    end
    
    subgraph "ğŸ“ˆ ìµœì í™” ë‹¨ê³„"
        H[src/optimization/<br/>í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹<br/>Optuna ê¸°ë°˜]
        I[src/calibration/<br/>ëª¨ë¸ ë³´ì •<br/>ì‹ ë¢°ë„ ì¡°ì •]
    end
    
    subgraph "ğŸ’¾ ê²°ê³¼ ì €ì¥"
        J[experiments/train/<br/>í•™ìŠµëœ ëª¨ë¸<br/>ì²´í¬í¬ì¸íŠ¸]
        K[submissions/<br/>Team ì œì¶œ íŒŒì¼<br/>F1: 0.9652 CSV]
        L[logs/<br/>í•™ìŠµ/ì¶”ë¡  ë¡œê·¸<br/>ì„±ëŠ¥ ê¸°ë¡]
    end
    
    A --> B
    B --> C
    D --> C
    C --> E
    E --> J
    
    J --> F
    G --> F
    F --> K
    
    C --> H
    H --> I
    I --> F
    
    C --> L
    F --> L
    
    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style F fill:#e8f5e8
    style H fill:#fff3e0
    style J fill:#ffebee
```

## ğŸ”€ íŒŒì´í”„ë¼ì¸ íë¦„ë„

```mermaid
flowchart TD
    subgraph Phase1 ["Phase 1: ë°ì´í„° ì¤€ë¹„"]
        direction LR
        A1["ğŸ“ ì›ë³¸ ë°ì´í„°<br/>ë¡œë”©"]
        A2["ğŸ” ë°ì´í„°<br/>íƒìƒ‰"]
        A3["âš™ï¸ ì „ì²˜ë¦¬<br/>ì„¤ì •"]
        A1 --> A2 --> A3
    end
    
    subgraph Phase2 ["Phase 2: ëª¨ë¸ í•™ìŠµ"]
        direction LR
        B1["ğŸ”€ Fold<br/>ë¶„í• "]
        B2["ğŸ¯ ëª¨ë¸<br/>í•™ìŠµ"]
        B3["ğŸ“Š ì„±ëŠ¥<br/>í‰ê°€"]
        B4["ğŸ’¾ ëª¨ë¸<br/>ì €ì¥"]
        B1 --> B2 --> B3 --> B4
    end
    
    subgraph Phase3 ["Phase 3: ëª¨ë¸ ìµœì í™”"]
        direction LR
        C1["ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„°<br/>íŠœë‹"]
        C2["ğŸ“ ëª¨ë¸<br/>ë³´ì •"]
        C3["ğŸ† ìµœì  ëª¨ë¸<br/>ì„ íƒ"]
        C1 --> C2 --> C3
    end
    
    subgraph Phase4 ["Phase 4: ì¶”ë¡  ë° ì œì¶œ"]
        direction LR
        D1["ğŸ”® ëª¨ë¸<br/>ì¶”ë¡ "]
        D2["ğŸ“Š ê²°ê³¼<br/>í›„ì²˜ë¦¬"]
        D3["ğŸ“ ì œì¶œ íŒŒì¼<br/>ìƒì„±"]
        D1 --> D2 --> D3
    end
    
    %% ì„œë¸Œê·¸ë˜í”„ ê°„ ì„¸ë¡œ ì—°ê²°
    Phase1 --> Phase2
    Phase2 --> Phase3
    Phase3 --> Phase4
    
    style A1 fill:#e1f5fe
    style B2 fill:#f3e5f5
    style C1 fill:#e8f5e8
    style D1 fill:#fff3e0
    style D3 fill:#ffebee
```

## ğŸ“ íŒŒì¼ ê°„ ì˜ì¡´ ê´€ê³„ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
flowchart TD
    subgraph "âš™ï¸ ì„¤ì • ê´€ë¦¬"
        direction LR
        CONFIG1[configs/train.yaml<br/>ê¸°ë³¸ í•™ìŠµ ì„¤ì •]
        CONFIG2[configs/train_highperf.yaml<br/>ê³ ì„±ëŠ¥ í•™ìŠµ ì„¤ì •]
        CONFIG3[configs/infer.yaml<br/>ê¸°ë³¸ ì¶”ë¡  ì„¤ì •]
        CONFIG4[configs/infer_highperf.yaml<br/>ê³ ì„±ëŠ¥ ì¶”ë¡  ì„¤ì •]
        CONFIG5[configs/optuna_config.yaml<br/>í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì„¤ì •]
    end
    
    subgraph "ğŸ§  í•µì‹¬ íŒŒì´í”„ë¼ì¸"
        direction LR
        TRAIN_MAIN[src/training/train_main.py<br/>í•™ìŠµ í†µí•© ì¸í„°í˜ì´ìŠ¤<br/>CLI íŒŒì‹± ë° ì‹¤í–‰ ê´€ë¦¬]
        INFER_MAIN[src/inference/infer_main.py<br/>ì¶”ë¡  í†µí•© ì¸í„°í˜ì´ìŠ¤<br/>ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡]
        OPTUNA_MAIN[src/optimization/optuna_optimize.py<br/>ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹<br/>ì„±ëŠ¥ ìµœì í™” ê´€ë¦¬]
    end
    
    subgraph "ğŸ”§ êµ¬í˜„ ë ˆì´ì–´"
        direction LR
        TRAIN[src/training/train.py<br/>í•µì‹¬ í•™ìŠµ ë¡œì§<br/>5-Fold CV êµ¬í˜„]
        INFER[src/inference/infer.py<br/>í•µì‹¬ ì¶”ë¡  ë¡œì§<br/>ì•™ìƒë¸” ì˜ˆì¸¡ êµ¬í˜„]
        MODELS[src/models/build.py<br/>ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¹Œë”<br/>ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ ê´€ë¦¬]
        DATA[src/data/dataset.py<br/>ë°ì´í„° ë¡œë”<br/>ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸]
        CALIB[src/calibration/calibrate.py<br/>ëª¨ë¸ ë³´ì •<br/>ì‹ ë¢°ë„ ì¡°ì •]
    end
    
    subgraph "ğŸ“Š ë°ì´í„° ì†ŒìŠ¤"
        direction LR
        TRAIN_DATA[data/raw/train/<br/>í•™ìŠµ ì´ë¯¸ì§€ ë°ì´í„°]
        TEST_DATA[data/raw/test/<br/>í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë°ì´í„°]
        META[data/raw/meta.csv<br/>ë©”íƒ€ë°ì´í„° ë° ë¼ë²¨]
    end
    
    subgraph "ğŸ› ï¸ ìœ í‹¸ë¦¬í‹°"
        direction TB
        UTILS[src/utils/common.py<br/>ê³µí†µ ìœ í‹¸ë¦¬í‹°<br/>ë¡œê¹… ë° í—¬í¼ í•¨ìˆ˜]
        METRICS[src/metrics/evaluate.py<br/>ì„±ëŠ¥ í‰ê°€<br/>F1-Score, Accuracy]
        LOGGING[src/logging/logger.py<br/>ë¡œê·¸ ê´€ë¦¬<br/>ì‹¤í—˜ ì¶”ì ]
    end
    
    subgraph "ğŸ’¾ ê²°ê³¼ ì €ì¥ì†Œ"
        direction TB
        EXPERIMENTS[experiments/<br/>ì‹¤í—˜ ê²°ê³¼ ì €ì¥ì†Œ]
        TRAIN_EXP[experiments/train/YYYYMMDD/<br/>í•™ìŠµ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸]
        OPT_EXP[experiments/optimization/<br/>ìµœì í™” ê²°ê³¼]
        SUBMISSIONS[submissions/YYYYMMDD/<br/>ì œì¶œ íŒŒì¼]
        LOGS_DIR[logs/YYYYMMDD/<br/>ë¡œê·¸ íŒŒì¼]
        WANDB[wandb/<br/>ì‹¤í—˜ ì¶”ì  ë°ì´í„°]
    end
    
    
    %% ì„¤ì • íŒŒì¼ ì—°ê²°
    CONFIG1 --> TRAIN_MAIN
    CONFIG2 --> TRAIN_MAIN
    CONFIG3 --> INFER_MAIN
    CONFIG4 --> INFER_MAIN
    CONFIG5 --> OPTUNA_MAIN
    
    %% ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ ì—°ê²°
    TRAIN_MAIN --> TRAIN
    INFER_MAIN --> INFER
    OPTUNA_MAIN --> TRAIN
    
    %% êµ¬í˜„ ë ˆì´ì–´ ì—°ê²°
    TRAIN --> MODELS
    TRAIN --> DATA
    TRAIN --> CALIB
    INFER --> MODELS
    INFER --> DATA
    
    %% ìœ í‹¸ë¦¬í‹° ì—°ê²°
    TRAIN --> UTILS
    TRAIN --> METRICS
    TRAIN --> LOGGING
    INFER --> UTILS
    INFER --> METRICS
    INFER --> LOGGING
    
    %% ë°ì´í„° ì—°ê²°
    TRAIN_DATA --> DATA
    TEST_DATA --> DATA
    META --> DATA
    
    %% ê²°ê³¼ ì €ì¥ ì—°ê²°
    TRAIN --> TRAIN_EXP
    TRAIN --> LOGS_DIR
    TRAIN --> WANDB
    INFER --> SUBMISSIONS
    INFER --> LOGS_DIR
    OPTUNA_MAIN --> OPT_EXP
    
    TRAIN_EXP --> EXPERIMENTS
    OPT_EXP --> EXPERIMENTS
    
    style CONFIG1 fill:#e1f5fe
    style TRAIN_MAIN fill:#f3e5f5
    style INFER_MAIN fill:#e8f5e8
    style OPTUNA_MAIN fill:#fff3e0
    style EXPERIMENTS fill:#ffebee
```

### ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡° ë° íŒŒì¼ ìƒì„¸ ì„¤ëª…

```
ğŸŒŸ Computer Vision Competition ML Pipeline
â”œâ”€â”€ configs/                               # âš™ï¸ ì„¤ì • íŒŒì¼ ëª¨ìŒ
â”‚   â”œâ”€â”€ train.yaml                         # ê¸°ë³¸ í•™ìŠµ ì„¤ì •
â”‚   â”œâ”€â”€ train_highperf.yaml                # ê³ ì„±ëŠ¥ í•™ìŠµ ì„¤ì •
â”‚   â”œâ”€â”€ train_fast_optimized.yaml          # ë¹ ë¥¸ í•™ìŠµ ì„¤ì •
â”‚   â”œâ”€â”€ infer.yaml                         # ê¸°ë³¸ ì¶”ë¡  ì„¤ì •
â”‚   â”œâ”€â”€ infer_highperf.yaml                # ê³ ì„±ëŠ¥ ì¶”ë¡  ì„¤ì •
â”‚   â”œâ”€â”€ optuna_config.yaml                 # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì„¤ì •
â”‚   â””â”€â”€ optuna_fast_config.yaml            # ë¹ ë¥¸ íŠœë‹ ì„¤ì •
â”‚
â”œâ”€â”€ src/                                   # ğŸ§  í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ training/                          # ğŸ“ í•™ìŠµ ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ train_main.py                  # í•™ìŠµ ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
â”‚   â”‚   â””â”€â”€ train.py                       # í•µì‹¬ í•™ìŠµ ë¡œì§
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/                         # ğŸ”® ì¶”ë¡  ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ infer_main.py                  # ì¶”ë¡  ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
â”‚   â”‚   â””â”€â”€ infer.py                       # í•µì‹¬ ì¶”ë¡  ë¡œì§
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                            # ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜
â”‚   â”‚   â”œâ”€â”€ build.py                       # ëª¨ë¸ ë¹Œë”
â”‚   â”‚   â””â”€â”€ backbones/                     # ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ ëª¨ìŒ
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                              # ğŸ“Š ë°ì´í„° ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ dataset.py                     # ë°ì´í„°ì…‹ ë° ë¡œë”
â”‚   â”‚   â””â”€â”€ transforms.py                  # ë°ì´í„° ë³€í™˜ ë° ì¦ê°•
â”‚   â”‚
â”‚   â”œâ”€â”€ optimization/                      # ğŸ“ˆ ìµœì í™” ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ optuna_optimize.py             # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
â”‚   â”‚   â””â”€â”€ hyperparameter_search.py       # íƒìƒ‰ ì „ëµ
â”‚   â”‚
â”‚   â”œâ”€â”€ calibration/                       # ğŸ“ ëª¨ë¸ ë³´ì •
â”‚   â”‚   â”œâ”€â”€ calibrate.py                   # ëª¨ë¸ ë³´ì • ë¡œì§
â”‚   â”‚   â””â”€â”€ temperature_scaling.py         # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics/                           # ğŸ“Š ì„±ëŠ¥ í‰ê°€
â”‚   â”‚   â”œâ”€â”€ evaluate.py                    # í‰ê°€ ì§€í‘œ ê³„ì‚°
â”‚   â”‚   â””â”€â”€ visualizations/                # ê²°ê³¼ ì‹œê°í™”
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                             # ğŸ› ï¸ ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”œâ”€â”€ common.py                      # ê³µí†µ í•¨ìˆ˜
â”‚   â”‚   â”œâ”€â”€ auto_batch_size.py             # GPU ë©”ëª¨ë¦¬ ìµœì í™”
â”‚   â”‚   â””â”€â”€ team_gpu_check.py              # GPU ìƒíƒœ í™•ì¸
â”‚   â”‚
â”‚   â””â”€â”€ logging/                           # ğŸ“ ë¡œê¹… ì‹œìŠ¤í…œ
â”‚       â”œâ”€â”€ logger.py                      # ë¡œê·¸ ê´€ë¦¬
â”‚       â””â”€â”€ wandb_integration.py           # WandB ì—°ë™
â”‚
â”œâ”€â”€ data/                                  # ğŸ“ ë°ì´í„° ì €ì¥ì†Œ
â”‚   â””â”€â”€ raw/                               # ì›ë³¸ ë°ì´í„°
â”‚       â”œâ”€â”€ train/                         # í•™ìŠµ ì´ë¯¸ì§€
â”‚       â”œâ”€â”€ test/                          # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
â”‚       â”œâ”€â”€ train.csv                      # í•™ìŠµ ë°ì´í„° ë©”íƒ€ì •ë³´
â”‚       â””â”€â”€ sample_submission.csv          # ì œì¶œ í˜•ì‹ ì˜ˆì‹œ
â”‚
â”œâ”€â”€ experiments/                           # ğŸ§ª ì‹¤í—˜ ê²°ê³¼
â”‚   â”œâ”€â”€ train/                             # í•™ìŠµ ì‹¤í—˜
â”‚   â”‚   â””â”€â”€ YYYYMMDD/                      # ë‚ ì§œë³„ ì‹¤í—˜
â”‚   â”‚       â””â”€â”€ model_YYYYMMDD_HHMM/
â”‚   â”‚           â”œâ”€â”€ ckpt/                  # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”‚   â”‚           â”œâ”€â”€ config.yaml            # ì‹¤í—˜ ì„¤ì •
â”‚   â”‚           â””â”€â”€ metrics.json           # ì„±ëŠ¥ ì§€í‘œ
â”‚   â”‚
â”‚   â””â”€â”€ optimization/                      # ìµœì í™” ì‹¤í—˜
â”‚       â””â”€â”€ YYYYMMDD/                      # ë‚ ì§œë³„ ìµœì í™” ê²°ê³¼
â”‚
â”œâ”€â”€ submissions/                           # ğŸ“¤ ì œì¶œ íŒŒì¼
â”‚   â””â”€â”€ YYYYMMDD/                          # ë‚ ì§œë³„ ì œì¶œ
â”‚       â”œâ”€â”€ single_model_*.csv
â”‚       â”œâ”€â”€ ensemble_*.csv
â”‚       â””â”€â”€ highperf_ensemble_*.csv
â”‚
â”œâ”€â”€ logs/                                  # ğŸ“‹ ë¡œê·¸ íŒŒì¼
â”‚   â””â”€â”€ YYYYMMDD/                          # ë‚ ì§œë³„ ë¡œê·¸
â”‚       â”œâ”€â”€ train/                         # í•™ìŠµ ë¡œê·¸
â”‚       â””â”€â”€ infer/                         # ì¶”ë¡  ë¡œê·¸
â”‚
â”œâ”€â”€ wandb/                                 # ğŸ“Š ì‹¤í—˜ ì¶”ì 
â”‚   â””â”€â”€ run-*/                             # WandB ì‹¤í–‰ ê¸°ë¡
â”‚
â”œâ”€â”€ scripts/                               # ğŸš€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ run_fast_training.sh               # ë¹ ë¥¸ í•™ìŠµ ì‹¤í–‰
â”‚   â”œâ”€â”€ run_highperf_training.sh           # ê³ ì„±ëŠ¥ í•™ìŠµ ì‹¤í–‰
â”‚   â”œâ”€â”€ monitor_training.sh                # í•™ìŠµ ëª¨ë‹ˆí„°ë§
â”‚   â””â”€â”€ update_inference_date.sh           # ì¶”ë¡  ë‚ ì§œ ì—…ë°ì´íŠ¸
â”‚
â”œâ”€â”€ notebooks/                             # ğŸ““ Jupyter ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ base/                              # ê¸°ë³¸ ë¶„ì„ ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ modular/                           # ëª¨ë“ˆë³„ í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ë¶
â”‚   â””â”€â”€ team/                              # íŒ€ ê³µìœ  ë…¸íŠ¸ë¶
â”‚
â”œâ”€â”€ font/                                   # ğŸ¨ í°íŠ¸ íŒŒì¼
â”‚   â””â”€â”€ NanumGothic.ttf                    # í•œê¸€ í°íŠ¸
â”‚
â”œâ”€â”€ submissions/                           # ğŸ“¤ ì œì¶œ íŒŒì¼
â”‚   â””â”€â”€ YYYYMMDD/                          # ë‚ ì§œë³„ ì œì¶œ
â”‚
â”œâ”€â”€ wandb/                                 # ğŸ“ˆ ì‹¤í—˜ ì¶”ì 
â”‚   â””â”€â”€ runs/                              # WandB ì‹¤í–‰ ê¸°ë¡
â”‚
â””â”€â”€ docs/                                  # ğŸ“š ë¬¸ì„œ
    â”œâ”€â”€ configs_í´ë”_ì„¤ì • íŒŒì¼_ìƒì„±/          # ì„¤ì • íŒŒì¼ ê°€ì´ë“œ
    â”œâ”€â”€ ëª¨ë¸/                              # ëª¨ë¸ ê´€ë ¨ ë¬¸ì„œ
    â”œâ”€â”€ ì‹œìŠ¤í…œ/                            # ì‹œìŠ¤í…œ ê´€ë ¨ ë¬¸ì„œ  
    â”œâ”€â”€ ìµœì í™”/                            # ìµœì í™” ê´€ë ¨ ë¬¸ì„œ
    â””â”€â”€ íŒŒì´í”„ë¼ì¸/                        # íŒŒì´í”„ë¼ì¸ ê´€ë ¨ ë¬¸ì„œ
```

#### ğŸ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ìƒì„¸ ì„¤ëª…

**1. ğŸ“Š ë°ì´í„° íŒŒì´í”„ë¼ì¸**
- **src/data/dataset.py**: ì´ë¯¸ì§€ ë¡œë”©, ì „ì²˜ë¦¬, ë°°ì¹˜ ìƒì„± ê´€ë¦¬
- **src/data/transforms.py**: Albumentations ê¸°ë°˜ ë°ì´í„° ì¦ê°• ì „ëµ
- **ê¸°ëŠ¥**: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬, ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©, GPU ìµœì í™”

**2. ğŸ“ í•™ìŠµ íŒŒì´í”„ë¼ì¸**
- **src/training/train_main.py**: CLI ì¸í„°í˜ì´ìŠ¤, ì„¤ì • ê´€ë¦¬, ì‹¤í–‰ ì œì–´
- **src/training/train.py**: 5-Fold CV, Early Stopping, LR Scheduling
- **ì˜ì¡´ì„±**: models/build.py â†’ ëª¨ë¸ ìƒì„±, data/dataset.py â†’ ë°ì´í„° ë¡œë”©

**3. ğŸ”® ì¶”ë¡  íŒŒì´í”„ë¼ì¸**
- **src/inference/infer_main.py**: ì¶”ë¡  ëª¨ë“œ ì„ íƒ, ê²°ê³¼ íŒŒì¼ ê´€ë¦¬
- **src/inference/infer.py**: ì•™ìƒë¸” ì˜ˆì¸¡, TTA, ì‹ ë¢°ë„ ê³„ì‚°
- **ì¶œë ¥**: submissions/YYYYMMDD/ ë””ë ‰í† ë¦¬ì— CSV íŒŒì¼ ìƒì„±

**4. ğŸ“ˆ ìµœì í™” íŒŒì´í”„ë¼ì¸**
- **src/optimization/optuna_optimize.py**: Optuna ê¸°ë°˜ ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- **src/calibration/calibrate.py**: Temperature Scaling ê¸°ë°˜ ëª¨ë¸ ë³´ì •
- **ê²°ê³¼**: experiments/optimization/ ë””ë ‰í† ë¦¬ì— ìµœì í™” ê²°ê³¼ ì €ì¥

**5. ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° ì‹œìŠ¤í…œ**
- **src/utils/auto_batch_size.py**: GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë™ì  ë°°ì¹˜ í¬ê¸° ê²°ì •
- **src/utils/team_gpu_check.py**: GPU í™˜ê²½ í™•ì¸ ë° ìµœì í™” ì¶”ì²œ
- **src/logging/logger.py**: í†µí•© ë¡œê¹… ì‹œìŠ¤í…œ, WandB ì—°ë™

## ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê°€ì´ë“œ

### 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
```bash
# Python í™˜ê²½ í™œì„±í™”
eval "$(pyenv init --path)" && pyenv activate cv_py3_11_9

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source path.env

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
cd /path/to/computer-vision-competition-1SEN
```

### 2. ë°ì´í„° ì¤€ë¹„ ë° íƒìƒ‰
```bash
# ë°ì´í„° êµ¬ì¡° í™•ì¸
ls -la data/raw/
echo "í•™ìŠµ ì´ë¯¸ì§€ ìˆ˜: $(ls data/raw/train/ | wc -l)"
echo "í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìˆ˜: $(ls data/raw/test/ | wc -l)"

# ë©”íƒ€ë°ì´í„° í™•ì¸
head -10 data/raw/meta.csv
wc -l data/raw/meta.csv

# í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„
python -c "
import pandas as pd
df = pd.read_csv('data/raw/meta.csv')
print('=== í´ë˜ìŠ¤ ë¶„í¬ ===')
print(df['label'].value_counts().sort_index())
print(f'\nì´ ì´ë¯¸ì§€: {len(df)}ê°œ')
print(f'í´ë˜ìŠ¤ ìˆ˜: {df[\"label\"].nunique()}ê°œ')
print(f'í´ë˜ìŠ¤ë³„ í‰ê· : {len(df)/df[\"label\"].nunique():.1f}ê°œ')
"

# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
python -c "
import os
from PIL import Image
import pandas as pd

df = pd.read_csv('data/raw/meta.csv')
corrupted = []
for idx, row in df.iterrows():
    img_path = f'data/raw/train/{row[\"image_id\"]}'
    try:
        img = Image.open(img_path)
        img.verify()
    except:
        corrupted.append(img_path)

print(f'ì†ìƒëœ ì´ë¯¸ì§€: {len(corrupted)}ê°œ')
if corrupted: print(corrupted[:5])
"
```

### 3. GPU í™˜ê²½ ìµœì í™”
```bash
# GPU ìƒíƒœ í™•ì¸
nvidia-smi
python src/utils/team_gpu_check.py

# ìë™ ë°°ì¹˜ í¬ê¸° ê²°ì • (í•™ìŠµìš©)
python src/utils/auto_batch_size.py --config configs/train.yaml
python src/utils/auto_batch_size.py --config configs/train_highperf.yaml
python src/utils/auto_batch_size.py --config configs/train_fast_optimized.yaml

# ìë™ ë°°ì¹˜ í¬ê¸° ê²°ì • (ì¶”ë¡ ìš©)
python src/utils/auto_batch_size.py --config configs/infer.yaml
python src/utils/auto_batch_size.py --config configs/infer_highperf.yaml
```

### 4. ë‹¨ê³„ë³„ í•™ìŠµ íŒŒì´í”„ë¼ì¸

#### Phase 1: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… (30ë¶„)
```bash
# ë¹ ë¥¸ í•™ìŠµìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¶•
python src/training/train_main.py --config configs/train_fast_optimized.yaml --mode fast

# ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
bash scripts/run_fast_training.sh

# íŠ¹ì • foldë§Œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
python src/training/train_main.py --config configs/train_fast_optimized.yaml --fold 0
```

#### Phase 2: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (1-2ì‹œê°„)
```bash
# Optuna ìë™ íŠœë‹ (ë¹ ë¥¸ ë²„ì „)
python src/optimization/optuna_optimize.py \
    --config configs/optuna_fast_config.yaml \
    --n-trials 20 \
    --timeout 1800

# Optuna ìë™ íŠœë‹ (ì •ë°€ ë²„ì „)
python src/optimization/optuna_optimize.py \
    --config configs/optuna_config.yaml \
    --n-trials 50 \
    --timeout 3600

# í•™ìŠµê³¼ ìµœì í™” í†µí•©
python src/training/train_main.py \
    --config configs/train.yaml \
    --optimize \
    --n-trials 30
```

#### Phase 3: ê³ ì„±ëŠ¥ í•™ìŠµ (2-3ì‹œê°„)
```bash
# ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ì •ë°€ í•™ìŠµ
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
bash scripts/run_highperf_training.sh

# ì¤‘ë‹¨ëœ í•™ìŠµ ìë™ ì¬ê°œ
python src/training/train_main.py --config configs/train_highperf.yaml --auto-continue

# íŠ¹ì • fold ì¬í•™ìŠµ
python src/training/train_main.py --config configs/train_highperf.yaml --resume --fold 2,3,4
```

#### Phase 4: ëª¨ë¸ ë³´ì • (30ë¶„)
```bash
# Temperature Scaling ë³´ì •
python src/calibration/calibrate.py \
    --model-dir experiments/train/$(date +%Y%m%d) \
    --calibration-data data/raw/train \
    --output-dir experiments/calibration/

# ë³´ì • í¬í•¨ ì¬í•™ìŠµ
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --use-calibration
```

### 5. ë‹¨ê³„ë³„ ì¶”ë¡  íŒŒì´í”„ë¼ì¸

#### ë¹ ë¥¸ ê²€ì¦ ì¶”ë¡  (5ë¶„)
```bash
# ìµœì‹  ëª¨ë¸ë¡œ ë¹ ë¥¸ ì¶”ë¡ 
LASTEST_MODEL=$(find experiments/train -name "best_fold0.pth" | sort | tail -1)
python src/inference/infer_main.py \
    --config configs/infer.yaml \
    --mode basic \
    --ckpt $LASTEST_MODEL
```

#### ì¤‘ê°„ ì„±ëŠ¥ ì¶”ë¡  (10ë¶„)
```bash
# ê¸°ë³¸ ì•™ìƒë¸” ì¶”ë¡ 
python src/inference/infer_main.py \
    --config configs/infer.yaml \
    --mode ensemble

# TTA í¬í•¨ ë‹¨ì¼ ëª¨ë¸
python src/inference/infer_main.py \
    --config configs/infer.yaml \
    --mode basic \
    --use-tta
```

#### ìµœê³  ì„±ëŠ¥ ì¶”ë¡  (20-30ë¶„)
```bash
# 5-Fold ì•™ìƒë¸” + TTA
LASTEST_FOLD=$(find experiments/train -name "fold_results" | sort | tail -1)
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --mode highperf \
    --use-tta \
    --fold-results $LASTEST_FOLD

# ê°€ì¤‘ì¹˜ ì•™ìƒë¸” (ìµœì¢… ì œì¶œìš©)
python src/inference/infer_main.py \
    --config configs/infer_highperf.yaml \
    --mode ensemble \
    --fold-results $LASTEST_FOLD \
    --ensemble-weights 0.3,0.25,0.2,0.15,0.1 \
    --use-tta
```

### 6. í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

#### ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰
```bash
# 1ì¼ì°¨: í”„ë¡œí† íƒ€ì… + ìµœì í™”
bash scripts/day1_prototype.sh

# 2ì¼ì°¨: ê³ ì„±ëŠ¥ í•™ìŠµ
bash scripts/day2_training.sh

# 3ì¼ì°¨: ìµœì¢… ì¶”ë¡ 
bash scripts/day3_inference.sh
```

#### ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```bash
# ë¹ ë¥¸ ì „ì²´ íŒŒì´í”„ë¼ì¸ (2ì‹œê°„)
python pipeline_runner.py --mode fast --auto

# ì •ë°€ ì „ì²´ íŒŒì´í”„ë¼ì¸ (6ì‹œê°„)
python pipeline_runner.py --mode highperf --auto

# íŠ¹ì • ë‹¨ê³„ë§Œ ì‹¤í–‰
python pipeline_runner.py --steps train,infer --config custom_config.yaml
```

### 7. ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„

#### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```bash
# í•™ìŠµ ëª¨ë‹ˆí„°ë§
bash scripts/monitor_training.sh

# GPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
watch -n 2 "nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits"

# ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
tail -f logs/$(date +%Y%m%d)/*/*.log

# ë©€í‹° í„°ë¯¸ë„ ëª¨ë‹ˆí„°ë§
tmux new-session -d 'tail -f logs/$(date +%Y%m%d)/train/*.log'
tmux split-window -h 'watch -n 1 nvidia-smi'
tmux attach
```

#### ì„±ê³¼ ë¶„ì„
```bash
# í•™ìŠµ ì„±ê³¼ ìš”ì•½
python -c "
import json, glob, pandas as pd
results = []
for file in glob.glob('experiments/train/$(date +%Y%m%d)/*/metrics.json'):
    with open(file) as f:
        data = json.load(f)
        results.append({
            'model': file.split('/')[-2],
            'f1': data.get('best_f1', 0),
            'acc': data.get('best_accuracy', 0)
        })
df = pd.DataFrame(results).sort_values('f1', ascending=False)
print(df.to_string(index=False))
"

# ì¶”ë¡  ê²°ê³¼ ë¹„êµ
python -c "
import pandas as pd, glob
files = glob.glob('submissions/$(date +%Y%m%d)/*.csv')
for file in files:
    df = pd.read_csv(file)
    name = file.split('/')[-1].split('.')[0]
    dist = df.iloc[:, 1].value_counts().sort_index()
    print(f'\n{name}:')
    print(dist.to_string())
"
```

### 8. ê²°ê³¼ ê²€ì¦ ë° ì œì¶œ

#### ê²°ê³¼ ê²€ì¦
```bash
# ì œì¶œ íŒŒì¼ í˜•ì‹ ê²€ì¦
python -c "
import pandas as pd, glob
files = glob.glob('submissions/$(date +%Y%m%d)/*.csv')
for file in files:
    df = pd.read_csv(file)
    print(f'\n{file}:')
    print(f'  Shape: {df.shape}')
    print(f'  Columns: {df.columns.tolist()}')
    print(f'  Missing: {df.isnull().sum().sum()}')
    print(f'  Unique predictions: {df.iloc[:, 1].nunique()}')
"

# íŒŒì¼ í¬ê¸° ë° ê°œìˆ˜ í™•ì¸
echo "=== ì œì¶œ íŒŒì¼ í˜„í™© ==="
ls -lh submissions/$(date +%Y%m%d)/
echo "ì´ ì œì¶œ íŒŒì¼ ìˆ˜: $(ls submissions/$(date +%Y%m%d)/*.csv | wc -l)"
```

#### ìµœì¢… ì œì¶œ íŒŒì¼ ì„ íƒ
```bash
# ê°€ì¥ ìµœì‹  ê³ ì„±ëŠ¥ ì•™ìƒë¸” íŒŒì¼ ì„ íƒ
FINAL_SUBMISSION=$(ls -t submissions/$(date +%Y%m%d)/highperf_ensemble_*.csv | head -1)
echo "ìµœì¢… ì œì¶œ íŒŒì¼: $FINAL_SUBMISSION"

# ì œì¶œ íŒŒì¼ ë°±ì—…
cp "$FINAL_SUBMISSION" "submissions/final_submission_$(date +%Y%m%d_%H%M).csv"
```

### 9. ì‹¤í—˜ ê´€ë¦¬ ë° ë²„ì „ ê´€ë¦¬

#### ì‹¤í—˜ ì •ë¦¬
```bash
# ì˜¤ëŠ˜ì˜ ì‹¤í—˜ ìš”ì•½
python -c "
import os, glob
from datetime import datetime

today = datetime.now().strftime('%Y%m%d')
print(f'=== {today} ì‹¤í—˜ ìš”ì•½ ===')

train_exp = glob.glob(f'experiments/train/{today}/*')
print(f'í•™ìŠµ ì‹¤í—˜: {len(train_exp)}ê°œ')

infer_files = glob.glob(f'submissions/{today}/*.csv')
print(f'ì¶”ë¡  ê²°ê³¼: {len(infer_files)}ê°œ')

log_files = glob.glob(f'logs/{today}/*/*')
print(f'ë¡œê·¸ íŒŒì¼: {len(log_files)}ê°œ')
"

# ì‹¤í—˜ ì•„ì¹´ì´ë¸Œ
tar -czf experiments_$(date +%Y%m%d).tar.gz experiments/train/$(date +%Y%m%d)/
tar -czf submissions_$(date +%Y%m%d).tar.gz submissions/$(date +%Y%m%d)/
```

#### WandB ë™ê¸°í™”
```bash
# WandB ë¡œê·¸ì¸
wandb login

# ì‹¤í—˜ ê²°ê³¼ ë™ê¸°í™”
wandb sync wandb/

# í”„ë¡œì íŠ¸ ê²°ê³¼ í™•ì¸
wandb project list
```

### 10. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ëª…ë ¹ì–´

#### ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°
```bash
# GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
python src/utils/auto_batch_size.py --config configs/train.yaml --find-optimal
python src/training/train_main.py --config configs/train.yaml --batch-size 8

# í•™ìŠµ ì¤‘ë‹¨ ì‹œ ì¬ê°œ
python src/training/train_main.py --config configs/train.yaml --auto-continue

# ëª¨ë¸ íŒŒì¼ ì†ìƒ ì‹œ ë³µêµ¬
python -c "
import torch, glob
for file in glob.glob('experiments/train/$(date +%Y%m%d)/*/ckpt/*.pth'):
    try:
        torch.load(file, map_location='cpu')
        print(f'OK: {file}')
    except:
        print(f'ERROR: {file}')
"

# ë¡œê·¸ íŒŒì¼ ì •ë¦¬
find logs/ -name "*.log" -mtime +7 -delete
find wandb/ -name "run-*" -mtime +14 -exec rm -rf {} +
```

ì´ ëª…ë ¹ì–´ ëª¨ìŒì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ML íŒŒì´í”„ë¼ì¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ê¶Œì¥ ì›Œí¬í”Œë¡œìš°

### Phase 1: íƒìƒ‰ ë‹¨ê³„ (1ì¼ì°¨)
1. **ë°ì´í„° íƒìƒ‰**: í´ë˜ìŠ¤ ë¶„í¬, ì´ë¯¸ì§€ í’ˆì§ˆ í™•ì¸
2. **ë¹ ë¥¸ í•™ìŠµ**: `train_fast_optimized.yaml`ë¡œ ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¶•
3. **ì´ˆê¸° ì¶”ë¡ **: ê¸°ë³¸ ëª¨ë¸ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±

### Phase 2: ìµœì í™” ë‹¨ê³„ (2-3ì¼ì°¨)
1. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: Optunaë¡œ ìµœì  ì„¤ì • íƒìƒ‰
2. **ê³ ì„±ëŠ¥ í•™ìŠµ**: `train_highperf.yaml`ë¡œ ì •ë°€ í•™ìŠµ
3. **ëª¨ë¸ ë³´ì •**: Temperature Scalingìœ¼ë¡œ ì‹ ë¢°ë„ ê°œì„ 

### Phase 3: ìµœì¢… ì œì¶œ ë‹¨ê³„ (4-5ì¼ì°¨)
1. **ì•™ìƒë¸” ì¶”ë¡ **: 5-Fold ëª¨ë¸ ì¡°í•©
2. **TTA ì ìš©**: Test-Time Augmentationìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
3. **ê²°ê³¼ ê²€ì¦**: ìµœì¢… ì œì¶œ íŒŒì¼ í’ˆì§ˆ í™•ì¸

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° íŒ

### ğŸ”§ GPU ë©”ëª¨ë¦¬ ìµœì í™”
```bash
# GPU ìƒíƒœ í™•ì¸
python src/utils/team_gpu_check.py

# ìë™ ë°°ì¹˜ í¬ê¸° ê²°ì •
python src/utils/auto_batch_size.py --config configs/train_highperf.yaml
```

### ğŸ“Š ì‹¤í—˜ ì¶”ì 
```bash
# WandB ë¡œê·¸ì¸
wandb login

# ì‹¤í—˜ ê²°ê³¼ í™•ì¸
wandb sync wandb/
```

### ğŸš¨ ë¬¸ì œ í•´ê²°
```bash
# ë¡œê·¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
tail -f logs/$(date +%Y%m%d)/train/*.log

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# ëª¨ë¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬
python -c "
import torch
model_path = 'experiments/train/$(date +%Y%m%d)/*/ckpt/best_fold0.pth'
state_dict = torch.load(model_path, map_location='cpu')
print(f'ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {len(state_dict)}')
```

ì´ ê°€ì´ë“œëŠ” ë°ì´í„° ì¤€ë¹„ë¶€í„° ìµœì¢… ì œì¶œê¹Œì§€ì˜ ì „ì²´ ML íŒŒì´í”„ë¼ì¸ì„ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.
ê° ë‹¨ê³„ë³„ ìƒì„¸í•œ ì„¤ëª…ì€ ê°œë³„ ê°€ì´ë“œ ë¬¸ì„œ(`í•™ìŠµ_íŒŒì´í”„ë¼ì¸_ê°€ì´ë“œ.md`, `ì¶”ë¡ _íŒŒì´í”„ë¼ì¸_ê°€ì´ë“œ.md`)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.