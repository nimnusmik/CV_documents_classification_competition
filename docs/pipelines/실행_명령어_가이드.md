# ðŸš€ ì‹¤í–‰ ëª…ë ¹ì–´ ì™„ì „ ê°€ì´ë“œ

## ðŸ“‹ ëª©ì°¨
1. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
2. [í†µí•© CLI ì¸í„°íŽ˜ì´ìŠ¤](#í†µí•©-cli-ì¸í„°íŽ˜ì´ìŠ¤)
3. [í•™ìŠµ íŒŒì´í”„ë¼ì¸](#í•™ìŠµ-íŒŒì´í”„ë¼ì¸)
4. [ì¶”ë¡  íŒŒì´í”„ë¼ì¸](#ì¶”ë¡ -íŒŒì´í”„ë¼ì¸)
5. [ìµœì í™” ë° ìº˜ë¦¬ë¸Œë ˆì´ì…˜](#ìµœì í™”-ë°-ìº˜ë¦¬ë¸Œë ˆì´ì…˜)
6. [ì „ì²´ íŒŒì´í”„ë¼ì¸](#ì „ì²´-íŒŒì´í”„ë¼ì¸)
7. [ë‹¨ìœ„ í…ŒìŠ¤íŠ¸](#ë‹¨ìœ„-í…ŒìŠ¤íŠ¸)
8. [ë¡œê·¸ ë° ê²°ê³¼ í™•ì¸](#ë¡œê·¸-ë°-ê²°ê³¼-í™•ì¸)
9. [ê³ ê¸‰ ì‚¬ìš©ë²•](#ê³ ê¸‰-ì‚¬ìš©ë²•)
10. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ðŸ”§ í™˜ê²½ ì„¤ì •

### **1. ê¸°ë³¸ í™˜ê²½ í™•ì¸**
```bash
# í˜„ìž¬ ë””ë ‰í† ë¦¬ í™•ì¸
pwd
# ì¶œë ¥: /home/ieyeppo/AI_Lab/computer-vision-competition-1SEN

# Python í™˜ê²½ í™•ì¸
python3 --version
# ì¶œë ¥: Python 3.11.9

# pyenv ê°€ìƒí™˜ê²½ í™œì„±í™”
pyenv activate cv_py3_11_9
echo $VIRTUAL_ENV
```

### **2. ì˜ì¡´ì„± ì„¤ì¹˜**
```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Optuna, scikit-learn í¬í•¨)
pip install -r requirements.txt

# ì„¤ì¹˜ í™•ì¸
pip list | grep -E "torch|pandas|numpy|matplotlib|optuna|sklearn"

# í•µì‹¬ ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸
python3 -c "
import torch, pandas, numpy, matplotlib, optuna
from src.utils.common import load_yaml
from src.data.dataset import HighPerfDocClsDataset
from src.optimization.optuna_tuner import OptunaTuner
from src.calibration.temperature_scaling import TemperatureScaling
print('âœ… ëª¨ë“  ì˜ì¡´ì„± ì •ìƒ ì„¤ì¹˜ë¨')
"
```

---

## ðŸŽ¯ í†µí•© CLI ì¸í„°íŽ˜ì´ìŠ¤

### **ìƒˆë¡œìš´ í†µí•© ì‹¤í–‰ ë°©ì‹**
ëª¨ë“  í•™ìŠµ ëª¨ë“œê°€ í•˜ë‚˜ì˜ CLIë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤:

```bash
python src/training/train_main.py [OPTIONS]
```

#### **ê¸°ë³¸ ì˜µì…˜ë“¤:**
- `--config`: ì„¤ì • íŒŒì¼ ê²½ë¡œ (í•„ìˆ˜)
- `--mode`: ì‹¤í–‰ ëª¨ë“œ (basic, highperf, full-pipeline)
- `--optimize`: Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‚¬ìš©
- `--n-trials`: ìµœì í™” ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 20)
- `--use-calibration`: Temperature Scaling ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‚¬ìš©
- `--auto-continue`: ìµœì í™” í›„ ìžë™ìœ¼ë¡œ ì „ì²´ í•™ìŠµ ì§„í–‰
- `--skip-training`: í•™ìŠµ ê±´ë„ˆë›°ê³  ì¶”ë¡ ë§Œ ì‹¤í–‰ (full-pipeline ëª¨ë“œ)

#### **ë„ì›€ë§ í™•ì¸:**
```bash
python src/training/train_main.py --help

## ðŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸

### **1. ì™„ì „ ìžë™í™” íŒŒì´í”„ë¼ì¸ (ìµœê³  ì„±ëŠ¥)**

#### **ê¶Œìž¥ ì‹¤í–‰ ëª…ë ¹ì–´**
```bash
# í™˜ê²½ ì„¤ì •
pyenv activate cv_py3_11_9

# ì™„ì „ ìµœì í™” íŒŒì´í”„ë¼ì¸
python src/training/train_main.py 
    --config configs/train_highperf.yaml 
    --optimize 
    --n-trials 20 
    --use-calibration 
    --mode full-pipeline 
    --auto-continue
```

ì´ ëª…ë ¹ì–´ì˜ ì‹¤í–‰ ìˆœì„œ:
1. **ðŸ” Optuna ìµœì í™”**: í•˜ì´í¼íŒŒë¼ë¯¸í„° 20ë²ˆ ì‹œë„ ìµœì í™”
2. **ðŸŽ¯ ìžë™ í•™ìŠµ**: ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ì „ì²´ K-Fold í•™ìŠµ
3. **ðŸŒ¡ï¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜**: Temperature Scaling ëª¨ë¸ ë³´ì •
4. **ðŸ”® ê³ ì„±ëŠ¥ ì¶”ë¡ **: ì•™ìƒë¸” + TTA ì¶”ë¡ 
5. **ðŸ“¤ ì œì¶œ íŒŒì¼**: ìžë™ ìƒì„± ë° ì €ìž¥

### **2. ë¹ ë¥¸ ì‹¤í–‰ (ìµœì í™” ê±´ë„ˆë›°ê¸°)**
```bash
# ì‚¬ì „ ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ë°”ë¡œ ì‹¤í–‰
python src/training/train_main.py 
    --config configs/train_optimized_20250907_1825.yaml 
    --use-calibration 
    --mode full-pipeline
```

### **3. ì¶”ë¡ ë§Œ ì‹¤í–‰**
```bash
# ê¸°í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡ ë§Œ ì‹¤í–‰
python src/training/train_main.py 
    --config configs/train_highperf.yaml 
    --mode full-pipeline 
    --skip-training 
    --use-calibration
```

---

## ðŸ§ª ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

### **ëª¨ë“ˆë³„ í…ŒìŠ¤íŠ¸**
```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/ -v

# íŠ¹ì • ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_optimization.py -v
python -m pytest tests/test_calibration.py -v

# í•µì‹¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python -c "
from src.optimization.optuna_tuner import OptunaTuner
from src.calibration.temperature_scaling import TemperatureScaling
print('âœ… ìµœì í™” ë° ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“ˆ ì •ìƒ')
"
```

---

## ðŸ“Š ë¡œê·¸ ë° ê²°ê³¼ í™•ì¸

### **ìƒˆë¡œìš´ ë¡œê·¸ íŒŒì¼ëª… í˜•ì‹**

#### **ì¦ê°• íƒ€ìž… êµ¬ë¶„**
- **ê¸°ë³¸ ì¦ê°•**: `train_basic_augmentation_YYYYMMDD-HHMM_model.log`
- **ê³ ê¸‰ ì¦ê°•**: `train_highperf_advanced_augmentation_YYYYMMDD-HHMM_model.log`

#### **ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜**
```bash
# í•™ìŠµ ë¡œê·¸
ls -la logs/train/

# ìµœì í™” ë¡œê·¸
ls -la logs/optimization/

# íŒŒì´í”„ë¼ì¸ ë¡œê·¸
ls -la logs/pipeline/

# ì¶”ë¡  ë¡œê·¸
ls -la logs/infer/
```

### **ì‹¤í—˜ ê²°ê³¼ í™•ì¸**
```bash
# ì‹¤í—˜ í´ë” (íƒ€ìž„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
ls -la experiments/train/20250907/

# ìµœì í™” ê²°ê³¼
ls -la experiments/optimization/

# ì œì¶œ íŒŒì¼
ls -la submissions/20250907/
```

---

## ðŸŽ¯ ê³ ê¸‰ ì‚¬ìš©ë²•

### **1. ì¦ê°• ì„¤ì • ì»¤ìŠ¤í…€**

#### **ê¸°ë³¸ ì¦ê°•ìœ¼ë¡œ ë³€ê²½**
`configs/train_highperf.yaml` ìˆ˜ì •:
```yaml
train:
  use_advanced_augmentation: false  # false = ê¸°ë³¸ ì¦ê°•
```

#### **ê³ ê¸‰ ì¦ê°•ìœ¼ë¡œ ë³€ê²½**
```yaml
train:
  use_advanced_augmentation: true   # true = ê³ ê¸‰ ì¦ê°•
```

### **2. Optuna ì„¤ì • ì»¤ìŠ¤í…€**

#### **ìµœì í™” ë²”ìœ„ ì¡°ì •**
`configs/optuna_config.yaml` ìˆ˜ì •:
```yaml
search_space:
  lr: [1e-5, 1e-3]          # í•™ìŠµë¥  ë²”ìœ„
  weight_decay: [1e-6, 1e-2] # Weight decay ë²”ìœ„
  batch_size: [16, 64]       # ë°°ì¹˜ í¬ê¸° ë²”ìœ„
```

### **3. GPU ë©”ëª¨ë¦¬ ìµœì í™”**
```bash
# ìž‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì‹œìž‘
python src/training/train_main.py 
    --config configs/train_highperf.yaml 
    --mode highperf

# ë°°ì¹˜ í¬ê¸°ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì • í›„ config íŒŒì¼ ìˆ˜ì •
```

---

## â— ë¬¸ì œ í•´ê²°

### **ì¼ë°˜ì ì¸ ë¬¸ì œë“¤**

#### **1. í™˜ê²½ ë¬¸ì œ**
```bash
# pyenv í™˜ê²½ ìž¬ì„¤ì •
pyenv deactivate
pyenv activate cv_py3_11_9

# ì˜ì¡´ì„± ìž¬ì„¤ì¹˜
pip install -r requirements.txt --force-reinstall
```

#### **2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±**
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸° (config íŒŒì¼ ìˆ˜ì •)
# train_highperf.yamlì—ì„œ batch_size: 16 -> 8

# ë˜ëŠ” ëª…ë ¹ì–´ë¡œ ì œí•œ
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 python src/training/train_main.py ...
```

#### **3. Optuna ìµœì í™” ì˜¤ë¥˜**
```bash
# Optuna ìž¬ì„¤ì¹˜
pip install optuna>=4.0.0 --force-reinstall

# ì‹œë„ íšŸìˆ˜ ì¤„ì´ê¸°
python src/training/train_main.py --optimize --n-trials 5 ...
```

#### **4. ë¡œê·¸ íŒŒì¼ëª… í™•ì¸**
```bash
# ì¦ê°• ì„¤ì • í™•ì¸
grep "use_advanced_augmentation" configs/train_highperf.yaml

# ë¡œê·¸ íŒŒì¼ íŒ¨í„´ í™•ì¸
ls -la logs/train/*augmentation*
```

### **ë””ë²„ê¹… ëª…ë ¹ì–´**
```bash
# ì„¤ì • íŒŒì¼ ê²€ì¦
python -c "
from src.utils.common import load_yaml
cfg = load_yaml('configs/train_highperf.yaml')
print(f'ì¦ê°• ì„¤ì •: {cfg["train"]["use_advanced_augmentation"]}')
print(f'ëª¨ë¸: {cfg["model"]["name"]}')
"

# ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸
python -c "
from src.optimization.optuna_tuner import OptunaTuner
from src.calibration.temperature_scaling import TemperatureScaling
print('âœ… ëª¨ë“  ëª¨ë“ˆ ì •ìƒ')
"
```
```bash
# CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

# GPU ì •ë³´ í™•ì¸
nvidia-smi

# PyTorch CUDA ë²„ì „ í™•ì¸
python3 -c "import torch; print(f'PyTorch version: {torch.__version__}')"
```

---

## ðŸƒ ê¸°ë³¸ ì‹¤í–‰ ëª…ë ¹ì–´

### **í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸**
```bash
# ì „ì²´ êµ¬ì¡° í™•ì¸
tree -L 3 -I '__pycache__'

# ì£¼ìš” ë””ë ‰í† ë¦¬ í™•ì¸
ls -la src/
ls -la configs/
ls -la data/raw/
ls -la notebooks/
```

### **ì„¤ì • íŒŒì¼ ê²€ì¦**
```bash
# YAML íŒŒì¼ ìœ íš¨ì„± í™•ì¸
python3 -c "
import yaml
configs = ['configs/train.yaml', 'configs/train_highperf.yaml', 'configs/infer.yaml']
for config in configs:
    try:
        with open(config) as f:
            yaml.safe_load(f)
        print(f'âœ… {config}')
    except Exception as e:
        print(f'âŒ {config}: {e}')
"
```

---

## ðŸŽ“ í•™ìŠµ íŒŒì´í”„ë¼ì¸

### **ì‚¬ì „ ì¤€ë¹„ (í•„ìˆ˜ ë‹¨ê³„)**
```bash
## ðŸŽ“ í•™ìŠµ íŒŒì´í”„ë¼ì¸

### **1. ê¸°ë³¸ í•™ìŠµ (Basic Mode)**

#### **ê¸°ë³¸ ì¦ê°• í•™ìŠµ**
```bash
# pyenv í™˜ê²½ í™œì„±í™”
pyenv activate cv_py3_11_9

# ê¸°ë³¸ ëª¨ë“œ (basic augmentation)
python src/training/train_main.py \
    --config configs/train.yaml \
    --mode basic
```

### **2. ê³ ì„±ëŠ¥ í•™ìŠµ (High Performance Mode) - ê¶Œìž¥**

#### **ê³ ê¸‰ ì¦ê°• í•™ìŠµ**
```bash
# ê³ ì„±ëŠ¥ ëª¨ë“œ (advanced augmentation + mixup)
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf
```

---

## ðŸ” ìµœì í™” ë° ìº˜ë¦¬ë¸Œë ˆì´ì…˜

### **1. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**

#### **ê¸°ë³¸ ìµœì í™” (20ë²ˆ ì‹œë„)**
```bash
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --optimize \
    --mode highperf
```

#### **ì»¤ìŠ¤í…€ ì‹œë„ íšŸìˆ˜**
```bash
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --optimize \
    --n-trials 50 \
    --mode highperf
```

### **2. Temperature Scaling ìº˜ë¦¬ë¸Œë ˆì´ì…˜**

#### **ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš© í•™ìŠµ**
```bash
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --use-calibration \
    --mode full-pipeline
```

### **3. ì™„ì „ ìµœì í™” (Optuna + Calibration)**

#### **ê¶Œìž¥ ì„¤ì •**
```bash
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --optimize \
    --n-trials 20 \
    --use-calibration \
    --mode full-pipeline \
    --auto-continue
```

ì´ ëª…ë ¹ì–´ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. **Optuna ìµœì í™”**: 20ë²ˆ ì‹œë„ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
2. **ìžë™ ì§„í–‰**: ìµœì í™” ì™„ë£Œ í›„ ìžë™ìœ¼ë¡œ ì „ì²´ í•™ìŠµ ì‹œìž‘
3. **Temperature Scaling**: ëª¨ë¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš©
4. **í†µí•© íŒŒì´í”„ë¼ì¸**: í•™ìŠµ + ì¶”ë¡  + ì œì¶œ íŒŒì¼ ìƒì„±

---

## ï¿½ ì¶”ë¡  íŒŒì´í”„ë¼ì¸

### **1. ê¸°ë³¸ ì¶”ë¡ **
```bash
python src/training/train_main.py \
    --config configs/train.yaml \
    --mode full-pipeline \
    --skip-training
```

### **2. ê³ ì„±ëŠ¥ ì¶”ë¡  (ì•™ìƒë¸” + TTA)**
```bash
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode full-pipeline \
    --skip-training \
    --use-calibration
```
```bash
# ê³ ì„±ëŠ¥ ì„¤ì • ìµœì í™”
pyenv activate cv_py3_11_9
python src/utils/team_gpu_check.py
python src/utils/auto_batch_size.py --config configs/train_highperf.yaml
```

#### **ì „ì²´ K-Fold í•™ìŠµ**
```bash
# ê³ ì„±ëŠ¥ ëª¨ë“œ (5-fold ì „ì²´)
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# ê³ ì„±ëŠ¥ ì„¤ì • íŒŒì¼ ì§€ì •
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ìž¥ì‹œê°„ í•™ìŠµ)
nohup python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf > training.log 2>&1 &

# ì‹¤í–‰ ìƒíƒœ í™•ì¸
tail -f training.log
```

#### **ì™„ì „í•œ ì‹¤í–‰ ì‹œí€€ìŠ¤ (ê¶Œìž¥)**
```bash
# 1-4. ì‚¬ì „ ì¤€ë¹„
pyenv activate cv_py3_11_9
python src/utils/team_gpu_check.py
python src/utils/auto_batch_size.py --config configs/train_highperf.yaml

# 5. ê³ ì„±ëŠ¥ í•™ìŠµ ì‹œìž‘
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf
```

#### **íŠ¹ì • Foldë“¤ë§Œ í•™ìŠµ**
```bash
# Fold 0, 1, 2ë§Œ í•™ìŠµ (ë§¤ê°œë³€ìˆ˜ ì§€ì› ì‹œ)
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# ë‹¨ì¼ Fold ê³ ì„±ëŠ¥ ëª¨ë“œ (ë§¤ê°œë³€ìˆ˜ ì§€ì› ì‹œ)  
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf
```

#### **WandB í†µí•© í•™ìŠµ**
```bash
# WandB ë¡œê·¸ì¸ (ìµœì´ˆ 1íšŒ)
wandb login

# WandB í†µí•© í•™ìŠµ (ì˜¬ë°”ë¥¸ ë°©ë²•) - WandBëŠ” ì„¤ì •íŒŒì¼ì—ì„œ ì œì–´
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# íŠ¹ì • í”„ë¡œì íŠ¸ëª… ì§€ì •
WANDB_PROJECT="cv-competition-1sen" python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf
```

### **3. í•™ìŠµ ëª¨ë‹ˆí„°ë§**

#### **ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸**
```bash
# ìµœì‹  í•™ìŠµ ë¡œê·¸ í™•ì¸
ls -t logs/$(date +%Y%m%d)/train/ | head -1 | xargs -I {} tail -f logs/$(date +%Y%m%d)/train/{}

# íŠ¹ì • ë¡œê·¸ íŒŒì¼ í™•ì¸
tail -f logs/$(date +%Y%m%d)/train/train_20250905-1400_v087-abc123.log

# í•™ìŠµ ì§„í–‰ë¥  í™•ì¸ (ì—í¬í¬ë³„)
grep "Epoch" logs/$(date +%Y%m%d)/train/train_*.log | tail -10
```

#### **í•™ìŠµ ì¤‘ë‹¨ ë° ìž¬ì‹œìž‘**
```bash
# í•™ìŠµ ì¤‘ë‹¨
pkill -f "python src/training/train_main.py"

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìž¬ì‹œìž‘ (êµ¬í˜„ í•„ìš”ì‹œ)
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf --resume experiments/train/20250905/fold_0/checkpoint.pth
```

---

## ðŸ”® ì¶”ë¡  íŒŒì´í”„ë¼ì¸

### **1. ê¸°ë³¸ ì¶”ë¡ **

#### **ë‹¨ì¼ ëª¨ë¸ ì¶”ë¡ **
```bash
# ê¸°ë³¸ ì¶”ë¡ 
python src/inference/infer_main.py

# ì„¤ì • íŒŒì¼ ì§€ì •
python src/inference/infer_main.py --config configs/infer.yaml

# íŠ¹ì • ëª¨ë¸ íŒŒì¼ ì‚¬ìš©
python src/inference/infer_main.py --model-path experiments/train/20250905/fold_0/best_model.pth

# GPU ì§€ì •
CUDA_VISIBLE_DEVICES=0 python src/inference/infer_main.py
```

### **2. ê³ ì„±ëŠ¥ ì¶”ë¡  (Fold ì•™ìƒë¸”)**

#### **Fold ê²°ê³¼ ì•™ìƒë¸”**
```bash
# ê³ ì„±ëŠ¥ ì¶”ë¡  (ëª¨ë“  fold ì•™ìƒë¸”)
python src/inference/infer_main.py --mode highperf --fold-results experiments/train/20250905/fold_results

# íŠ¹ì • foldë“¤ë§Œ ì•™ìƒë¸”
python src/inference/infer_main.py --mode highperf --fold-results experiments/train/20250905/fold_results --folds 0,1,2,3,4

# ì•™ìƒë¸” ë°©ë²• ì§€ì •
python src/inference/infer_main.py --mode highperf --fold-results experiments/train/20250905/fold_results --ensemble-method mean
```

#### **ì‹¤í–‰ ì˜ˆì‹œ**
```bash
# ë‚ ì§œë³„ ì‹¤í—˜ ê²°ê³¼ í™•ì¸
ls -la experiments/train/

# ìµœì‹  ì‹¤í—˜ì˜ fold ê²°ê³¼ ì‚¬ìš©
LATEST_EXP=$(ls -t experiments/train/ | head -1)
python src/inference/infer_main.py --mode highperf --fold-results experiments/train/$LATEST_EXP/fold_results

# ì¶œë ¥ ì˜ˆì‹œ:
# ðŸ“‹ ì¶”ë¡  ì„¤ì • ë¡œë“œ ì™„ë£Œ
# ðŸŽ¯ ëª¨ë¸: swin_base_patch4_window7_224
# ðŸ“ Fold ê²°ê³¼ ë””ë ‰í† ë¦¬: experiments/train/20250905/fold_results
# ðŸ” ë°œê²¬ëœ fold ëª¨ë¸: [0, 1, 2, 3, 4]
# ðŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 3,141ê°œ ìƒ˜í”Œ
# [1/5] Fold 0 ì¶”ë¡  ì™„ë£Œ
# ...
# ðŸŽ¯ ì•™ìƒë¸” ì¶”ë¡  ì™„ë£Œ
# ðŸ’¾ ì œì¶œ íŒŒì¼ ì €ìž¥: submissions/20250905/submission.csv
```

### **3. ì¶”ë¡  ê²°ê³¼ í™•ì¸**

#### **ì œì¶œ íŒŒì¼ ê²€ì¦**
```bash
# ìµœì‹  ì œì¶œ íŒŒì¼ í™•ì¸
ls -la submissions/$(ls -t submissions/ | head -1)/

# ì œì¶œ íŒŒì¼ í˜•ì‹ í™•ì¸
head -10 submissions/$(ls -t submissions/ | head -1)/*.csv

# ì œì¶œ íŒŒì¼ í†µê³„
python3 -c "
import pandas as pd
import glob
latest_sub = sorted(glob.glob('submissions/*/submission.csv'))[-1]
df = pd.read_csv(latest_sub)
print(f'ì œì¶œ íŒŒì¼: {latest_sub}')
print(f'ìƒ˜í”Œ ìˆ˜: {len(df)}')
print(f'í´ëž˜ìŠ¤ ë¶„í¬: {df.iloc[:, 1].value_counts().head()}')
"
```

---

## ðŸ”„ ì „ì²´ íŒŒì´í”„ë¼ì¸

### **ì‚¬ì „ ì¤€ë¹„ (ì „ì²´ íŒŒì´í”„ë¼ì¸ìš©)**
```bash
# pyenv ê°€ìƒí™˜ê²½ í™œì„±í™”
pyenv activate cv_py3_11_9

# GPU í˜¸í™˜ì„± ì²´í¬
python src/utils/team_gpu_check.py

# í•™ìŠµìš© ë°°ì¹˜ í¬ê¸° ìµœì í™”
python src/utils/auto_batch_size.py --config configs/train_highperf.yaml

# ì¶”ë¡ ìš© ë°°ì¹˜ í¬ê¸° ìµœì í™” (ì˜µì…˜)
python src/utils/auto_batch_size.py --config configs/infer.yaml --test-only
```

### **1. ì™„ì „ ìžë™í™” íŒŒì´í”„ë¼ì¸**
```bash
# í•™ìŠµ + ì¶”ë¡  ì „ì²´ íŒŒì´í”„ë¼ì¸
python src/pipeline/full_pipeline.py

# ì„¤ì • íŒŒì¼ ì§€ì •
python src/pipeline/full_pipeline.py --train-config configs/train_highperf.yaml --infer-config configs/infer.yaml

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup python src/pipeline/full_pipeline.py > full_pipeline.log 2>&1 &
```

### **2. ë‹¨ê³„ë³„ ì‹¤í–‰**
```bash
# ì‚¬ì „ ì¤€ë¹„
pyenv activate cv_py3_11_9
python src/utils/team_gpu_check.py
python src/utils/auto_batch_size.py --config configs/train_highperf.yaml

# 1ë‹¨ê³„: ê³ ì„±ëŠ¥ í•™ìŠµ
echo "ðŸŽ“ 1ë‹¨ê³„: í•™ìŠµ ì‹œìž‘..."
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# 2ë‹¨ê³„: ê²°ê³¼ í™•ì¸
echo "ðŸ“Š 2ë‹¨ê³„: í•™ìŠµ ê²°ê³¼ í™•ì¸..."
ls -la experiments/train/$(ls -t experiments/train/ | head -1)/fold_results/

# 3ë‹¨ê³„: ì•™ìƒë¸” ì¶”ë¡ 
echo "ðŸ”® 3ë‹¨ê³„: ì•™ìƒë¸” ì¶”ë¡  ì‹œìž‘..."
LATEST_EXP=$(ls -t experiments/train/ | head -1)
python src/inference/infer_main.py --mode highperf --fold-results experiments/train/$LATEST_EXP/fold_results

# 4ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
echo "âœ… 4ë‹¨ê³„: ìµœì¢… ê²°ê³¼ í™•ì¸..."
ls -la submissions/$(ls -t submissions/ | head -1)/
```

---

## ðŸ§ª ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

### **1. Jupyter ë…¸íŠ¸ë¶ ì‹¤í–‰**

#### **ë…¸íŠ¸ë¶ ì„œë²„ ì‹œìž‘**
```bash
# Jupyter Notebook ì‹œìž‘
jupyter notebook --port=8888 --no-browser

# JupyterLab ì‹œìž‘ (ê¶Œìž¥)
jupyter lab --port=8888 --no-browser

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup jupyter lab --port=8888 --no-browser > jupyter.log 2>&1 &
```

#### **ê°œë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ë¶**
```bash
# 1. ê³ ì„±ëŠ¥ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
# notebooks/unit_tests/test_highperf_dataset.ipynb

# 2. Mixup ì¦ê°• í…ŒìŠ¤íŠ¸  
# notebooks/unit_tests/test_mixup_augmentation.ipynb

# 3. Swin ëª¨ë¸ í…ŒìŠ¤íŠ¸
# notebooks/unit_tests/test_swin_model.ipynb

# 4. ë¡œê¹… í†µí•© í…ŒìŠ¤íŠ¸ (ì˜ˆì‹œ)
# notebooks/test_highperf_dataset_with_logging.ipynb
```

### **2. í†µí•© í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ë¶**

#### **ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸**
```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦
# notebooks/test_full_pipeline.ipynb

# WandB í†µí•© í…ŒìŠ¤íŠ¸
# notebooks/test_wandb_integration.ipynb
```

### **3. ëª…ë ¹ì¤„ì—ì„œ ë…¸íŠ¸ë¶ ì‹¤í–‰**
```bash
# nbconvertë¡œ ë…¸íŠ¸ë¶ ì‹¤í–‰ (ì„ íƒì )
jupyter nbconvert --to notebook --execute notebooks/unit_tests/test_highperf_dataset.ipynb

# ê²°ê³¼ HTMLë¡œ ë³€í™˜
jupyter nbconvert --to html notebooks/unit_tests/test_highperf_dataset.ipynb
```

---

## ðŸ“Š ë¡œê·¸ ë° ê²°ê³¼ í™•ì¸

### **1. í•™ìŠµ ë¡œê·¸ ë¶„ì„**

#### **ê¸°ë³¸ ë¡œê·¸ í™•ì¸**
```bash
# ìµœì‹  í•™ìŠµ ë¡œê·¸ í™•ì¸
ls -t logs/train/ | head -5

# íŠ¹ì • ë¡œê·¸ ë‚´ìš© í™•ì¸
cat logs/$(date +%Y%m%d)/train/train_20250905-1400_v087-abc123.log

# ì—ëŸ¬ ë¡œê·¸ë§Œ í™•ì¸
grep -i "error\|fail" logs/$(date +%Y%m%d)/train/train_*.log

# ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ
grep "Val F1\|Best F1" logs/$(date +%Y%m%d)/train/train_*.log | tail -10
```

#### **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**
```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¼
tail -f logs/$(date +%Y%m%d)/train/train_*.log

# íŠ¹ì • íŒ¨í„´ ëª¨ë‹ˆí„°ë§
tail -f logs/$(date +%Y%m%d)/train/train_*.log | grep --line-buffered "Epoch\|F1"

# ë‹¤ì¤‘ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
multitail logs/$(date +%Y%m%d)/train/train_*.log logs/$(date +%Y%m%d)/infer/infer_*.log
```

### **2. ì¶”ë¡  ë¡œê·¸ ë¶„ì„**

#### **ì¶”ë¡  ê²°ê³¼ í™•ì¸**
```bash
# ìµœì‹  ì¶”ë¡  ë¡œê·¸
ls -t logs/infer/ | head -3

# ì¶”ë¡  ì„±ëŠ¥ í™•ì¸
grep "Inference completed\|Processing time" logs/$(date +%Y%m%d)/infer/infer_*.log

# ì•™ìƒë¸” ê²°ê³¼ í™•ì¸
grep "Ensemble\|Average" logs/infer/infer_*.log
```

### **3. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë¡œê·¸ (ë¡œê¹… ì‹œìŠ¤í…œ ì‚¬ìš©ì‹œ)**

#### **ë¡œê¹… ì‹œìŠ¤í…œ ê²°ê³¼ í™•ì¸**
```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë¡œê·¸ ë””ë ‰í† ë¦¬
ls -la logs/unit_test/

# íŠ¹ì • í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸
TEST_NAME="highperf_dataset"
LATEST_RUN=$(ls -t logs/unit_test/$TEST_NAME/ | head -1)
ls -la logs/unit_test/$TEST_NAME/$LATEST_RUN/

# í…ŒìŠ¤íŠ¸ ìš”ì•½ JSON í™•ì¸
cat logs/unit_test/$TEST_NAME/$LATEST_RUN/test_summary.json | jq

# ì €ìž¥ëœ ì´ë¯¸ì§€ í™•ì¸
ls logs/unit_test/$TEST_NAME/$LATEST_RUN/images/

# ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸
ls logs/unit_test/$TEST_NAME/$LATEST_RUN/data/
```

### **4. ì‹¤í—˜ ê²°ê³¼ ê´€ë¦¬**

#### **ì‹¤í—˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°**
```bash
# ì‹¤í—˜ ê²°ê³¼ í™•ì¸
tree experiments/ -L 3

# íŠ¹ì • ë‚ ì§œ ì‹¤í—˜ í™•ì¸
ls -la experiments/train/20250905/

# Foldë³„ ê²°ê³¼ í™•ì¸
ls -la experiments/train/20250905/fold_results/

# ëª¨ë¸ íŒŒì¼ í™•ì¸
find experiments/ -name "*.pth" -type f | head -10
```

#### **ì œì¶œ íŒŒì¼ ê´€ë¦¬**
```bash
# ì œì¶œ íŒŒì¼ ížˆìŠ¤í† ë¦¬
ls -la submissions/

# ìµœì‹  ì œì¶œ íŒŒì¼ ë¶„ì„
python3 -c "
import pandas as pd
import glob
import os

# ìµœì‹  ì œì¶œ íŒŒì¼ ì°¾ê¸°
submission_files = glob.glob('submissions/*/submission.csv')
if submission_files:
    latest = max(submission_files, key=os.path.getctime)
    df = pd.read_csv(latest)
    print(f'ìµœì‹  ì œì¶œ íŒŒì¼: {latest}')
    print(f'ìƒì„± ì‹œê°„: {pd.Timestamp.fromtimestamp(os.path.getctime(latest))}')
    print(f'íŒŒì¼ í¬ê¸°: {len(df)} í–‰')
    print(f'í´ëž˜ìŠ¤ ë¶„í¬:')
    print(df.iloc[:, 1].value_counts().head())
else:
    print('ì œì¶œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.')
"
```

---

## ðŸŽ›ï¸ ê³ ê¸‰ ì‚¬ìš©ë²•

### **1. í™˜ê²½ ë³€ìˆ˜ í™œìš©**

#### **GPU ê´€ë¦¬**
```bash
# íŠ¹ì • GPU ì‚¬ìš©
export CUDA_VISIBLE_DEVICES=0
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# ë©€í‹° GPU ì‚¬ìš©
export CUDA_VISIBLE_DEVICES=0,1
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# GPU ë©”ëª¨ë¦¬ ì œí•œ
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf
```

#### **WandB ì„¤ì •**
```bash
# WandB í”„ë¡œì íŠ¸ ì„¤ì •
export WANDB_PROJECT="cv-competition-1sen"
export WANDB_ENTITY="your-team-name"

# WandB ì‹¤í–‰ ì´ë¦„ ì„¤ì •
export WANDB_RUN_NAME="highperf-swin-fold-all"
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf
```

#### **ë¡œê¹… ë ˆë²¨ ì„¤ì •**
```bash
# ë””ë²„ê·¸ ëª¨ë“œ
export LOG_LEVEL=DEBUG
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# ì¡°ìš©í•œ ëª¨ë“œ
export LOG_LEVEL=WARNING
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf
```

### **2. ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸**

#### **ì „ì²´ ì‹¤í—˜ ìžë™í™”**
```bash
#!/bin/bash
# run_full_experiment.sh

set -e  # ì—ëŸ¬ì‹œ ì¤‘ë‹¨

echo "ðŸš€ ì „ì²´ ì‹¤í—˜ ì‹œìž‘: $(date)"

# 1. í™˜ê²½ í™•ì¸
echo "ðŸ“‹ í™˜ê²½ í™•ì¸..."
python3 -c "from src.utils.common import load_yaml; print('âœ… í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ')"

# 2. í•™ìŠµ ì‹¤í–‰
echo "ðŸŽ“ í•™ìŠµ ì‹œìž‘..."
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# 3. ìµœì‹  ì‹¤í—˜ ê²°ê³¼ ì°¾ê¸°
LATEST_EXP=$(ls -t experiments/train/ | head -1)
echo "ðŸ“ ìµœì‹  ì‹¤í—˜: $LATEST_EXP"

# 4. ì¶”ë¡  ì‹¤í–‰
echo "ðŸ”® ì¶”ë¡  ì‹œìž‘..."
python src/inference/infer_main.py --mode highperf --fold-results experiments/train/$LATEST_EXP/fold_results

# 5. ê²°ê³¼ í™•ì¸
echo "âœ… ì‹¤í—˜ ì™„ë£Œ: $(date)"
ls -la submissions/$(ls -t submissions/ | head -1)/

echo "ðŸŽ¯ ì‹¤í—˜ ìš”ì•½:"
echo "   í•™ìŠµ ê²°ê³¼: experiments/train/$LATEST_EXP/"
echo "   ì œì¶œ íŒŒì¼: submissions/$(ls -t submissions/ | head -1)/"
```

#### **ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**
```bash
# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x run_full_experiment.sh

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup ./run_full_experiment.sh > experiment.log 2>&1 &

# ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
tail -f experiment.log
```

### **3. ì„±ëŠ¥ ìµœì í™”**

#### **ë©”ëª¨ë¦¬ ìµœì í™”**
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
sed -i 's/batch_size: 16/batch_size: 8/' configs/train_highperf.yaml

# ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸°
sed -i 's/img_size: 384/img_size: 224/' configs/train_highperf.yaml

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi
```

#### **ì†ë„ ìµœì í™”**
```bash
# ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜ ì¦ê°€
export NUM_WORKERS=8
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# í˜¼í•© ì •ë°€ë„ í•™ìŠµ (êµ¬í˜„ì‹œ)
export USE_AMP=true
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf
```

---

## ðŸ”§ ë¬¸ì œ í•´ê²°

### **1. ì¼ë°˜ì ì¸ ë¬¸ì œ**

#### **Import ì—ëŸ¬**
```bash
# ë¬¸ì œ: ModuleNotFoundError
# í•´ê²°: Python ê²½ë¡œ í™•ì¸
echo $PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# ë˜ëŠ” sys.path ì¶”ê°€
python3 -c "import sys; sys.path.append('.'); from src.utils.common import load_yaml"
```

#### **CUDA ì—ëŸ¬**
```bash
# ë¬¸ì œ: CUDA out of memory
# í•´ê²°: ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
export BATCH_SIZE=4
python src/training/train_main.py --fold 0

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
python3 -c "import torch; torch.cuda.empty_cache()"
```

#### **íŒŒì¼ ê²½ë¡œ ì—ëŸ¬**
```bash
# ë¬¸ì œ: ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
# í•´ê²°: í˜„ìž¬ ë””ë ‰í† ë¦¬ í™•ì¸
pwd
ls -la data/raw/

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
cd /home/ieyeppo/AI_Lab/computer-vision-competition-1SEN
```

### **2. ë¡œê·¸ ê¸°ë°˜ ë””ë²„ê¹…**

#### **ì—ëŸ¬ ë¡œê·¸ ë¶„ì„**
```bash
# ìµœê·¼ ì—ëŸ¬ ì°¾ê¸°
grep -r -i "error\|exception\|failed" logs/ | tail -10

# íŠ¹ì • ì—ëŸ¬ íŒ¨í„´ ì°¾ê¸°
grep -r "CUDA\|memory\|import" logs/ | tail -5

# ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤ í™•ì¸
grep -A 10 -B 2 "Traceback" logs/train/train_*.log
```

#### **ì„±ëŠ¥ ì´ìŠˆ ë””ë²„ê¹…**
```bash
# í•™ìŠµ ì†ë„ í™•ì¸
grep "Epoch.*Time" logs/train/train_*.log | tail -5

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
grep -i "memory\|GPU" logs/train/train_*.log | tail -5

# ë°ì´í„° ë¡œë”© ì‹œê°„ í™•ì¸
grep -i "loading\|batch" logs/train/train_*.log | tail -5
```

### **3. ê²€ì¦ ëª…ë ¹ì–´**

#### **ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦**
```bash
# ì¢…í•© ê±´ê°•ë„ ê²€ì‚¬
python3 -c "
import sys
sys.path.append('.')

# 1. ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸
try:
    from src.utils.common import load_yaml
    from src.data.dataset import HighPerfDocClsDataset
    from src.models.build import build_model
    print('âœ… ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ')
except Exception as e:
    print(f'âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}')
    sys.exit(1)

# 2. ì„¤ì • íŒŒì¼ í…ŒìŠ¤íŠ¸
try:
    cfg = load_yaml('configs/train_highperf.yaml')
    print('âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ')
except Exception as e:
    print(f'âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}')
    sys.exit(1)

# 3. ë°ì´í„° íŒŒì¼ í…ŒìŠ¤íŠ¸
import os
required_files = [
    'data/raw/train.csv',
    'data/raw/meta.csv', 
    'data/raw/sample_submission.csv'
]
for file in required_files:
    if os.path.exists(file):
        print(f'âœ… {file} ì¡´ìž¬')
    else:
        print(f'âŒ {file} ì—†ìŒ')

print('ðŸŽ¯ ì‹œìŠ¤í…œ ê²€ì¦ ì™„ë£Œ!')
"
```

#### **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**
```bash
# ë¹ ë¥¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ (1 ì—í¬í¬)
python3 -c "
import sys
sys.path.append('.')
import time
from src.training.train import train_model
from src.utils.common import load_yaml

cfg = load_yaml('configs/train.yaml')
cfg['training']['epochs'] = 1
cfg['training']['batch_size'] = 4

start_time = time.time()
# train_model(cfg, fold=0)  # ì‹¤ì œ ì‹¤í–‰ì‹œ ì£¼ì„ í•´ì œ
end_time = time.time()

print(f'â±ï¸ 1 ì—í¬í¬ ì˜ˆìƒ ì‹œê°„: {end_time - start_time:.1f}ì´ˆ')
"
```

---

## ðŸ“ž ì¶”ê°€ ì§€ì›

### **ë¡œê·¸ ìˆ˜ì§‘**
```bash
# ë¬¸ì œ ë°œìƒì‹œ ë¡œê·¸ ìˆ˜ì§‘
mkdir -p debug_logs/$(date +%Y%m%d_%H%M%S)
cp -r logs/ debug_logs/$(date +%Y%m%d_%H%M%S)/
cp configs/*.yaml debug_logs/$(date +%Y%m%d_%H%M%S)/
```

### **ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘**
```bash
# ì‹œìŠ¤í…œ ì •ë³´ ì €ìž¥
echo "=== ì‹œìŠ¤í…œ ì •ë³´ ===" > system_info.txt
date >> system_info.txt
uname -a >> system_info.txt
python3 --version >> system_info.txt
pip list | grep -E "torch|pandas|numpy" >> system_info.txt
nvidia-smi >> system_info.txt
```
