# ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¢…í•© ì‹¤í–‰ ê°€ì´ë“œ

> ê³ ì„±ëŠ¥ ë¬¸ì„œ ë¶„ë¥˜ ê²½ì§„ëŒ€íšŒë¥¼ ìœ„í•œ ì™„ì „í•œ End-to-End íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ê°œìš”](#-ì‹œìŠ¤í…œ-ê°œìš”)
2. [í™˜ê²½ ì„¤ì •](#-í™˜ê²½-ì„¤ì •)
3. [ë°ì´í„° ì¤€ë¹„](#-ë°ì´í„°-ì¤€ë¹„)
4. [ì„¤ì • íŒŒì¼ ê´€ë¦¬](#-ì„¤ì •-íŒŒì¼-ê´€ë¦¬)
5. [í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰](#-í•™ìŠµ-íŒŒì´í”„ë¼ì¸-ì‹¤í–‰)
6. [ì¶”ë¡  ë° ì œì¶œ](#-ì¶”ë¡ -ë°-ì œì¶œ)
7. [ì‹¤í—˜ ì¶”ì  ë° ê´€ë¦¬](#-ì‹¤í—˜-ì¶”ì -ë°-ê´€ë¦¬)
8. [ì„±ëŠ¥ ìµœì í™”](#-ì„±ëŠ¥-ìµœì í™”)
9. [ë¬¸ì œ í•´ê²°](#-ë¬¸ì œ-í•´ê²°)
10. [íŒ€ í˜‘ì—… ì›Œí¬í”Œë¡œìš°](#-íŒ€-í˜‘ì—…-ì›Œí¬í”Œë¡œìš°)

---

## ğŸ¯ ì‹œìŠ¤í…œ ê°œìš”

### ì•„í‚¤í…ì²˜ êµ¬ì¡°

```mermaid
graph TB
    A[ë°ì´í„° ì¤€ë¹„] --> B[ì„¤ì • ê´€ë¦¬]
    B --> C[í•™ìŠµ íŒŒì´í”„ë¼ì¸]
    C --> D[ëª¨ë¸ ê²€ì¦]
    D --> E[ì¶”ë¡  ì‹¤í–‰]
    E --> F[ê²°ê³¼ ì œì¶œ]
    
    G[WandB ì‹¤í—˜ ì¶”ì ] --> C
    G --> D
    G --> E
    
    H[ë‹¨ìœ„ í…ŒìŠ¤íŠ¸] --> C
    I[ë¡œê¹… ì‹œìŠ¤í…œ] --> C
    I --> D
    I --> E
```

### í•µì‹¬ íŠ¹ì§•

- **ğŸ¯ ê³ ì„±ëŠ¥ ëª¨ë¸**: Swin Transformer, ConvNext ê¸°ë°˜ ì•™ìƒë¸”
- **ğŸ¨ ê³ ê¸‰ ì¦ê°•**: Hard Augmentation + Mixup + TTA
- **ğŸ“Š ì‹¤í—˜ ì¶”ì **: WandBë¥¼ í†µí•œ ì²´ê³„ì  ì‹¤í—˜ ê´€ë¦¬
- **ğŸ”„ í†µí•© íŒŒì´í”„ë¼ì¸**: í•™ìŠµë¶€í„° ì¶”ë¡ ê¹Œì§€ ì›ìŠ¤í†± ì‹¤í–‰
- **ğŸ§ª ê²€ì¦ ì‹œìŠ¤í…œ**: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ í’ˆì§ˆ ë³´ì¦

### ì„±ëŠ¥ ëª©í‘œ

- **F1 Score**: 0.940+ (Optuna ìµœì í™” + Temperature Scaling ì ìš©)
- **ì²˜ë¦¬ ì†ë„**: ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì‹¤ì‹œê°„ ì²˜ë¦¬
- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: 8GB GPUì—ì„œ ì•ˆì •ì  ë™ì‘
- **ì¬í˜„ì„±**: ë™ì¼ ì‹œë“œë¡œ ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥
- **ìë™í™”**: í†µí•© CLIë¡œ ì›í´ë¦­ ì‹¤í–‰

---

## ğŸ› ï¸ í™˜ê²½ ì„¤ì •

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# ìµœì†Œ ìš”êµ¬ì‚¬í•­
- Python 3.8+
- CUDA 11.0+ (GPU í•„ìˆ˜)
- RAM 16GB+
- GPU Memory 8GB+
- Storage 50GB+

# ê¶Œì¥ ì‚¬ì–‘ (ìµœì í™” í¬í•¨)
- Python 3.11+ (pyenv ì‚¬ìš©)
- CUDA 11.7+
- RAM 32GB+
- GPU Memory 16GB+ (RTX 3080/4080+)
- SSD 100GB+
```

### ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# 1. pyenv ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
pyenv virtualenv 3.11.9 cv_py3_11_9
pyenv activate cv_py3_11_9

# 2. PyTorch ì„¤ì¹˜ (CUDA ë²„ì „ì— ë§ê²Œ)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 3. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (Optuna, scikit-learn í¬í•¨)
pip install -r requirements.txt

# 4. ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
pip install optuna>=4.0.0 scikit-learn wandb timm albumentations opencv-python

# 5. ì„¤ì¹˜ í™•ì¸
python -c "
import torch, optuna
from src.optimization.optuna_tuner import OptunaTuner
from src.calibration.temperature_scaling import TemperatureScaling
print('CUDA Available:', torch.cuda.is_available())
print('Optuna Version:', optuna.__version__)
print('âœ… ëª¨ë“  ëª¨ë“ˆ ì •ìƒ')
"
```

### ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸

```bash
# í”„ë¡œì íŠ¸ êµ¬ì¡° ê²€ì¦
tree -L 3
```

ì˜ˆìƒ ì¶œë ¥:
```
.
â”œâ”€â”€ configs/                    # ì„¤ì • íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ train_highperf.yaml    # ê³ ì„±ëŠ¥ í•™ìŠµ ì„¤ì •
â”‚   â”œâ”€â”€ train.yaml             # ê¸°ë³¸ í•™ìŠµ ì„¤ì •
â”‚   â””â”€â”€ infer.yaml             # ì¶”ë¡  ì„¤ì •
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                   # ì›ë³¸ ë°ì´í„°
â”‚       â”œâ”€â”€ train/             # í•™ìŠµ ì´ë¯¸ì§€
â”‚       â”œâ”€â”€ test/              # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
â”‚       â”œâ”€â”€ train.csv          # í•™ìŠµ ë©”íƒ€ë°ì´í„°
â”‚       â””â”€â”€ meta.csv           # í…ŒìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ src/                       # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ data/                  # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ models/                # ëª¨ë¸ ì •ì˜
â”‚   â”œâ”€â”€ training/              # í•™ìŠµ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ inference/             # ì¶”ë¡  ëª¨ë“ˆ
â”‚   â”œâ”€â”€ pipeline/              # í†µí•© íŒŒì´í”„ë¼ì¸
â”‚   â””â”€â”€ utils/                 # ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ notebooks/                 # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ë¶
â”œâ”€â”€ logs/                      # ë¡œê·¸ íŒŒì¼ë“¤
â”œâ”€â”€ experiments/               # ì‹¤í—˜ ê²°ê³¼
â””â”€â”€ submissions/               # ì œì¶œ íŒŒì¼ë“¤
```

---

## ğŸ“ ë°ì´í„° ì¤€ë¹„

### ë°ì´í„° êµ¬ì¡° í™•ì¸

```bash
# ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
ls -la data/raw/
echo "í•™ìŠµ ì´ë¯¸ì§€ ìˆ˜: $(ls data/raw/train/ | wc -l)"
echo "í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìˆ˜: $(ls data/raw/test/ | wc -l)"

# CSV íŒŒì¼ ê¸°ë³¸ ì •ë³´
head -5 data/raw/train.csv
wc -l data/raw/train.csv
```

### ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

```python
# ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
python -c "
import pandas as pd
import os

# CSV íŒŒì¼ ë¡œë“œ
train_df = pd.read_csv('data/raw/train.csv')
test_df = pd.read_csv('data/raw/meta.csv')

print(f'í•™ìŠµ ë°ì´í„°: {len(train_df)} ìƒ˜í”Œ')
print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)} ìƒ˜í”Œ')
print(f'í´ë˜ìŠ¤ ìˆ˜: {train_df[\"target\"].nunique()}')
print(f'í´ë˜ìŠ¤ ë¶„í¬: {dict(train_df[\"target\"].value_counts().head())}')

# ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ í™•ì¸
missing_train = 0
for idx, row in train_df.head(100).iterrows():
    if not os.path.exists(f'data/raw/train/{row[\"ID\"]}.jpg'):
        missing_train += 1

print(f'ëˆ„ë½ëœ í•™ìŠµ ì´ë¯¸ì§€ (ìƒ˜í”Œ 100ê°œ ì¤‘): {missing_train}')
"
```

### ë°ì´í„° ì „ì²˜ë¦¬ (ì„ íƒì‚¬í•­)

```bash
# ì´ë¯¸ì§€ í¬ê¸° ë¶„í¬ í™•ì¸
python src/utils/analyze_images.py --data_dir data/raw/train --sample_size 1000

# ì†ìƒëœ ì´ë¯¸ì§€ ê²€ì‚¬
python src/utils/check_corrupted_images.py --data_dir data/raw/train
```

---

## âš™ï¸ ì„¤ì • íŒŒì¼ ê´€ë¦¬

### ì£¼ìš” ì„¤ì • íŒŒì¼ ê°œìš”

#### 1. `configs/train_highperf.yaml` - ê³ ì„±ëŠ¥ í•™ìŠµ ì„¤ì •

```yaml
model:
  name: "swin_base_patch4_window12_384"  # Swin Transformer Base
  img_size: 384                          # ê³ í•´ìƒë„ ì…ë ¥
  num_classes: 17                        # ë¬¸ì„œ í´ë˜ìŠ¤ ìˆ˜
  pretrained: true                       # ImageNet ì‚¬ì „ í•™ìŠµ ëª¨ë¸
  
training:
  epochs: 30                             # í•™ìŠµ ì—í¬í¬
  batch_size: 16                         # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
  learning_rate: 1e-4                    # í•™ìŠµë¥ 
  n_folds: 5                             # K-Fold êµì°¨ ê²€ì¦
  early_stopping_patience: 7             # ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´
  save_every_epoch: false                # ë§¤ ì—í¬í¬ ì €ì¥ ì—¬ë¶€
  
augmentation:
  hard_augmentation:
    enabled: true                        # Hard Augmentation í™œì„±í™”
    initial_prob: 0.1                    # ì´ˆê¸° í™•ë¥ 
    final_prob: 0.8                      # ìµœì¢… í™•ë¥ 
    progression: "linear"                # ì§„í–‰ ë°©ì‹
  
  mixup:
    enabled: true                        # Mixup í™œì„±í™”
    alpha: 0.2                          # Beta ë¶„í¬ íŒŒë¼ë¯¸í„°
    prob: 0.5                           # ì ìš© í™•ë¥ 
    
  tta:                                  # Test Time Augmentation
    enabled: true
    num_augments: 5                     # TTA íšŸìˆ˜

wandb:
  enabled: true                         # WandB ë¡œê¹… í™œì„±í™”
  project_name: "document-classification-team"
  experiment_name: null                 # ìë™ ìƒì„±
  tags: ["high-performance", "swin-transformer"]
```

#### 2. ì„¤ì • íŒŒì¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```bash
# ê°œì¸ ì‹¤í—˜ìš© ì„¤ì • ìƒì„±
cp configs/train_highperf.yaml configs/my_experiment.yaml

# ì„¤ì • ìˆ˜ì • ì˜ˆì‹œ
vim configs/my_experiment.yaml
```

**ì£¼ìš” ìˆ˜ì • í¬ì¸íŠ¸**:

```yaml
# GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
training:
  batch_size: 8    # 8GB GPU
  # batch_size: 16   # 16GB GPU
  # batch_size: 32   # 24GB+ GPU

# ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•œ ì„¤ì •
training:
  epochs: 10
  n_folds: 3
  
# ê°œì¸ WandB í”„ë¡œì íŠ¸
wandb:
  experiment_name: "ì‹¤í—˜ìì´ë¦„_ì‹¤í—˜ì„¤ëª…_20250905"
  tags: ["ê°œì¸ì‹¤í—˜", "ë¹ ë¥¸í…ŒìŠ¤íŠ¸"]
```

### ì„¤ì • ê²€ì¦

```bash
# ì„¤ì • íŒŒì¼ êµ¬ë¬¸ ê²€ì‚¬
python -c "
from src.utils.common import load_yaml
cfg = load_yaml('configs/train_highperf.yaml')
print('ì„¤ì • ë¡œë“œ ì„±ê³µ!')
print(f'ëª¨ë¸: {cfg[\"model\"][\"name\"]}')
print(f'ì´ë¯¸ì§€ í¬ê¸°: {cfg[\"model\"][\"img_size\"]}')
print(f'ì—í¬í¬: {cfg[\"training\"][\"epochs\"]}')
"
```

---

## ğŸƒâ€â™‚ï¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

### 1. ğŸš€ ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸ (ê¶Œì¥)

ìµœê³  ì„±ëŠ¥ì„ ìœ„í•œ ì™„ì „ ìµœì í™” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

#### **ì‚¬ì „ ì¤€ë¹„ (í•„ìˆ˜)**
```bash
# 1. pyenv ê°€ìƒí™˜ê²½ í™œì„±í™”
pyenv activate cv_py3_11_9

# 2. í™˜ê²½ í™•ì¸
python -c "
import torch, optuna
from src.optimization.optuna_tuner import OptunaTuner
from src.calibration.temperature_scaling import TemperatureScaling
print('âœ… ëª¨ë“  ëª¨ë“ˆ ì •ìƒ')
"
```

#### **ğŸ† ìµœê³  ì„±ëŠ¥ íŒŒì´í”„ë¼ì¸**
```bash
# ì™„ì „ ìµœì í™” íŒŒì´í”„ë¼ì¸ (Optuna + Calibration + ì•™ìƒë¸”)
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --optimize \
    --n-trials 20 \
    --use-calibration \
    --mode full-pipeline \
    --auto-continue
```

**ì´ ëª…ë ¹ì–´ê°€ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…**:
1. **ğŸ” Optuna ìµœì í™”**: í•˜ì´í¼íŒŒë¼ë¯¸í„° 20ë²ˆ ì‹œë„ ìµœì í™”
2. **ğŸ¯ ìë™ í•™ìŠµ**: ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ K-Fold í•™ìŠµ
3. **ğŸŒ¡ï¸ Temperature Scaling**: ëª¨ë¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš©
4. **ğŸ”® ì•™ìƒë¸” ì¶”ë¡ **: 5-Fold ì•™ìƒë¸” + TTA
5. **ğŸ“¤ ì œì¶œ íŒŒì¼**: ìë™ ìƒì„± ë° ì €ì¥

### 2. ğŸ¯ ë¹ ë¥¸ ì‹¤í–‰ (ì‚¬ì „ ìµœì í™” ì‚¬ìš©)

ì´ë¯¸ ìµœì í™”ëœ ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ ì‹¤í–‰í•©ë‹ˆë‹¤.

#### **ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰**
```bash
# ì‚¬ì „ ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
python src/training/train_main.py \
    --config configs/train_optimized_20250907_1825.yaml \
    --use-calibration \
    --mode full-pipeline
```

### 3. ğŸ§ª ë‹¨ê³„ë³„ ì‹¤í–‰ (ê°œë°œ ë° ë””ë²„ê¹…)

ë” ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•œ ê²½ìš° ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### **3-1. ê¸°ë³¸ í•™ìŠµ**
```bash
# ê¸°ë³¸ ì¦ê°• í•™ìŠµ
python src/training/train_main.py \
    --config configs/train.yaml \
    --mode basic
```

#### **3-2. ê³ ì„±ëŠ¥ í•™ìŠµ**
```bash
# ê³ ê¸‰ ì¦ê°• + Mixup í•™ìŠµ
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf
```

#### **3-3. ìµœì í™”ë§Œ ì‹¤í–‰**
```bash
# Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë§Œ
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --optimize \
    --n-trials 10 \
    --mode highperf
```

### 4. ğŸ® ê³ ê¸‰ ì˜µì…˜ë“¤

#### **ì»¤ìŠ¤í…€ ìµœì í™” ì‹œë„ íšŸìˆ˜**
```bash
# ë” ë§ì€ ì‹œë„ë¡œ ì •ë°€ ìµœì í™” (ì‹œê°„ ì†Œìš”)
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --optimize \
    --n-trials 50 \
    --use-calibration \
    --mode full-pipeline \
    --auto-continue
```

#### **GPU ë©”ëª¨ë¦¬ ìµœì í™”**
```bash
# ë©”ëª¨ë¦¬ ì œí•œ í™˜ê²½ì—ì„œ ì‹¤í–‰
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf
```

#### **ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰**
```bash
# ê¸´ í•™ìŠµì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
nohup python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --optimize \
    --n-trials 20 \
    --use-calibration \
    --mode full-pipeline \
    --auto-continue > training.log 2>&1 &

# ì§„í–‰ ìƒí™© í™•ì¸
tail -f training.log
```
```

#### **2-2. í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡ **
```bash
# ì¶”ë¡ ìš© ë°°ì¹˜ í¬ê¸° ìµœì í™” (ì˜µì…˜)
python src/utils/auto_batch_size.py --config configs/infer.yaml --test-only

# 2ë‹¨ê³„: í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡ 
python src/inference/infer_main.py \
    --model_dir experiments/train/20250905/v087 \
    --config configs/train_highperf.yaml
```

### 3. ì‹¤í–‰ ì¤‘ ëª¨ë‹ˆí„°ë§

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
tail -f logs/train/train_*.log

# GPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
du -sh experiments/ logs/ submissions/
```

### 4. ì‹¤í–‰ ì˜µì…˜ ìƒì„¸

```bash
# ë„ì›€ë§ í™•ì¸
python src/training/train_main.py --help
```

**ì£¼ìš” ì˜µì…˜**:
- `--mode`: ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
  - `basic`: ê¸°ë³¸ í•™ìŠµ
  - `highperf`: ê³ ì„±ëŠ¥ í•™ìŠµ
  - `full-pipeline`: í†µí•© íŒŒì´í”„ë¼ì¸ (ê¶Œì¥)
- `--config`: ì„¤ì • íŒŒì¼ ê²½ë¡œ
- `--resume`: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
- `--debug`: ë””ë²„ê·¸ ëª¨ë“œ (ì†Œê·œëª¨ ë°ì´í„°)

---

## ğŸ”® ì¶”ë¡  ë° ì œì¶œ

### 1. ìë™ ì¶”ë¡  (ê¶Œì¥)

í†µí•© íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œ ê²½ìš° ì¶”ë¡ ì´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.

```bash
# ê²°ê³¼ í™•ì¸
ls -la submissions/$(date +%Y%m%d)/
cat submissions/$(date +%Y%m%d)/submission_info.json
```

### 2. ìˆ˜ë™ ì¶”ë¡ 

ë³„ë„ë¡œ ì¶”ë¡ ì„ ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ê²½ìš°:

```bash
# ìµœì‹  ëª¨ë¸ë¡œ ì¶”ë¡ 
python src/inference/infer_main.py \
    --mode ensemble \
    --model_dir experiments/train/$(date +%Y%m%d) \
    --output_dir submissions/$(date +%Y%m%d)

# íŠ¹ì • ëª¨ë¸ë¡œ ì¶”ë¡ 
python src/inference/infer_main.py \
    --mode single \
    --model_path experiments/train/20250905/v087/best_model_fold_1.pth \
    --config configs/train_highperf.yaml
```

### 3. Test Time Augmentation (TTA)

ë” ë†’ì€ ì„±ëŠ¥ì„ ì›í•˜ëŠ” ê²½ìš°:

```bash
# TTA í™œì„±í™” ì¶”ë¡ 
python src/inference/infer_main.py \
    --mode ensemble \
    --tta_enabled \
    --tta_num_augments 8 \
    --model_dir experiments/train/$(date +%Y%m%d)
```

### 4. ì œì¶œ íŒŒì¼ ê²€ì¦

```bash
# ì œì¶œ íŒŒì¼ í˜•ì‹ í™•ì¸
python -c "
import pandas as pd
df = pd.read_csv('submissions/$(date +%Y%m%d)/submission.csv')
print(f'ì œì¶œ íŒŒì¼ í¬ê¸°: {df.shape}')
print(f'ì—´ ì´ë¦„: {list(df.columns)}')
print(f'í´ë˜ìŠ¤ ë¶„í¬: {dict(df[\"target\"].value_counts().head())}')
print(f'ëˆ„ë½ëœ ê°’: {df.isnull().sum().sum()}')
"

# ìƒ˜í”Œ ë°ì´í„° í™•ì¸
head -10 submissions/$(date +%Y%m%d)/submission.csv
```

---

## ğŸ“Š ì‹¤í—˜ ì¶”ì  ë° ê´€ë¦¬

### WandB ì„¤ì • ë° ì‚¬ìš©

#### 1. ì´ˆê¸° ì„¤ì •

```bash
# WandB ë¡œê·¸ì¸ (ìµœì´ˆ 1íšŒ)
wandb login

# API í‚¤ ì…ë ¥ í›„ í™•ì¸
wandb whoami
```

#### 2. íŒ€ í”„ë¡œì íŠ¸ ì„¤ì •

```yaml
# configs/train_highperf.yaml
wandb:
  enabled: true
  project_name: "document-classification-team"  # íŒ€ ê³µìš© í”„ë¡œì íŠ¸
  experiment_name: null                        # ìë™ ìƒì„±
  tags: ["team-experiment", "high-performance"]
```

#### 3. ê°œì¸ ì‹¤í—˜ ê´€ë¦¬

```bash
# ê°œì¸ ì‹¤í—˜ëª… ì„¤ì •
export WANDB_EXPERIMENT_NAME="ê¹€ì² ìˆ˜_swin384_hard_aug_$(date +%m%d)"
python src/training/train_main.py --config configs/train_highperf.yaml --mode full-pipeline
```

### ì‹¤í—˜ ë¹„êµ ë° ë¶„ì„

#### WandB ëŒ€ì‹œë³´ë“œ í™œìš©

1. **ì„±ëŠ¥ ë¹„êµ**: ì—¬ëŸ¬ ì‹¤í—˜ì˜ F1 ìŠ¤ì½”ì–´ ë¹„êµ
2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ì„**: ìµœì  ì„¤ì • íƒìƒ‰
3. **í•™ìŠµ ê³¡ì„ **: ê³¼ì í•©/ê³¼ì†Œì í•© ëª¨ë‹ˆí„°ë§
4. **ëª¨ë¸ ì•„í‹°íŒ©íŠ¸**: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```python
# WandB APIë¥¼ í†µí•œ ì‹¤í—˜ ë¶„ì„
import wandb

# íŒ€ í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ì‹¤í—˜ ì¡°íšŒ
api = wandb.Api()
runs = api.runs("document-classification-team")

# ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°
best_run = max(runs, key=lambda run: run.summary.get("best_f1", 0))
print(f"ìµœê³  F1 ì ìˆ˜: {best_run.summary['best_f1']:.4f}")
print(f"ì‹¤í—˜ëª…: {best_run.name}")
```

### ë¡œê·¸ íŒŒì¼ ê´€ë¦¬

```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°
logs/
â”œâ”€â”€ train/                    # í•™ìŠµ ë¡œê·¸
â”‚   â””â”€â”€ train_20250905-1715_v087-ef727c.log
â”œâ”€â”€ infer/                    # ì¶”ë¡  ë¡œê·¸
â”‚   â””â”€â”€ infer_20250905-1721_v087.log
â””â”€â”€ unit_test/               # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë¡œê·¸
    â”œâ”€â”€ highperf_dataset/
    â”œâ”€â”€ mixup_augmentation/
    â””â”€â”€ ...

# ë¡œê·¸ ë¶„ì„ ë„êµ¬
grep "best_f1" logs/train/*.log | tail -5
grep "ERROR" logs/train/*.log
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ìµœì í™”

#### 1. ë°°ì¹˜ í¬ê¸° ì¡°ì •

```python
# GPU ë©”ëª¨ë¦¬ë³„ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°
GPU_MEMORY_BATCH_SIZE = {
    8: {"img_size": 224, "batch_size": 16},   # RTX 3070/4060
    12: {"img_size": 384, "batch_size": 8},   # RTX 3080
    16: {"img_size": 384, "batch_size": 12},  # RTX 3080 Ti/4070 Ti
    24: {"img_size": 384, "batch_size": 16},  # RTX 3090/4090
}

# ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì •
python src/utils/auto_batch_size.py --config configs/train_highperf.yaml
```

#### 2. Mixed Precision í™œìš©

```yaml
# configs/train_highperf.yaml
training:
  mixed_precision: true      # AMP í™œì„±í™”
  gradient_clipping: 1.0     # Gradient clipping
```

#### 3. Gradient Accumulation

```yaml
training:
  batch_size: 8              # ì‹¤ì œ ë°°ì¹˜ í¬ê¸°
  accumulate_grad_batches: 2 # ìœ íš¨ ë°°ì¹˜ í¬ê¸° = 8 * 2 = 16
```

### í•™ìŠµ ì†ë„ ìµœì í™”

#### 1. ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”

```yaml
data:
  num_workers: 4           # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •
  pin_memory: true         # GPU ì „ì†¡ ê°€ì†í™”
  persistent_workers: true # Worker ì¬ì‚¬ìš©
```

#### 2. ëª¨ë¸ ì»´íŒŒì¼ (PyTorch 2.0+)

```python
# ëª¨ë¸ ì»´íŒŒì¼ë¡œ ì¶”ë¡  ì†ë„ í–¥ìƒ
model = torch.compile(model, mode="reduce-overhead")
```

#### 3. ìºì‹± ì „ëµ

```bash
# ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ìºì‹±
export CACHE_PREPROCESSED=true
python src/training/train_main.py --config configs/train_highperf.yaml --mode full-pipeline
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```bash
# ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
cat > monitor_gpu.sh << 'EOF'
#!/bin/bash
while true; do
    nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv,nounits,noheader
    sleep 5
done
EOF

chmod +x monitor_gpu.sh
./monitor_gpu.sh
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. CUDA Out of Memory

**ì¦ìƒ**: RuntimeError: CUDA out of memory

**í•´ê²° ë°©ë²•**:
```bash
# 1. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
vim configs/train_highperf.yaml  # batch_size: 16 â†’ 8

# 2. ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸°
# img_size: 384 â†’ 320 ë˜ëŠ” 224

# 3. Gradient Accumulation ì‚¬ìš©
# accumulate_grad_batches: 2

# 4. GPU ë©”ëª¨ë¦¬ ì •ë¦¬
python -c "import torch; torch.cuda.empty_cache()"
```

#### 2. í•™ìŠµì´ ë©ˆì¶”ëŠ” í˜„ìƒ

**ì¦ìƒ**: íŠ¹ì • ì—í¬í¬ì—ì„œ ì§„í–‰ì´ ë©ˆì¶¤

**ì§„ë‹¨ ë° í•´ê²°**:
```bash
# 1. í”„ë¡œì„¸ìŠ¤ ìƒíƒœ í™•ì¸
ps aux | grep python

# 2. GPU ì‚¬ìš©ë¥  í™•ì¸
nvidia-smi

# 3. ë°ì´í„° ë¡œë”© ë¬¸ì œ í™•ì¸
python -c "
from src.data.dataset import HighPerfDocClsDataset
# ë°ì´í„°ì…‹ ë¡œë”© í…ŒìŠ¤íŠ¸
"

# 4. ë¡œê·¸ í™•ì¸
tail -f logs/train/*.log
```

#### 3. ì„±ëŠ¥ì´ ê¸°ëŒ€ë³´ë‹¤ ë‚®ìŒ

**ì§„ë‹¨ ê³¼ì •**:
```bash
# 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
jupyter nbconvert --execute notebooks/test_highperf_dataset.ipynb
jupyter nbconvert --execute notebooks/test_swin_model.ipynb

# 2. ë°ì´í„° í’ˆì§ˆ í™•ì¸
python src/utils/analyze_data_quality.py

# 3. ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¶„ì„
python src/utils/analyze_model_weights.py --model_path experiments/train/latest/best_model_fold_1.pth
```

#### 4. WandB ì—°ë™ ì‹¤íŒ¨

**í•´ê²° ë°©ë²•**:
```bash
# 1. ë¡œê·¸ì¸ ë‹¤ì‹œ ì‹œë„
wandb login --relogin

# 2. ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ì„ì‹œ ì‚¬ìš©
export WANDB_MODE=offline
python src/training/train_main.py --config configs/train_highperf.yaml --mode full-pipeline

# 3. ì—°ê²° í…ŒìŠ¤íŠ¸
python -c "import wandb; wandb.init(project='test'); wandb.finish()"
```

### ë””ë²„ê¹… ë„êµ¬

#### 1. ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰

```bash
# ì†Œê·œëª¨ ë°ì´í„°ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (--debug ì˜µì…˜ì´ ì§€ì›ë˜ëŠ” ê²½ìš°)
python src/training/train_main.py --config configs/train_highperf.yaml --mode full-pipeline

# ìƒì„¸ ë¡œê¹… í™œì„±í™”
export LOG_LEVEL=DEBUG
python src/training/train_main.py --config configs/train_highperf.yaml --mode full-pipeline
```

#### 2. í”„ë¡œíŒŒì¼ë§

```bash
# ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
python -m cProfile -o profile.stats src/training/train_main.py --config configs/train_highperf.yaml --mode full-pipeline
python -c "
import pstats
stats = pstats.Stats('profile.stats')
stats.sort_stats('cumulative').print_stats(20)
"
```

#### 3. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
pip install memory_profiler
python -m memory_profiler src/training/train_main.py --config configs/train_highperf.yaml --mode full-pipeline
```

---

## ğŸ‘¥ íŒ€ í˜‘ì—… ì›Œí¬í”Œë¡œìš°

### ë¸Œëœì¹˜ ì „ëµ

```bash
# ê°œì¸ ì‘ì—… ë¸Œëœì¹˜ ìƒì„±
git checkout -b feature/ê¹€ì² ìˆ˜-swin-optimization
git checkout -b experiment/ì´ì˜í¬-mixup-tuning

# ì‘ì—… ì™„ë£Œ í›„ ë©”ì¸ ë¸Œëœì¹˜ì— ë³‘í•©
git checkout main
git merge feature/ê¹€ì² ìˆ˜-swin-optimization
```

### ì‹¤í—˜ ëª…ëª… ê·œì¹™

```bash
# í˜•ì‹: {ì´ë¦„}_{ëª¨ë¸}_{ì£¼ìš”ë³€ê²½ì‚¬í•­}_{ë‚ ì§œ}
ì‹¤í—˜ëª… ì˜ˆì‹œ:
- "ê¹€ì² ìˆ˜_swin384_hard_aug_0905"
- "ì´ì˜í¬_convnext_mixup02_0906"
- "ë°•ë¯¼ìˆ˜_ensemble_tta8_0907"
```

### ê²°ê³¼ ê³µìœ  í”„ë¡œì„¸ìŠ¤

#### 1. ì‹¤í—˜ ì™„ë£Œ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] WandBì— ì‹¤í—˜ ê²°ê³¼ ì—…ë¡œë“œ ì™„ë£Œ
- [ ] ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ íŒ€ ê³µìœ  ë””ë ‰í† ë¦¬ì— ì €ì¥
- [ ] ì‹¤í—˜ ë…¸íŠ¸ ì‘ì„± (ì„¤ì •, ê²°ê³¼, ì¸ì‚¬ì´íŠ¸)
- [ ] íŒ€ ìŠ¬ë™/ì±„ë„ì— ê²°ê³¼ ê³µìœ 

#### 2. ì½”ë“œ ë¦¬ë·° í”„ë¡œì„¸ìŠ¤

```bash
# Pull Request ìƒì„±
git push origin feature/ê¹€ì² ìˆ˜-swin-optimization
# GitHubì—ì„œ PR ìƒì„±

# ë¦¬ë·° ìš”ì²­
@íŒ€ì›1 @íŒ€ì›2 ì½”ë“œ ë¦¬ë·° ë¶€íƒë“œë¦½ë‹ˆë‹¤.
ì£¼ìš” ë³€ê²½ì‚¬í•­: Swin Transformer ìµœì í™”
ì„±ëŠ¥ í–¥ìƒ: F1 0.87 â†’ 0.92
```

#### 3. ëª¨ë¸ ì•™ìƒë¸” ì „ëµ

```python
# íŒ€ì›ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìˆ˜ì§‘
TEAM_MODELS = {
    "ê¹€ì² ìˆ˜": "experiments/team/kimcs_swin384_f1_0928.pth",
    "ì´ì˜í¬": "experiments/team/leeyh_convnext_f1_0934.pth", 
    "ë°•ë¯¼ìˆ˜": "experiments/team/parkms_ensemble_f1_0931.pth"
}

# ì•™ìƒë¸” ì¶”ë¡  ì‹¤í–‰
python src/inference/team_ensemble.py --models TEAM_MODELS
```

### ìµœì¢… ì œì¶œ í”„ë¡œì„¸ìŠ¤

#### 1. ìµœì¢… ëª¨ë¸ ì„ ì • íšŒì˜

- **ì°¸ì„ì**: ì „ì²´ íŒ€ì›
- **ì•ˆê±´**: 
  - ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
  - ì•™ìƒë¸” ì „ëµ ê²°ì •
  - ìµœì¢… ì œì¶œ íŒŒì¼ ì„ ì •

#### 2. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±

```bash
# ìµœì¢… ì•™ìƒë¸” ëª¨ë¸ ì‹¤í–‰
python src/inference/final_ensemble.py \
    --models experiments/team/final_selection/ \
    --output submissions/final_submission_$(date +%Y%m%d).csv \
    --tta_enabled \
    --confidence_threshold 0.95

# ì œì¶œ íŒŒì¼ ê²€ì¦
python src/utils/validate_submission.py \
    --submission submissions/final_submission_$(date +%Y%m%d).csv \
    --sample_submission data/raw/sample_submission.csv
```

#### 3. ë°±ì—… ë° ë¬¸ì„œí™”

```bash
# ìµœì¢… ëª¨ë¸ ë°±ì—…
tar -czf final_models_$(date +%Y%m%d).tar.gz experiments/team/final_selection/

# ì‹¤í—˜ íˆìŠ¤í† ë¦¬ ë¬¸ì„œí™”
python src/utils/generate_experiment_report.py \
    --wandb_project document-classification-team \
    --output docs/Final_Experiment_Report.md
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ëª©í‘œ

### í˜„ì¬ ì„±ëŠ¥ ê¸°ì¤€ì„ 

- **ê¸°ì¡´ ëª¨ë“ˆí™” ë²„ì „**: F1 Score 0.372 (ì„±ëŠ¥ ì €í•˜ ë¬¸ì œ)
- **ì›ë³¸ ë…¸íŠ¸ë¶ ë²„ì „**: F1 Score 0.870
- **íŒ€ ìµœê³  ì„±ëŠ¥**: F1 Score 0.934 (ConvNext ëª¨ë¸)

### ëª©í‘œ ì„±ëŠ¥

- **ë‹¨ê¸° ëª©í‘œ**: F1 Score 0.900+ (ì•ˆì •ì  0.9 ë‹¬ì„±)
- **ì¤‘ê¸° ëª©í‘œ**: F1 Score 0.935+ (íŒ€ ìµœê³  ê¸°ë¡ ê°±ì‹ )
- **ìµœì¢… ëª©í‘œ**: F1 Score 0.940+ (ìƒìœ„ 5% ì§„ì…)

### ì„±ëŠ¥ í–¥ìƒ ë¡œë“œë§µ

#### Phase 1: ê¸°ë°˜ ì‹œìŠ¤í…œ ì•ˆì •í™” âœ…
- [x] ëª¨ë“ˆí™” ì‹œìŠ¤í…œ êµ¬ì¶•
- [x] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•
- [x] WandB ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ
- [x] í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬í˜„

#### Phase 2: ê³ ì„±ëŠ¥ ëª¨ë¸ í†µí•© âœ…
- [x] Swin Transformer í†µí•©
- [x] Hard Augmentation êµ¬í˜„
- [x] Mixup ë°ì´í„° ì¦ê°•
- [x] 5-Fold Cross Validation

#### Phase 3: ì„±ëŠ¥ ìµœì í™” (ì§„í–‰ ì¤‘)
- [ ] TTA (Test Time Augmentation) ìµœì í™”
- [ ] ëª¨ë¸ ì•™ìƒë¸” ì „ëµ ê°œì„ 
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹
- [ ] ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì²˜ë¦¬ ìµœì í™”

#### Phase 4: ìµœì¢… íŠœë‹ (ì˜ˆì •)
- [ ] Pseudo Labeling ê¸°ë²• ì ìš©
- [ ] ëª¨ë¸ ì¦ë¥˜ (Knowledge Distillation)
- [ ] í›„ì²˜ë¦¬ ìµœì í™”
- [ ] ì œì¶œ ì „ëµ ë‹¤ì–‘í™”

---

## ğŸ¯ ë§ˆë¬´ë¦¬ ë° ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¼ì¼ ì‘ì—… ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì‹¤í—˜ ì‹œì‘ ì „
- [ ] GPU ë©”ëª¨ë¦¬ ë° ìƒíƒœ í™•ì¸
- [ ] ì´ì „ ì‹¤í—˜ ê²°ê³¼ ë°±ì—…
- [ ] WandB í”„ë¡œì íŠ¸ ì„¤ì • í™•ì¸
- [ ] ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦

#### ì‹¤í—˜ ì‹¤í–‰ ì¤‘
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (GPU, ë©”ëª¨ë¦¬)
- [ ] ë¡œê·¸ íŒŒì¼ ì£¼ê¸°ì  í™•ì¸
- [ ] WandB ëŒ€ì‹œë³´ë“œ ëª¨ë‹ˆí„°ë§
- [ ] ì¤‘ê°„ ê²°ê³¼ ìŠ¤ëƒ…ìƒ· ì €ì¥

#### ì‹¤í—˜ ì™„ë£Œ í›„
- [ ] ìµœì¢… ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡
- [ ] ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë°±ì—…
- [ ] ì‹¤í—˜ ë…¸íŠ¸ ì‘ì„±
- [ ] íŒ€ì›ë“¤ê³¼ ê²°ê³¼ ê³µìœ 

### ì£¼ê°„ ì ê²€ ì‚¬í•­

- [ ] ì „ì²´ íŒŒì´í”„ë¼ì¸ End-to-End í…ŒìŠ¤íŠ¸
- [ ] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì„±ëŠ¥ ì €í•˜ ê²€ì‚¬
- [ ] ì½”ë“œ ë¦¬íŒ©í† ë§ ë° ìµœì í™”
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸

### ìµœì¢… ì œì¶œ ì „ í™•ì¸ì‚¬í•­

- [ ] ëª¨ë“  ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì œì¶œ íŒŒì¼ í˜•ì‹ ê²€ì¦
- [ ] ì¬í˜„ì„± í…ŒìŠ¤íŠ¸ (ë™ì¼ ì‹œë“œë¡œ ë™ì¼ ê²°ê³¼)
- [ ] ë°±ì—… ì œì¶œ íŒŒì¼ ì¤€ë¹„
- [ ] íŒ€ ì „ì²´ ìŠ¹ì¸

---

## ğŸš€ Quick Start ëª…ë ¹ì–´ ëª¨ìŒ

### ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë“¤

```bash
# 1. ì „ì²´ ì‹œìŠ¤í…œ ì²´í¬
python -c "
from src.utils.common import load_yaml
from src.data.dataset import HighPerfDocClsDataset
from src.models.build import create_model
print('âœ… ëª¨ë“  ëª¨ë“ˆ ì •ìƒ ë¡œë“œ')
"

# 2. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ë¶„) - ì„¤ì • íŒŒì¼ í•„ìˆ˜
python src/training/train_main.py --config configs/train_highperf.yaml --mode full-pipeline

# 3. ì‹¤ì œ ê³ ì„±ëŠ¥ í•™ìŠµ (2-3ì‹œê°„)
nohup python src/training/train_main.py --config configs/train_highperf.yaml --mode full-pipeline > training_$(date +%m%d_%H%M).log 2>&1 &

# 4. ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§
tail -f training_*.log
watch -n 10 "nvidia-smi | head -15"

# 5. ê²°ê³¼ í™•ì¸
ls -la submissions/$(date +%Y%m%d)/
head -5 submissions/$(date +%Y%m%d)/submission.csv
```

### ê¸´ê¸‰ ìƒí™© ëŒ€ì‘

```bash
# GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ê¸´ê¸‰ ëŒ€ì‘
export CUDA_VISIBLE_DEVICES=0
python src/training/train_main.py --config configs/emergency_small.yaml --mode full-pipeline

# ë¹ ë¥¸ ì„±ëŠ¥ í™•ì¸ (1ì‹œê°„) - ê¸°ë³¸ ëª¨ë“œëŠ” config í•„ìˆ˜
python src/training/train_main.py --config configs/train.yaml --mode basic

# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
python -c "
import torch
import gc
torch.cuda.empty_cache()
gc.collect()
print('ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ')
"
```
