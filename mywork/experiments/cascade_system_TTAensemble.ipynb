{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b33696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c278793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "def free_cuda():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33cc277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fcc030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target vulnerable classes: [3, 4, 7, 14]\n"
     ]
    }
   ],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "model_name = 'convnext_base_384_in22ft1k'  # ê¸°ì¡´ê³¼ ë™ì¼í•œ ëª¨ë¸\n",
    "img_size = 512\n",
    "LR = 2e-4\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "num_workers = 8\n",
    "\n",
    "# ì·¨ì•½ í´ë˜ìŠ¤ ì„¤ì •\n",
    "vulnerable_classes = [3, 4, 7, 14]\n",
    "print(f\"Target vulnerable classes: {vulnerable_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf816a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼, __init__ë§Œ ìˆ˜ì •)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, path, epoch=0, total_epochs=10, is_train=True):\n",
    "        if isinstance(data, str):\n",
    "            df_temp = pd.read_csv(data)\n",
    "        else:\n",
    "            df_temp = data\n",
    "        \n",
    "        # ìˆ˜ì •: í•­ìƒ ['ID', 'target'] ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ self.df ì´ˆê¸°í™”\n",
    "        self.df = df_temp[['ID', 'target']].values\n",
    "        self.path = path\n",
    "        self.epoch = epoch\n",
    "        self.total_epochs = total_epochs\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Hard augmentation í™•ë¥  ê³„ì‚°\n",
    "        self.p_hard = 0.2 + 0.3 * (epoch / total_epochs) if is_train else 0\n",
    "        \n",
    "        # Normal augmentation\n",
    "        self.normal_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=img_size),\n",
    "            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=[90, 90], p=1.0),\n",
    "                A.Rotate(limit=[180, 180], p=1.0),\n",
    "                A.Rotate(limit=[270, 270], p=1.0),\n",
    "            ], p=0.6),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n",
    "            A.GaussNoise(var_limit=(30.0, 100.0), p=0.7),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        # Hard augmentation\n",
    "        self.hard_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=img_size),\n",
    "            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=[90, 90], p=1.0),\n",
    "                A.Rotate(limit=[180, 180], p=1.0),\n",
    "                A.Rotate(limit=[270, 270], p=1.0),\n",
    "                A.Rotate(limit=[-15, 15], p=1.0),\n",
    "            ], p=0.8),\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(blur_limit=15, p=1.0),\n",
    "                A.GaussianBlur(blur_limit=15, p=1.0),\n",
    "            ], p=0.95),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, p=0.9),\n",
    "            A.GaussNoise(var_limit=(50.0, 150.0), p=0.8),\n",
    "            A.JpegCompression(quality_lower=70, quality_upper=100, p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)).convert('RGB'))\n",
    "        \n",
    "        # ë°°ì¹˜ë³„ ì¦ê°• ì„ íƒ\n",
    "        if self.is_train and random.random() < self.p_hard:\n",
    "            img = self.hard_aug(image=img)['image']\n",
    "        else:\n",
    "            img = self.normal_aug(image=img)['image']\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83201539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixup í•¨ìˆ˜ ì •ì˜\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Cutmix/Mixup ì ìš© (30% í™•ë¥ )\n",
    "        if random.random() < 0.3:\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(image, targets, alpha=1.0)\n",
    "            with autocast(): \n",
    "                preds = model(mixed_x)\n",
    "            loss = lam * loss_fn(preds, y_a) + (1 - lam) * loss_fn(preds, y_b)\n",
    "        else:\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "# ê²€ì¦ í•¨ìˆ˜\n",
    "def validate_one_epoch(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Validating\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "            \n",
    "            pbar.set_description(f\"Val Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "    \n",
    "    return {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d0d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 1570\n",
      "Filtered dataset size: 350\n",
      "\n",
      "Class distribution:\n",
      "Class 3: 100 samples\n",
      "Class 4: 100 samples\n",
      "Class 7: 100 samples\n",
      "Class 14: 50 samples\n",
      "\n",
      "Label mapping:\n",
      "Original class 3 -> New class 0\n",
      "Original class 4 -> New class 1\n",
      "Original class 7 -> New class 2\n",
      "Original class 14 -> New class 3\n",
      "\n",
      "New class distribution:\n",
      "New class 0: 100 samples\n",
      "New class 1: 100 samples\n",
      "New class 2: 100 samples\n",
      "New class 3: 50 samples\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 1. ì·¨ì•½ í´ë˜ìŠ¤ ë°ì´í„° ì¤€ë¹„\n",
    "# ========================================\n",
    "\n",
    "# ì›ë³¸ ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "print(f\"Original dataset size: {len(train_df)}\")\n",
    "\n",
    "# ì·¨ì•½ í´ë˜ìŠ¤ë§Œ í•„í„°ë§\n",
    "filtered_df = train_df[train_df['target'].isin(vulnerable_classes)].copy()\n",
    "print(f\"Filtered dataset size: {len(filtered_df)}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸\n",
    "print(\"\\nClass distribution:\")\n",
    "for cls in vulnerable_classes:\n",
    "    count = len(filtered_df[filtered_df['target'] == cls])\n",
    "    print(f\"Class {cls}: {count} samples\")\n",
    "\n",
    "# ë¼ë²¨ ì¬ë§¤í•‘ (3->0, 4->1, 7->2, 14->3)\n",
    "label_mapping = {3: 0, 4: 1, 7: 2, 14: 3}\n",
    "filtered_df['original_target'] = filtered_df['target']  # ì›ë³¸ ë¼ë²¨ ë³´ì¡´\n",
    "filtered_df['target'] = filtered_df['target'].map(label_mapping)\n",
    "\n",
    "print(\"\\nLabel mapping:\")\n",
    "for orig, new in label_mapping.items():\n",
    "    print(f\"Original class {orig} -> New class {new}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸\n",
    "print(\"\\nNew class distribution:\")\n",
    "for new_cls in range(4):\n",
    "    count = len(filtered_df[filtered_df['target'] == new_cls])\n",
    "    print(f\"New class {new_cls}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2. 3-Fold Cross Validationìœ¼ë¡œ ì„œë¸Œì…‹ ëª¨ë¸ í•™ìŠµ\n",
    "# ========================================\n",
    "\n",
    "# 3-Fold ì„¤ì •\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "fold_results = []\n",
    "fold_models = []\n",
    "\n",
    "print(f\"Starting {N_FOLDS}-Fold Cross Validation for Subset Model...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(filtered_df, filtered_df['target'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"SUBSET FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ train/validation ë°ì´í„° ë¶„í• \n",
    "    train_fold_df = filtered_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_fold_df = filtered_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ Dataset ìƒì„±\n",
    "    trn_dataset = ImageDataset(\n",
    "        train_fold_df,\n",
    "        \"../data/train/\",\n",
    "        epoch=0,\n",
    "        total_epochs=EPOCHS,\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = ImageDataset(\n",
    "        val_fold_df,\n",
    "        \"../data/train/\",\n",
    "        epoch=0,\n",
    "        total_epochs=EPOCHS,\n",
    "        is_train=False\n",
    "    )\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ DataLoader ìƒì„±\n",
    "    trn_loader = DataLoader(\n",
    "        trn_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(trn_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™” (4ê°œ í´ë˜ìŠ¤)\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=4  # ì·¨ì•½ í´ë˜ìŠ¤ 4ê°œ\n",
    "    ).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # í˜„ì¬ foldì˜ ìµœê³  ì„±ëŠ¥ ì¶”ì \n",
    "    best_val_f1 = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    # í˜„ì¬ fold í•™ìŠµ\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_ret = validate_one_epoch(val_loader, model, loss_fn, device)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d} | \"\n",
    "              f\"Train Loss: {train_ret['train_loss']:.4f} | \"\n",
    "              f\"Train F1: {train_ret['train_f1']:.4f} | \"\n",
    "              f\"Val Loss: {val_ret['val_loss']:.4f} | \"\n",
    "              f\"Val F1: {val_ret['val_f1']:.4f}\")\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "        if val_ret['val_f1'] > best_val_f1:\n",
    "            best_val_f1 = val_ret['val_f1']\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # í˜„ì¬ fold ê²°ê³¼ ì €ì¥\n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'train_samples': len(trn_dataset),\n",
    "        'val_samples': len(val_dataset)\n",
    "    })\n",
    "    \n",
    "    fold_models.append(best_model)\n",
    "    \n",
    "    print(f\"Subset Fold {fold + 1} Best Validation F1: {best_val_f1:.4f}\")\n",
    "\n",
    "# ê²°ê³¼ ìš”ì•½\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUBSET MODEL CROSS VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "val_f1_scores = [result['best_val_f1'] for result in fold_results]\n",
    "mean_f1 = np.mean(val_f1_scores)\n",
    "std_f1 = np.std(val_f1_scores)\n",
    "\n",
    "for result in fold_results:\n",
    "    print(f\"Fold {result['fold']}: {result['best_val_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV F1: {mean_f1:.4f} Â± {std_f1:.4f}\")\n",
    "print(f\"Best single fold: {max(val_f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d030b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3. ì„œë¸Œì…‹ ëª¨ë¸ ì €ì¥\n",
    "# ========================================\n",
    "\n",
    "# ì„œë¸Œì…‹ ëª¨ë¸ë“¤ ì €ì¥\n",
    "save_dir = \"subset_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nSaving subset models to {save_dir}/\")\n",
    "for fold, state_dict in enumerate(fold_models):\n",
    "    model_path = f\"{save_dir}/subset_fold_{fold}_model.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': state_dict,\n",
    "        'fold': fold,\n",
    "        'classes': vulnerable_classes,\n",
    "        'label_mapping': label_mapping,\n",
    "        'model_name': model_name,\n",
    "        'img_size': img_size,\n",
    "        'num_classes': 4,\n",
    "        'best_f1': fold_results[fold]['best_val_f1']\n",
    "    }, model_path)\n",
    "    print(f\"âœ… Fold {fold} model saved: {model_path}\")\n",
    "\n",
    "print(\"\\nğŸ‰ 4-Class subset training completed!\")\n",
    "print(f\"ğŸ“Š Final Results Summary:\")\n",
    "print(f\"   - Target classes: {vulnerable_classes}\")\n",
    "print(f\"   - Training samples: {len(filtered_df)}\")\n",
    "print(f\"   - Mean CV F1: {mean_f1:.4f} Â± {std_f1:.4f}\")\n",
    "print(f\"   - Models saved in: {save_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f64e3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ë…¸íŠ¸ë¶ ì…€ 4 ìˆ˜ì •: ê¸°ì¡´ CascadeClassifierì— TTAë§Œ ì¶”ê°€\n",
    "# ========================================\n",
    "\n",
    "\n",
    "class CascadeClassifier:\n",
    "    \"\"\"\n",
    "    TTAê°€ ì¶”ê°€ëœ 2ë‹¨ê³„ ìºìŠ¤ì¼€ì´ë“œ ë¶„ë¥˜ ì‹œìŠ¤í…œ\n",
    "    \n",
    "    1ë‹¨ê³„: ë¶„ë¥˜ê¸° A (17ê°œ í´ë˜ìŠ¤ ì „ì²´ ë¶„ë¥˜) â†’ TTA + K-fold ì•™ìƒë¸”\n",
    "    2ë‹¨ê³„: ë¶„ë¥˜ê¸° B (ì·¨ì•½ í´ë˜ìŠ¤ 3,4,7,14ë§Œ ë¶„ë¥˜) â†’ TTA + K-fold ì•™ìƒë¸”\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, main_models, subset_models, vulnerable_classes=[3,4,7,14], \n",
    "                 confidence_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            main_models: ë¶„ë¥˜ê¸° Aì˜ ì•™ìƒë¸” ëª¨ë¸ë“¤ (17ê°œ í´ë˜ìŠ¤)\n",
    "            subset_models: ë¶„ë¥˜ê¸° Bì˜ ì•™ìƒë¸” ëª¨ë¸ë“¤ (4ê°œ í´ë˜ìŠ¤)\n",
    "            vulnerable_classes: ì·¨ì•½ í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸\n",
    "            confidence_threshold: 2ë‹¨ê³„ ë¶„ë¥˜ê¸°ë¡œ ë„˜ì–´ê°ˆ ì‹ ë¢°ë„ ì„ê³„ê°’\n",
    "        \"\"\"\n",
    "        self.main_models = main_models\n",
    "        self.subset_models = subset_models\n",
    "        self.vulnerable_classes = vulnerable_classes\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # ì·¨ì•½ í´ë˜ìŠ¤ ë§¤í•‘ (ì›ë³¸ í´ë˜ìŠ¤ -> ì„œë¸Œì…‹ í´ë˜ìŠ¤)\n",
    "        self.class_mapping = {cls: idx for idx, cls in enumerate(vulnerable_classes)}\n",
    "        \n",
    "        # ê¸°ì¡´ ì‚¬ìš©ìì˜ TTA ë³€í™˜ë“¤ ì„¤ì •\n",
    "        self.essential_tta_transforms = self._setup_tta_transforms()\n",
    "        \n",
    "        print(f\"TTA ìºìŠ¤ì¼€ì´ë“œ ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        print(f\"- ì·¨ì•½ í´ë˜ìŠ¤: {vulnerable_classes}\")\n",
    "        print(f\"- ì‹ ë¢°ë„ ì„ê³„ê°’: {confidence_threshold}\")\n",
    "        print(f\"- ë©”ì¸ ëª¨ë¸ ìˆ˜: {len(main_models)}\")\n",
    "        print(f\"- ì„œë¸Œì…‹ ëª¨ë¸ ìˆ˜: {len(subset_models)}\")\n",
    "        print(f\"- TTA ë³€í™˜ ìˆ˜: {len(self.essential_tta_transforms)}\")\n",
    "    \n",
    "    def _setup_tta_transforms(self):\n",
    "        \"\"\"ì‚¬ìš©ìì˜ ê¸°ì¡´ TTA ë³€í™˜ë“¤ ì„¤ì •\"\"\"\n",
    "        img_size = 512\n",
    "        \n",
    "        return [\n",
    "            # ì›ë³¸\n",
    "            A.Compose([\n",
    "                A.LongestMaxSize(max_size=img_size),\n",
    "                A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ]),\n",
    "            # 90ë„ íšŒì „\n",
    "            A.Compose([\n",
    "                A.LongestMaxSize(max_size=img_size),\n",
    "                A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "                A.Rotate(limit=[90, 90], p=1.0),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ]),\n",
    "            # 180ë„ íšŒì „\n",
    "            A.Compose([\n",
    "                A.LongestMaxSize(max_size=img_size),\n",
    "                A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "                A.Rotate(limit=[180, 180], p=1.0),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ]),\n",
    "            # -90ë„ íšŒì „ (270ë„)\n",
    "            A.Compose([\n",
    "                A.LongestMaxSize(max_size=img_size),\n",
    "                A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "                A.Rotate(limit=[-90, -90], p=1.0),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ]),\n",
    "            # ë°ê¸° ê°œì„ \n",
    "            A.Compose([\n",
    "                A.LongestMaxSize(max_size=img_size),\n",
    "                A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "                A.RandomBrightnessContrast(brightness_limit=[0.3, 0.3], contrast_limit=[0.3, 0.3], p=1.0),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ]),\n",
    "        ]\n",
    "    \n",
    "    def _apply_tta_to_image(self, image_array):\n",
    "        \"\"\"numpy ì´ë¯¸ì§€ ë°°ì—´ì— ëª¨ë“  TTA ë³€í™˜ ì ìš©\"\"\"\n",
    "        tta_tensors = []\n",
    "        for transform in self.essential_tta_transforms:\n",
    "            transformed = transform(image=image_array)['image']\n",
    "            tta_tensors.append(transformed)\n",
    "        return tta_tensors\n",
    "    \n",
    "    def predict_single(self, image, device):\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ìºìŠ¤ì¼€ì´ë“œ ì˜ˆì¸¡\n",
    "        \n",
    "        Args:\n",
    "            image: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í…ì„œ [C, H, W] ë˜ëŠ” numpy ë°°ì—´\n",
    "            device: GPU/CPU ë””ë°”ì´ìŠ¤\n",
    "            \n",
    "        Returns:\n",
    "            final_prediction: ìµœì¢… ì˜ˆì¸¡ í´ë˜ìŠ¤\n",
    "            confidence: ì˜ˆì¸¡ ì‹ ë¢°ë„\n",
    "            used_cascade: ì‚¬ìš©ëœ ë¶„ë¥˜ê¸° ('main' ë˜ëŠ” 'cascade')\n",
    "        \"\"\"\n",
    "        # ì…ë ¥ì´ í…ì„œì¸ ê²½ìš° numpyë¡œ ë³€í™˜ (TTAë¥¼ ìœ„í•´)\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            # í…ì„œë¥¼ ë‹¤ì‹œ PIL -> numpyë¡œ ë³€í™˜í•´ì•¼ í•¨ (ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ TTA ì ìš©ì„ ìœ„í•´)\n",
    "            # ì‹¤ì œë¡œëŠ” ì›ë³¸ ì´ë¯¸ì§€ íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¢‹ìŒ\n",
    "            # ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ë¥¼ ìœ„í•œ ì„ì‹œ ì²˜ë¦¬\n",
    "            image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "            # ì •ê·œí™” ì—­ë³€í™˜ (ëŒ€ëµì )\n",
    "            image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "        else:\n",
    "            image_np = image\n",
    "        \n",
    "        # 1ë‹¨ê³„: ë©”ì¸ ë¶„ë¥˜ê¸°ë¡œ TTA + K-fold ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "        main_probs = self._predict_main_ensemble(image_np, device)\n",
    "        main_pred = torch.argmax(main_probs).item()\n",
    "        main_confidence = torch.max(main_probs).item()\n",
    "        \n",
    "        # 1ë‹¨ê³„ ì˜ˆì¸¡ì´ ì·¨ì•½ í´ë˜ìŠ¤ì´ê³  ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ 2ë‹¨ê³„ë¡œ\n",
    "        if (main_pred in self.vulnerable_classes and \n",
    "            main_confidence < self.confidence_threshold):\n",
    "            \n",
    "            # 2ë‹¨ê³„: ì„œë¸Œì…‹ ë¶„ë¥˜ê¸°ë¡œ TTA + K-fold ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "            subset_probs = self._predict_subset_ensemble(image_np, device)\n",
    "            subset_pred_idx = torch.argmax(subset_probs).item()\n",
    "            subset_confidence = torch.max(subset_probs).item()\n",
    "            \n",
    "            # ì„œë¸Œì…‹ ì˜ˆì¸¡ì„ ì›ë³¸ í´ë˜ìŠ¤ë¡œ ë³€í™˜\n",
    "            final_prediction = self.vulnerable_classes[subset_pred_idx]\n",
    "            final_confidence = subset_confidence\n",
    "            used_cascade = 'cascade'\n",
    "            \n",
    "            print(f\"ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: {main_pred}({main_confidence:.3f}) -> {final_prediction}({subset_confidence:.3f})\")\n",
    "            \n",
    "        else:\n",
    "            # 1ë‹¨ê³„ ì˜ˆì¸¡ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "            final_prediction = main_pred\n",
    "            final_confidence = main_confidence\n",
    "            used_cascade = 'main'\n",
    "        \n",
    "        return final_prediction, final_confidence, used_cascade\n",
    "    \n",
    "    def _predict_main_ensemble(self, image_array, device):\n",
    "        \"\"\"ë©”ì¸ ë¶„ë¥˜ê¸° TTA + K-fold ì•™ìƒë¸” ì˜ˆì¸¡\"\"\"\n",
    "        # TTA ì ìš©\n",
    "        tta_tensors = self._apply_tta_to_image(image_array)\n",
    "        all_predictions = []\n",
    "        \n",
    "        # ê° TTA ë³€í™˜ì— ëŒ€í•´ K-fold ì•™ìƒë¸”\n",
    "        for tta_tensor in tta_tensors:\n",
    "            tta_tensor = tta_tensor.unsqueeze(0).to(device)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
    "            \n",
    "            # K-fold ì•™ìƒë¸”\n",
    "            fold_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for model in self.main_models:\n",
    "                    model.eval()\n",
    "                    preds = model(tta_tensor)\n",
    "                    probs = torch.softmax(preds, dim=1)\n",
    "                    fold_predictions.append(probs)\n",
    "            \n",
    "            # K-fold í‰ê· \n",
    "            fold_ensemble = torch.mean(torch.stack(fold_predictions), dim=0)\n",
    "            all_predictions.append(fold_ensemble)\n",
    "        \n",
    "        # TTA í‰ê· \n",
    "        final_prediction = torch.mean(torch.stack(all_predictions), dim=0).squeeze()\n",
    "        return final_prediction\n",
    "    \n",
    "    def _predict_subset_ensemble(self, image_array, device):\n",
    "        \"\"\"ì„œë¸Œì…‹ ë¶„ë¥˜ê¸° TTA + K-fold ì•™ìƒë¸” ì˜ˆì¸¡\"\"\"\n",
    "        # TTA ì ìš©\n",
    "        tta_tensors = self._apply_tta_to_image(image_array)\n",
    "        all_predictions = []\n",
    "        \n",
    "        # ê° TTA ë³€í™˜ì— ëŒ€í•´ K-fold ì•™ìƒë¸”\n",
    "        for tta_tensor in tta_tensors:\n",
    "            tta_tensor = tta_tensor.unsqueeze(0).to(device)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
    "            \n",
    "            # K-fold ì•™ìƒë¸”\n",
    "            fold_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for model in self.subset_models:\n",
    "                    model.eval()\n",
    "                    preds = model(tta_tensor)\n",
    "                    probs = torch.softmax(preds, dim=1)\n",
    "                    fold_predictions.append(probs)\n",
    "            \n",
    "            # K-fold í‰ê· \n",
    "            fold_ensemble = torch.mean(torch.stack(fold_predictions), dim=0)\n",
    "            all_predictions.append(fold_ensemble)\n",
    "        \n",
    "        # TTA í‰ê· \n",
    "        final_prediction = torch.mean(torch.stack(all_predictions), dim=0).squeeze()\n",
    "        return final_prediction\n",
    "    \n",
    "    def predict_batch(self, dataloader, device):\n",
    "        \"\"\"\n",
    "        ë°°ì¹˜ ë°ì´í„°ì— ëŒ€í•œ ìºìŠ¤ì¼€ì´ë“œ ì˜ˆì¸¡\n",
    "        \n",
    "        Args:\n",
    "            dataloader: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”\n",
    "            device: GPU/CPU ë””ë°”ì´ìŠ¤\n",
    "            \n",
    "        Returns:\n",
    "            predictions: ìµœì¢… ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸\n",
    "            confidences: ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¦¬ìŠ¤íŠ¸\n",
    "            cascade_usage: ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš© í†µê³„\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        all_confidences = []\n",
    "        cascade_usage = {'main': 0, 'cascade': 0}\n",
    "        \n",
    "        for images, _ in tqdm(dataloader, desc=\"TTA Cascade Prediction\"):\n",
    "            batch_predictions = []\n",
    "            batch_confidences = []\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                single_image = images[i]\n",
    "                pred, conf, used = self.predict_single(single_image, device)\n",
    "                \n",
    "                batch_predictions.append(pred)\n",
    "                batch_confidences.append(conf)\n",
    "                cascade_usage[used] += 1\n",
    "            \n",
    "            all_predictions.extend(batch_predictions)\n",
    "            all_confidences.extend(batch_confidences)\n",
    "        \n",
    "        return all_predictions, all_confidences, cascade_usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5057054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë©”ì¸ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...\n",
      "âœ… ë©”ì¸ ëª¨ë¸ 1 ë¡œë“œ ì™„ë£Œ\n",
      "âœ… ë©”ì¸ ëª¨ë¸ 2 ë¡œë“œ ì™„ë£Œ\n",
      "âœ… ë©”ì¸ ëª¨ë¸ 3 ë¡œë“œ ì™„ë£Œ\n",
      "âœ… ë©”ì¸ ëª¨ë¸ 4 ë¡œë“œ ì™„ë£Œ\n",
      "âœ… ë©”ì¸ ëª¨ë¸ 5 ë¡œë“œ ì™„ë£Œ\n",
      "ì´ 5ê°œì˜ ë©”ì¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "ì„œë¸Œì…‹ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...\n",
      "âœ… ì„œë¸Œì…‹ ëª¨ë¸ 0 ë¡œë“œ ì™„ë£Œ (F1: 0.8820)\n",
      "âœ… ì„œë¸Œì…‹ ëª¨ë¸ 1 ë¡œë“œ ì™„ë£Œ (F1: 0.9059)\n",
      "âœ… ì„œë¸Œì…‹ ëª¨ë¸ 2 ë¡œë“œ ì™„ë£Œ (F1: 0.8886)\n",
      "âœ… ì„œë¸Œì…‹ ëª¨ë¸ 3 ë¡œë“œ ì™„ë£Œ (F1: 0.9750)\n",
      "âœ… ì„œë¸Œì…‹ ëª¨ë¸ 4 ë¡œë“œ ì™„ë£Œ (F1: 0.8390)\n",
      "ì´ 5ê°œì˜ ì„œë¸Œì…‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 5. ë©”ì¸ ëª¨ë¸ê³¼ ì„œë¸Œì…‹ ëª¨ë¸ ë¡œë“œ\n",
    "# ========================================\n",
    "\n",
    "# ë©”ì¸ ëª¨ë¸ë“¤ ë¡œë“œ (17ê°œ í´ë˜ìŠ¤)\n",
    "print(\"ë©”ì¸ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...\")\n",
    "main_models = []\n",
    "for fold in range(5):\n",
    "\n",
    "    #model_path = f\"best_model_fold_{fold+1}.pth\"\n",
    "    #model_path = f\"fold_{fold+1}_best.pth\"\n",
    "    model_path = f\"BH_512_base_best_model_fold_{fold+1}.pth\"\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        # ë©”ì¸ ëª¨ë¸ ìƒì„± (17ê°œ í´ë˜ìŠ¤)\n",
    "        main_model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "        main_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        main_model.eval()\n",
    "        \n",
    "        main_models.append(main_model)\n",
    "        print(f\"âœ… ë©”ì¸ ëª¨ë¸ {fold+1} ë¡œë“œ ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(f\"âŒ ë©”ì¸ ëª¨ë¸ {fold+1} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
    "\n",
    "print(f\"ì´ {len(main_models)}ê°œì˜ ë©”ì¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "save_dir = 'subset_models'\n",
    "\n",
    "# ì„œë¸Œì…‹ ëª¨ë¸ë“¤ ë¡œë“œ (4ê°œ í´ë˜ìŠ¤)\n",
    "print(\"\\nì„œë¸Œì…‹ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...\")\n",
    "subset_models = []\n",
    "for fold in range(5):\n",
    "    model_path = f\"{save_dir}/subset_fold_{fold}_model.pth\"\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        # ì„œë¸Œì…‹ ëª¨ë¸ ìƒì„± (4ê°œ í´ë˜ìŠ¤)\n",
    "        subset_model = timm.create_model(model_name, pretrained=True, num_classes=4).to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        subset_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        subset_model.eval()\n",
    "        \n",
    "        subset_models.append(subset_model)\n",
    "        print(f\"âœ… ì„œë¸Œì…‹ ëª¨ë¸ {fold} ë¡œë“œ ì™„ë£Œ (F1: {checkpoint['best_f1']:.4f})\")\n",
    "    else:\n",
    "        print(f\"âŒ ì„œë¸Œì…‹ ëª¨ë¸ {fold} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
    "\n",
    "print(f\"ì´ {len(subset_models)}ê°œì˜ ì„œë¸Œì…‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ea6b713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA ìºìŠ¤ì¼€ì´ë“œ ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ\n",
      "- ì·¨ì•½ í´ë˜ìŠ¤: [3, 4, 7, 14]\n",
      "- ì‹ ë¢°ë„ ì„ê³„ê°’: 0.4\n",
      "- ë©”ì¸ ëª¨ë¸ ìˆ˜: 5\n",
      "- ì„œë¸Œì…‹ ëª¨ë¸ ìˆ˜: 5\n",
      "- TTA ë³€í™˜ ìˆ˜: 5\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: 3140\n",
      "ìºìŠ¤ì¼€ì´ë“œ ì‹œìŠ¤í…œìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:   0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.373) -> 7(0.329)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:   2%|â–         | 2/99 [00:26<20:53, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 4(0.231) -> 14(0.521)\n",
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 3(0.280) -> 3(0.398)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  10%|â–ˆ         | 10/99 [02:06<18:35, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 3(0.289) -> 7(0.403)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  14%|â–ˆâ–        | 14/99 [02:57<17:54, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 4(0.281) -> 4(0.492)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  16%|â–ˆâ–Œ        | 16/99 [03:23<17:40, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.356) -> 4(0.397)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  18%|â–ˆâ–Š        | 18/99 [03:49<17:26, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 4(0.260) -> 3(0.308)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  19%|â–ˆâ–‰        | 19/99 [04:02<17:23, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.356) -> 7(0.404)\n",
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 4(0.370) -> 3(0.347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  30%|â–ˆâ–ˆâ–ˆ       | 30/99 [06:25<14:54, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 4(0.347) -> 4(0.464)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  33%|â–ˆâ–ˆâ–ˆâ–      | 33/99 [07:05<14:19, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.360) -> 3(0.334)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/99 [07:57<13:28, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.393) -> 3(0.506)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/99 [08:23<13:04, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.237) -> 14(0.360)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/99 [09:16<12:10, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.320) -> 7(0.372)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/99 [10:34<10:48, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 14(0.342) -> 14(0.372)\n",
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 4(0.371) -> 3(0.320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/99 [12:31<08:52, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 3(0.298) -> 7(0.580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71/99 [15:20<06:02, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.364) -> 7(0.473)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/99 [16:12<05:11, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.235) -> 7(0.541)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 77/99 [16:38<04:46, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.374) -> 7(0.416)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 78/99 [16:52<04:35, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 4(0.384) -> 3(0.478)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 81/99 [17:31<03:54, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 4(0.347) -> 7(0.390)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 88/99 [19:02<02:22, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.380) -> 7(0.488)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 91/99 [19:41<01:44, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 7(0.235) -> 3(0.373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 96/99 [20:46<00:39, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 3(0.344) -> 7(0.456)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Cascade Prediction: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [21:14<00:00, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš© í†µê³„:\n",
      "- ë©”ì¸ ë¶„ë¥˜ê¸°ë§Œ ì‚¬ìš©: 3115ê°œ (99.2%)\n",
      "- ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: 25ê°œ (0.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 6. ìºìŠ¤ì¼€ì´ë“œ ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "# ========================================\n",
    "\n",
    "# ìºìŠ¤ì¼€ì´ë“œ ë¶„ë¥˜ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "cascade_classifier = CascadeClassifier(\n",
    "    main_models=main_models,      # ë¶„ë¥˜ê¸° A (17ê°œ í´ë˜ìŠ¤)\n",
    "    subset_models=subset_models,  # ë¶„ë¥˜ê¸° B (4ê°œ í´ë˜ìŠ¤)\n",
    "    vulnerable_classes=vulnerable_classes, # [3, 4, 7, 14]\n",
    "    confidence_threshold=0.4      # ì‹ ë¢°ë„ ì„ê³„ê°’\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "test_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(test_df)}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "test_dataset = ImageDataset(\n",
    "    test_df,\n",
    "    \"../data/test/\",\n",
    "    epoch=0,\n",
    "    total_epochs=EPOCHS,\n",
    "    is_train=False  # í…ŒìŠ¤íŠ¸ì´ë¯€ë¡œ ì¦ê°• ì—†ìŒ\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,  # ë°°ì¹˜ í¬ê¸° ì¤„ì„\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"ìºìŠ¤ì¼€ì´ë“œ ì‹œìŠ¤í…œìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘...\")\n",
    "\n",
    "# ì˜¬ë°”ë¥¸ ì½”ë“œ - ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ë©”ì„œë“œ í˜¸ì¶œ\n",
    "test_predictions, test_confidences, cascade_usage = cascade_classifier.predict_batch(\n",
    "    test_loader, device\n",
    ")\n",
    "\n",
    "print(f\"\\nìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš© í†µê³„:\")\n",
    "print(f\"- ë©”ì¸ ë¶„ë¥˜ê¸°ë§Œ ì‚¬ìš©: {cascade_usage['main']}ê°œ ({cascade_usage['main']/len(test_predictions)*100:.1f}%)\")\n",
    "print(f\"- ìºìŠ¤ì¼€ì´ë“œ ì‚¬ìš©: {cascade_usage['cascade']}ê°œ ({cascade_usage['cascade']/len(test_predictions)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a602d517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ê²°ê³¼ ì €ì¥: ../data/output/cascade_submission_CSW_5.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "result_df = test_df.copy()\n",
    "result_df['target'] = test_predictions\n",
    "result_df['confidence'] = test_confidences\n",
    "\n",
    "# submission íŒŒì¼ ì €ì¥\n",
    "output_path = \"../data/output/cascade_submission_CSW_5.csv\"\n",
    "print(f\"ğŸ“ ê²°ê³¼ ì €ì¥: {output_path}\")\n",
    "result_df[['ID', 'target']].to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f6c53c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 11 05:23:44 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:21:00.0 Off |                  N/A |\n",
      "| 30%   40C    P8              19W / 350W |   4288MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342ead5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
