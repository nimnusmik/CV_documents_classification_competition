{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bbe02e",
   "metadata": {},
   "source": [
    "# üìÑ Document type classification baseline code with WandB Integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92dc69ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations==1.3.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: ipykernel==6.27.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (6.27.1)\n",
      "Requirement already satisfied: ipython==8.15.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (8.15.0)\n",
      "Requirement already satisfied: ipywidgets==8.1.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (8.1.1)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (0.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (1.26.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 8)) (2.1.4)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 9)) (9.4.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: timm==0.9.12 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 11)) (0.9.12)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 12)) (6.3.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 13)) (3.10.6)\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 14)) (4.5.0)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 15)) (0.21.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.11.4)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.22.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (4.8.1.78)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.5.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (1.5.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.7.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1->-r ../requirements.txt (line 4)) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1->-r ../requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.0.6)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.12.0)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.16.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.34.4)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.10/site-packages (from plotly->-r ../requirements.txt (line 12)) (2.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (3.2.3)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (1.16.5)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (2.0.43)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (4.67.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 15)) (8.0.4)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 15)) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 15)) (4.1.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 15)) (6.32.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 15)) (2.11.7)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 15)) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 15)) (2.37.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /opt/conda/lib/python3.10/site-packages (from wandb->-r ../requirements.txt (line 15)) (4.15.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (1.3.10)\n",
      "Requirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (2.0.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r ../requirements.txt (line 15)) (4.0.12)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 15)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 15)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 15)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 15)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 15)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 15)) (2023.7.22)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (2023.12.9)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.3)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna->-r ../requirements.txt (line 14)) (3.2.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (3.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (2023.9.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (1.1.9)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.1.1)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (5.9.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.12.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.25.2)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.0.9)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.2.3)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r ../requirements.txt (line 15)) (5.0.2)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.1.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.5.0)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.13.1)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.20.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.19.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (21.2.0)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.1)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.8.19.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 0. Prepare Environments & Install Libraries\n",
    "# =============================================================================\n",
    "\n",
    "# ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÑ§ÏπòÌï©ÎãàÎã§.\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a366485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "# ÌòÑÏû¨ ÎÖ∏Ìä∏Î∂ÅÏóêÏÑú Î∞îÎ°ú Ïã§ÌñâÌïòÏÑ∏Ïöî\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def quick_cleanup():\n",
    "    \"\"\"Ï¶âÏãú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Îπ†Î•∏ Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ ÏôÑÎ£å\")\n",
    "\n",
    "# Î∞îÎ°ú Ïã§Ìñâ\n",
    "quick_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773408ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Import Libraries & Define Functions\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import optuna, math\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed PrecisionÏö©\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# WandB Í¥ÄÎ†® import Ï∂îÍ∞Ä\n",
    "import wandb\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e142354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB Î°úÍ∑∏Ïù∏ ÏÉÅÌÉú: kimsunmin0227\n",
      "ÌîÑÎ°úÏ†ùÌä∏: document-classification-team-CV\n",
      "Ïã§ÌóòÎ™Ö: convnext_base_384_in22ft1k\n",
      "ÌåÄÏõêÎì§ÏùÄ EXPERIMENT_NAMEÏùÑ Í∞ÅÏûê Îã§Î•¥Í≤å Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1-1. WandB Login and Configuration\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "üöÄ ÌåÄÏõê ÏÇ¨Ïö© Í∞ÄÏù¥Îìú:\n",
    "\n",
    "1. WandB Í≥ÑÏ†ï ÏÉùÏÑ±: https://wandb.ai/signup\n",
    "2. Ïù¥ ÏÖÄ Ïã§Ìñâ Ïãú Î°úÍ∑∏Ïù∏ ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä ÎÇòÌÉÄÎÇòÎ©¥ Í∞úÏù∏ API ÌÇ§ ÏûÖÎ†•\n",
    "3. EXPERIMENT_NAMEÏùÑ Îã§ÏùåÍ≥º Í∞ôÏù¥ Î≥ÄÍ≤Ω:\n",
    "   - \"member1-baseline\"\n",
    "   - \"member2-augmentation-test\"  \n",
    "   - \"member3-hyperparameter-tuning\"\n",
    "   Îì±Îì± Í∞ÅÏûê Îã§Î•∏ Ïù¥Î¶Ñ ÏÇ¨Ïö©\n",
    "\n",
    "4. ÌåÄ ÎåÄÏãúÎ≥¥Îìú URL: [Ïó¨Í∏∞Ïóê ÎãπÏã†Ïùò ÌîÑÎ°úÏ†ùÌä∏ URL Ï∂îÍ∞Ä]\n",
    "\n",
    "‚ö†Ô∏è Ï£ºÏùòÏÇ¨Ìï≠:\n",
    "- Ï†àÎåÄ API ÌÇ§Î•º ÏΩîÎìúÏóê ÌïòÎìúÏΩîÎî©ÌïòÏßÄ ÎßàÏÑ∏Ïöî\n",
    "- EXPERIMENT_NAMEÎßå Î≥ÄÍ≤ΩÌïòÍ≥† PROJECT_NAMEÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÏÑ∏Ïöî\n",
    "- Í∞ÅÏûê Í∞úÏù∏ Í≥ÑÏ†ïÏúºÎ°ú Î°úÍ∑∏Ïù∏Ìï¥ÏÑú Ïã§ÌóòÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî\n",
    "\"\"\"\n",
    "\n",
    "# WandB Î°úÍ∑∏Ïù∏ (Í∞ÅÏûê Ïã§Ìñâ)\n",
    "try:\n",
    "    if wandb.api.api_key is None:\n",
    "        print(\"WandBÏóê Î°úÍ∑∏Ïù∏Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.\")\n",
    "        wandb.login()\n",
    "    else:\n",
    "        print(f\"WandB Î°úÍ∑∏Ïù∏ ÏÉÅÌÉú: {wandb.api.viewer()['username']}\")\n",
    "except:\n",
    "    print(\"WandB Î°úÍ∑∏Ïù∏ÏùÑ ÏßÑÌñâÌï©ÎãàÎã§...\")\n",
    "    wandb.login()\n",
    "\n",
    "# ÌîÑÎ°úÏ†ùÌä∏ ÏÑ§Ï†ï (Í∞ÅÏûê ÏàòÏ†ïÌï† Î∂ÄÎ∂Ñ)\n",
    "PROJECT_NAME = \"document-classification-team-CV\"  # Î™®Îì† ÌåÄÏõê ÎèôÏùº\n",
    "ENTITY = None  # Í∞ÅÏûê Í∞úÏù∏ Í≥ÑÏ†ï ÏÇ¨Ïö©\n",
    "EXPERIMENT_NAME = \"convnext_base_384_in22ft1k\"  # ÌåÄÏõêÎ≥ÑÎ°ú Î≥ÄÍ≤Ω (Ïòà: \"member1-hyperopt\", \"member2-augmentation\")\n",
    "\n",
    "print(f\"ÌîÑÎ°úÏ†ùÌä∏: {PROJECT_NAME}\")\n",
    "print(f\"Ïã§ÌóòÎ™Ö: {EXPERIMENT_NAME}\")\n",
    "print(\"ÌåÄÏõêÎì§ÏùÄ EXPERIMENT_NAMEÏùÑ Í∞ÅÏûê Îã§Î•¥Í≤å Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "448a2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. Seed & basic augmentations (Mixup)\n",
    "# =============================================================================\n",
    "\n",
    "# ÏãúÎìúÎ•º Í≥†Ï†ïÌï©ÎãàÎã§.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9d1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. Dataset Class\n",
    "# =============================================================================\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transform=None, problem_class_transform=None):\n",
    "        # CSV ÌååÏùºÏù¥Î©¥ ÏùΩÍ≥†, DataFrameÏù¥Î©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values  # DataFrameÏùÑ numpy arrayÎ°ú Î≥ÄÌôò\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.problem_class_transform = problem_class_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        \n",
    "        # Í∏∞Î≥∏ transform Ï†ÅÏö©\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        \n",
    "        # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌï¥ÏÑúÎßå Ï∂îÍ∞Ä augmentation Ï†ÅÏö©\n",
    "        if self.problem_class_transform:\n",
    "            \n",
    "            # tensorÎ•º Îã§Ïãú numpyÎ°ú Î≥ÄÌôò (Ï∂îÍ∞Ä augmentationÏùÑ ÏúÑÌï¥)\n",
    "            img_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "            # Ï†ïÍ∑úÌôî Ìï¥Ï†ú\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img_np = (img_np * std + mean) * 255\n",
    "            img_np = np.clip(img_np, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Ï∂îÍ∞Ä augmentation Ï†ÅÏö© (Í≤∞Í≥ºÎäî numpy array, H,W,C ÌòïÌÉú)\n",
    "            extra_aug = self.problem_class_transform(image=img_np)['image']\n",
    "            \n",
    "            # numpy arrayÎ•º torch tensorÎ°ú Î≥ÄÌôòÌïòÍ≥† Ï∞®Ïõê ÏàúÏÑú Î≥ÄÍ≤Ω (H,W,C ‚Üí C,H,W)\n",
    "            extra_aug = torch.from_numpy(extra_aug).float()\n",
    "            extra_aug = extra_aug.permute(2, 0, 1)  # (H,W,C) ‚Üí (C,H,W)Î°ú Î≥ÄÍ≤Ω\n",
    "            \n",
    "            # Ï†ïÍ∑úÌôî Ï†ÅÏö© (Ïù¥Ï†ú Ï∞®ÏõêÏù¥ ÎßûÏùå)\n",
    "            extra_aug = extra_aug / torch.tensor(255.0, dtype=torch.float32)  # float32 Ïä§ÏπºÎùº ÏÇ¨Ïö©\n",
    "            mean_tensor = torch.tensor(mean, dtype=torch.float32).view(3, 1, 1)\n",
    "            std_tensor = torch.tensor(std, dtype=torch.float32).view(3, 1, 1)\n",
    "            extra_aug = (extra_aug - mean_tensor) / std_tensor\n",
    "            img = extra_aug.float()\n",
    "            \n",
    "        return img, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "403c7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutout (Random Erasing) Ìï®Ïàò Ï†ïÏùò\n",
    "def random_erasing(image, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)):\n",
    "    if random.random() > p:\n",
    "        return image\n",
    "    img_c, img_h, img_w = image.shape[1], image.shape[2], image.shape[3]\n",
    "    area = img_h * img_w\n",
    "    \n",
    "    target_area = torch.tensor(random.uniform(scale[0], scale[1]), dtype=torch.float32) * area\n",
    "    aspect_ratio = torch.tensor(random.uniform(ratio[0], ratio[1]), dtype=torch.float32)\n",
    "    h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "    w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "    \n",
    "    # h, wÍ∞Ä Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ ÎÇ¥Ïóê ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "    if w < img_w and h < img_h:\n",
    "        x = random.randint(0, img_w - w)\n",
    "        y = random.randint(0, img_h - h)\n",
    "        \n",
    "        # float32 ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±\n",
    "        mask = torch.ones_like(image, dtype=torch.float32)\n",
    "        mask[:, y:y+h, x:x+w] = 0.0  # ÎòêÎäî ÎûúÎç§ Í∞í: torch.rand(3, h, w, dtype=torch.float32)\n",
    "        \n",
    "        # erasing Ï†ÅÏö©\n",
    "        erased = image * mask\n",
    "        return erased.float()  # float32 Ï∂úÎ†• Î≥¥Ïû•\n",
    "    return image.float()\n",
    "\n",
    "# RandomCrop Ìï®Ïàò Ï†ïÏùò\n",
    "def random_crop(image, crop_size=0.7):\n",
    "    img_c, img_h, img_w = image.shape[1], image.shape[2], image.shape[3]\n",
    "    crop_h = int(img_h * crop_size)\n",
    "    crop_w = int(img_w * crop_size)\n",
    "    \n",
    "    if crop_h >= img_h or crop_w >= img_w:\n",
    "        return image\n",
    "    \n",
    "    x = random.randint(0, img_w - crop_w)\n",
    "    y = random.randint(0, img_h - crop_h)\n",
    "    cropped_image = image[:, :, y:y+crop_h, x:x+crop_w]\n",
    "    \n",
    "    # Ìå®Îî©ÏúºÎ°ú ÏõêÎûò ÌÅ¨Í∏∞ Î≥µÏõê\n",
    "    padded_image = torch.zeros_like(image)\n",
    "    padded_image[:, :, y:y+crop_h, x:x+crop_w] = cropped_image\n",
    "    return padded_image\n",
    "\n",
    "# Mixup Ìï®Ïàò Ï†ïÏùò\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cacecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 6. training and validation\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, epoch=None, fold=None):\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Training Epoch {epoch+1 if epoch else '?'}\")\n",
    "    batch_count = 0\n",
    "    \n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device).float()\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "         \n",
    "        aug_type = random.choices(['mixup', 'cutout', 'random_crop'], weights=[0.4, 0.3, 0.3])[0]\n",
    "        mixup_applied = False\n",
    "        cutout_applied = False\n",
    "        random_crop_applied = False\n",
    "        \n",
    "        if aug_type == 'mixup':\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(image, targets, alpha=1.0)\n",
    "            with autocast(): \n",
    "                preds = model(mixed_x)\n",
    "            loss = lam * loss_fn(preds.float(), y_a) + (1 - lam) * loss_fn(preds.float(), y_b)\n",
    "            mixup_applied = True\n",
    "       \n",
    "        elif aug_type == 'cutout':\n",
    "            image = random_erasing(image, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds.float(), targets)  \n",
    "            cutout_applied = True\n",
    "       \n",
    "        elif aug_type == 'random_crop':\n",
    "            image = random_crop(image, crop_size=0.8)\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds.float(), targets)   \n",
    "            random_crop_applied = True\n",
    "       \n",
    "        else:\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds.float(), targets)  \n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        \n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        # Î∞∞ÏπòÎ≥Ñ ÏÉÅÏÑ∏ Î°úÍπÖ (100 Î∞∞ÏπòÎßàÎã§)\n",
    "        #if batch_count % 100 == 0 and wandb.run is not None:\n",
    "        #    step = epoch * len(loader) + batch_count if epoch is not None else batch_count\n",
    "        #    wandb.log({\n",
    "        #        f\"fold_{fold}/train_batch_loss\": loss.item(),\n",
    "        #        f\"fold_{fold}/mixup_applied\": int(mixup_applied),\n",
    "        #        f\"fold_{fold}/cutout_applied\": int(cutout_applied),\n",
    "        #        f\"fold_{fold}/random_crop_applied\": int(random_crop_applied),\n",
    "        #        f\"fold_{fold}/batch_step\": step\n",
    "        #    })\n",
    "        \n",
    "        batch_count += 1\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}, Mixup: {mixup_applied}, Cutout: {cutout_applied}, RandomCrop: {random_crop_applied}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def validate_one_epoch(loader, model, loss_fn, device, epoch=None, fold=None, log_confusion=False):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=f\"Validating Epoch {epoch+1 if epoch else '?'}\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "            \n",
    "            pbar.set_description(f\"Val Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "    \n",
    "    # üéØ Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Îì§Ïùò ÏÑ±Îä• Î≥ÑÎèÑ Í≥ÑÏÇ∞ Î∞è Ï∂îÏ†Å\n",
    "    target_classes = [3, 7, 14]\n",
    "    \n",
    "    # Ï†ÑÏ≤¥ ÌÅ¥ÎûòÏä§Î≥Ñ F1 Ïä§ÏΩîÏñ¥ Í≥ÑÏÇ∞\n",
    "    class_f1_scores = f1_score(targets_list, preds_list, average=None, labels=list(range(17)), zero_division=0)\n",
    "    \n",
    "    # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Îì§Ïùò F1 Ïä§ÏΩîÏñ¥ Ï∂îÏ∂ú\n",
    "    problem_class_f1 = {}\n",
    "    problem_class_performance = []\n",
    "    \n",
    "    for cls in target_classes:\n",
    "        if cls < len(class_f1_scores):\n",
    "            cls_f1 = class_f1_scores[cls]\n",
    "            problem_class_f1[f\"class_{cls}_f1\"] = cls_f1\n",
    "            problem_class_performance.append(cls_f1)\n",
    "            \n",
    "            # ÏΩòÏÜîÏóê Ï∂úÎ†•\n",
    "            print(f\"  Class {cls} F1: {cls_f1:.4f}\")\n",
    "    \n",
    "    # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Îì§Ïùò ÌèâÍ∑† F1 Í≥ÑÏÇ∞\n",
    "    avg_problem_f1 = np.mean(problem_class_performance) if problem_class_performance else 0.0\n",
    "    print(f\"  Problem Classes Avg F1: {avg_problem_f1:.4f}\")\n",
    "    \n",
    "    # WandB Î°úÍπÖ (Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä•)\n",
    "    if wandb.run is not None:\n",
    "        log_dict = {\n",
    "            f\"fold_{fold}/val_loss\": val_loss,\n",
    "            f\"fold_{fold}/val_acc\": val_acc,\n",
    "            f\"fold_{fold}/val_f1\": val_f1,\n",
    "            f\"fold_{fold}/problem_classes_avg_f1\": avg_problem_f1,\n",
    "        }\n",
    "        \n",
    "        # Í∞Å Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Î≥Ñ F1 Ïä§ÏΩîÏñ¥ Î°úÍπÖ\n",
    "        for cls in target_classes:\n",
    "            if cls < len(class_f1_scores):\n",
    "                log_dict[f\"fold_{fold}/class_{cls}_f1\"] = class_f1_scores[cls]\n",
    "        \n",
    "        wandb.log(log_dict)\n",
    "    \n",
    "    # Confusion Matrix Î°úÍπÖ (ÎßàÏßÄÎßâ epochÏóêÎßå)\n",
    "    if log_confusion and wandb.run is not None:\n",
    "        try:\n",
    "            wandb.log({\n",
    "                f\"fold_{fold}/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=targets_list,\n",
    "                    preds=preds_list,\n",
    "                    class_names=[f\"Class_{i}\" for i in range(17)]\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            # Ï†ÑÏ≤¥ ÌÅ¥ÎûòÏä§Î≥Ñ F1 Ïä§ÏΩîÏñ¥ Î°úÍπÖ\n",
    "            for i, class_f1 in enumerate(class_f1_scores):\n",
    "                wandb.log({f\"fold_{fold}/all_class_{i}_f1\": class_f1})\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Confusion matrix Î°úÍπÖ Ïã§Ìå®: {e}\")\n",
    "    \n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,  \n",
    "        \"val_f1\": val_f1,\n",
    "        \"problem_class_f1\": problem_class_f1,  # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ F1 Ïä§ÏΩîÏñ¥ Ï∂îÍ∞Ä\n",
    "        \"avg_problem_f1\": avg_problem_f1,      # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÌèâÍ∑† F1 Ï∂îÍ∞Ä\n",
    "    }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "193dae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using device: cuda\n",
      " ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï ÏôÑÎ£å!\n",
      " Î™®Îç∏: convnext_base_384_in22ft1k\n",
      " Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞: 388x388\n",
      " Î∞∞Ïπò ÌÅ¨Í∏∞: 4\n",
      " ÌïôÏäµÎ•†: 0.0002\n",
      " ÏóêÌè≠: 100\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. Hyper-parameters with WandB Config\n",
    "# =============================================================================\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "\n",
    "# model config\n",
    "model_name = 'convnext_base_384_in22ft1k' # 'resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "# training config\n",
    "img_size = 388\n",
    "LR = 2e-4\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 4\n",
    "num_workers = 30\n",
    "\n",
    "# K-Fold config\n",
    "N_FOLDS = 5  # 5-foldÎ°ú ÏÑ§Ï†ï\n",
    "\n",
    "# WandB Config ÏÑ§Ï†ï\n",
    "config = {\n",
    "    # Model config\n",
    "    \"model_name\": model_name,\n",
    "    \"img_size\": img_size,\n",
    "    \"num_classes\": 17,\n",
    "    \"architecture\": \"convnext_base_384_in22ft1k\",\n",
    "    \n",
    "    # Training config  \n",
    "    \"lr\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"device\": str(device),\n",
    "    \n",
    "    # K-Fold config\n",
    "    \"n_folds\": N_FOLDS,\n",
    "    \"seed\": SEED,\n",
    "    \"cv_strategy\": \"StratifiedKFold\",\n",
    "    \n",
    "    # Augmentation & Training techniques\n",
    "    \"mixup_alpha\": 1.0,\n",
    "    \"mixup_prob\": 0.3,\n",
    "    \"label_smoothing\": 0.2,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"mixed_precision\": True,\n",
    "    \n",
    "    # Optimizer & Scheduler\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \n",
    "    # Data\n",
    "    \"data_path\": data_path,\n",
    "    \"train_transforms\": \"Advanced\",\n",
    "    \"test_transforms\": \"Basic\",\n",
    "}\n",
    "\n",
    "print(\" ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï ÏôÑÎ£å!\")\n",
    "print(f\" Î™®Îç∏: {model_name}\")\n",
    "print(f\" Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞: {img_size}x{img_size}\")\n",
    "print(f\" Î∞∞Ïπò ÌÅ¨Í∏∞: {BATCH_SIZE}\")\n",
    "print(f\" ÌïôÏäµÎ•†: {LR}\")\n",
    "print(f\" ÏóêÌè≠: {EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e4bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ÏÑ§Ï†ï ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. Data Transforms\n",
    "# =============================================================================\n",
    "\n",
    "# augmentationÏùÑ ÏúÑÌïú transform ÏΩîÎìú\n",
    "trn_transform = A.Compose([\n",
    "    # ÎπÑÏú® Î≥¥Ï°¥ Î¶¨ÏÇ¨Ïù¥Ïßï (ÌïµÏã¨ Í∞úÏÑ†)\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "    \n",
    "    # Î¨∏ÏÑú ÌäπÌôî ÌöåÏ†Ñ + ÎØ∏ÏÑ∏ ÌöåÏ†Ñ Ï∂îÍ∞Ä\n",
    "    A.OneOf([\n",
    "        A.Rotate(limit=[90,90], p=1.0),\n",
    "        A.Rotate(limit=[180,180], p=1.0),\n",
    "        A.Rotate(limit=[270,270], p=1.0),\n",
    "        A.Rotate(limit=(-15, 15), p=1.0),  # ÎØ∏ÏÑ∏ ÌöåÏ†Ñ Ï∂îÍ∞Ä\n",
    "        A.Rotate(limit=(-30, 30), p=1.0), \n",
    "    ], p=0.7),\n",
    "    \n",
    "    # Í∏∞ÌïòÌïôÏ†Å Î≥ÄÌôò Í∞ïÌôî\n",
    "    A.OneOf([\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=5, p=1.0),\n",
    "        A.ElasticTransform(alpha=50, sigma=5, p=1.0),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.2, p=1.0),\n",
    "        A.OpticalDistortion(distort_limit=0.2, shift_limit=0.1, p=1.0),\n",
    "    ], p=0.6),\n",
    "    \n",
    "    # ÏÉâÏÉÅ Î∞è Ï°∞Î™Ö Î≥ÄÌôò Í∞ïÌôî\n",
    "    A.OneOf([\n",
    "        A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1, p=1.0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=1.0),\n",
    "        A.RandomGamma(gamma_limit=(70, 130), p=1.0),\n",
    "    ], p=0.9),\n",
    "    \n",
    "    # Î∏îÎü¨ Î∞è ÎÖ∏Ïù¥Ï¶à Í∞ïÌôî\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=(5, 15), p=1.0),\n",
    "        A.GaussianBlur(blur_limit=(3, 15), p=1.0),\n",
    "        A.MedianBlur(blur_limit=7, p=1.0),\n",
    "        A.Blur(blur_limit=7, p=1.0),\n",
    "    ], p=0.8),\n",
    "    \n",
    "    # Îã§ÏñëÌïú ÎÖ∏Ïù¥Ï¶à Ï∂îÍ∞Ä\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 150.0), p=1.0),\n",
    "        A.ISONoise(color_shift=(0.01, 0.08), intensity=(0.1, 0.8), p=1.0),\n",
    "        A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n",
    "    ], p=0.8),\n",
    "    \n",
    "    # Î¨∏ÏÑú ÌíàÏßà ÏãúÎÆ¨Î†àÏù¥ÏÖò (Ïä§Ï∫î/Î≥µÏÇ¨ Ìö®Í≥º)\n",
    "    A.OneOf([\n",
    "        A.Downscale(scale_min=0.7, scale_max=0.9, p=1.0),\n",
    "        A.ImageCompression(quality_lower=60, quality_upper=95, p=1.0),\n",
    "        A.Posterize(num_bits=6, p=1.0),\n",
    "    ], p=0.5),\n",
    "    \n",
    "    # ÌîΩÏÖÄ Î†àÎ≤® Î≥ÄÌôò\n",
    "    A.OneOf([\n",
    "        A.ChannelShuffle(p=1.0),\n",
    "        A.InvertImg(p=1.0),\n",
    "        A.Solarize(threshold=128, p=1.0),\n",
    "        A.Equalize(p=1.0),\n",
    "    ], p=0.3),\n",
    "    \n",
    "    # Í≥µÍ∞Ñ Î≥ÄÌôò\n",
    "    A.OneOf([\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),  # Î¨∏ÏÑúÏóêÏÑúÎèÑ Ïú†Ïö©Ìï† Ïàò ÏûàÏùå\n",
    "        A.Transpose(p=1.0),\n",
    "    ], p=0.6),\n",
    "    \n",
    "    # Ï°∞Í∞Å Ï†úÍ±∞ (Cutout Í≥ÑÏó¥)\n",
    "    A.OneOf([\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, \n",
    "                       min_holes=1, min_height=8, min_width=8, \n",
    "                       fill_value=0, p=1.0),\n",
    "        A.GridDropout(ratio=0.3, unit_size_min=8, unit_size_max=32, \n",
    "                     holes_number_x=5, holes_number_y=5, p=1.0),\n",
    "    ], p=0.4),\n",
    "    \n",
    "    # ÏµúÏ¢Ö Ï†ïÍ∑úÌôî\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Î¨∏Ï†ú ÌÅ¥ÎûòÏä§(3, 7, 14) Ï†ÑÏö© Ï∂îÍ∞Ä augmentation\n",
    "problem_class_extra_transform = A.Compose([\n",
    "    # Ï∂îÍ∞Ä ÌöåÏ†Ñ Î≥ÄÌòï (Îçî ÏûêÏ£º Ï†ÅÏö©)\n",
    "    A.OneOf([\n",
    "        A.Rotate(limit=(-20, 20), p=1.0),  # Îçî ÎÑìÏùÄ Î≤îÏúÑ\n",
    "        A.Rotate(limit=(-10, 10), p=1.0),\n",
    "    ], p=0.5),  # 50% ÌôïÎ•†Î°ú Ï∂îÍ∞Ä ÌöåÏ†Ñ\n",
    "    \n",
    "    # Ï∂îÍ∞Ä ÏÉâÏÉÅ Î≥ÄÌòï\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=1.0),\n",
    "    ], p=0.4),\n",
    "    \n",
    "    # Ï∂îÍ∞Ä Ïä§ÏºÄÏùº Î≥ÄÌòï\n",
    "    A.OneOf([\n",
    "        A.RandomScale(scale_limit=0.1, p=1.0),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=3, p=1.0),\n",
    "    ], p=0.3),\n",
    "\n",
    "    # ÌÅ¨Í∏∞Î•º Îã§Ïãú ÎßûÏ∂∞Ï£ºÍ∏∞ (Ï§ëÏöî!)\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "])\n",
    "\n",
    "# test image Î≥ÄÌôòÏùÑ ÏúÑÌïú transform ÏΩîÎìú\n",
    "tst_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ÏÑ§Ï†ï ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f420558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß WandB Ïó∞Í≤∞ Î¨∏Ï†ú Ìï¥Í≤∞ Ï§ë...\n",
      "WandB Ï¥àÍ∏∞Ìôî ÏãúÎèÑ 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimsunmin0227\u001b[0m (\u001b[33mkimsunmin0227-hufs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/computervisioncompetition-cv-1/mywork/experiments/wandb/run-20250909_154619-rb3me803</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/rb3me803' target=\"_blank\">convnext_base_384_in22ft1k-0909-1546</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/rb3me803' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/rb3me803</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WandB Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ!\n",
      "\n",
      "üöÄ WandB Ïã§Ìóò ÏãúÏûë!\n",
      "üìä ÎåÄÏãúÎ≥¥Îìú: https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/rb3me803\n",
      "üìã Ïã§ÌóòÎ™Ö: convnext_base_384_in22ft1k-0909-1546\n"
     ]
    }
   ],
   "source": [
    "# wandb.finish()\n",
    "# =============================================================================\n",
    "# WandB MailboxClosedError Ìï¥Í≤∞ÏùÑ ÏúÑÌïú Ï†ïÎ¶¨ Î∞è Ïû¨ÏãúÏûë\n",
    "# =============================================================================\n",
    " \n",
    "# 1. Í∏∞Ï°¥ WandB Îü∞ Í∞ïÏ†ú Ï¢ÖÎ£å\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        print(\"Í∏∞Ï°¥ WandB Îü∞ Ï¢ÖÎ£å Ï§ë...\")\n",
    "        wandb.finish()\n",
    "        time.sleep(2)  # Ï¢ÖÎ£å ÎåÄÍ∏∞\n",
    "except Exception as e:\n",
    "    print(f\"Í∏∞Ï°¥ Îü∞ Ï¢ÖÎ£å Ï§ë ÏóêÎü¨ (Î¨¥Ïãú Í∞ÄÎä•): {e}\")\n",
    "\n",
    "# 2. WandB ÌîÑÎ°úÏÑ∏Ïä§ Ï†ïÎ¶¨\n",
    "try:\n",
    "    # WandB ÎÇ¥Î∂Ä ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî\n",
    "    wandb.teardown()\n",
    "    time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(f\"WandB teardown Ï§ë ÏóêÎü¨ (Î¨¥Ïãú Í∞ÄÎä•): {e}\")\n",
    "\n",
    "# 3. ÌôòÍ≤Ω Î≥ÄÏàò Ïû¨ÏÑ§Ï†ï (ÏÑ†ÌÉùÏÇ¨Ìï≠)\n",
    "os.environ['WANDB_START_METHOD'] = 'thread'  # ÌîÑÎ°úÏÑ∏Ïä§ Ï∂©Îèå Î∞©ÏßÄ\n",
    "\n",
    "# 4. ÏïàÏ†ÑÌïú WandB Ï¥àÍ∏∞Ìôî Ìï®Ïàò\n",
    "def safe_wandb_init(project, name, config, **kwargs):\n",
    "    \"\"\"ÏïàÏ†ÑÌïú WandB Ï¥àÍ∏∞Ìôî\"\"\"\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"WandB Ï¥àÍ∏∞Ìôî ÏãúÎèÑ {attempt + 1}/{max_retries}...\")\n",
    "            \n",
    "            # Í∏∞Ï°¥ Îü∞Ïù¥ ÏûàÎã§Î©¥ Ï¢ÖÎ£å\n",
    "            if wandb.run is not None:\n",
    "                wandb.finish()\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # ÏÉàÎ°úÏö¥ Îü∞ ÏãúÏûë\n",
    "            run = wandb.init(\n",
    "                project=project,\n",
    "                name=name,\n",
    "                config=config,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ WandB Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ!\")\n",
    "            return run\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ÏãúÎèÑ {attempt + 1} Ïã§Ìå®: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"‚è≥ {retry_delay}Ï¥à ÌõÑ Ïû¨ÏãúÎèÑ...\")\n",
    "                time.sleep(retry_delay)\n",
    "                \n",
    "                # WandB ÌîÑÎ°úÏÑ∏Ïä§ Í∞ïÏ†ú Ï†ïÎ¶¨\n",
    "                try:\n",
    "                    wandb.teardown()\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                print(\"‚ùå Î™®Îì† Ïû¨ÏãúÎèÑ Ïã§Ìå®. WandB ÏóÜÏù¥ ÏßÑÌñâÌï©ÎãàÎã§.\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 5. ÏàòÏ†ïÎêú Î©îÏù∏ Ï¥àÍ∏∞Ìôî ÏΩîÎìú\n",
    "print(\"üîß WandB Ïó∞Í≤∞ Î¨∏Ï†ú Ìï¥Í≤∞ Ï§ë...\")\n",
    "\n",
    "# Í∏∞Ï°¥ ÏΩîÎìú ÎåÄÏ≤¥\n",
    "main_run = safe_wandb_init(\n",
    "    project=PROJECT_NAME,\n",
    "    name=f\"{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "    config=config,\n",
    "    entity=ENTITY,\n",
    "    tags=[\"k-fold-cv\", \"ensemble\", model_name, \"baseline\", \"main-experiment\"],\n",
    "    group=\"k-fold-experiment\", \n",
    "    job_type=\"cross-validation\",\n",
    "    notes=f\"{N_FOLDS}-Fold Cross Validation with {model_name}\"\n",
    ")\n",
    "\n",
    "if main_run is not None:\n",
    "    print(f\"\\nüöÄ WandB Ïã§Ìóò ÏãúÏûë!\")\n",
    "    print(f\"üìä ÎåÄÏãúÎ≥¥Îìú: {main_run.url}\")\n",
    "    print(f\"üìã Ïã§ÌóòÎ™Ö: {main_run.name}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WandB ÏóÜÏù¥ Ïã§ÌóòÏùÑ ÏßÑÌñâÌï©ÎãàÎã§.\")\n",
    "    print(\"Í≤∞Í≥ºÎäî ÏΩòÏÜîÍ≥º Î°úÏª¨ ÌååÏùºÎ°úÎßå Ï†ÄÏû•Îê©ÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6514acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïôÏäµ Îç∞Ïù¥ÌÑ∞: 1570Í∞ú ÏÉòÌîå\n",
      " ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨: {0: 100, 1: 46, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100, 10: 100, 11: 100, 12: 100, 13: 74, 14: 50, 15: 100, 16: 100}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">convnext_base_384_in22ft1k-0909-1546</strong> at: <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/rb3me803' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/rb3me803</a><br> View project at: <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250909_154619-rb3me803/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/computervisioncompetition-cv-1/mywork/experiments/wandb/run-20250909_154620-pb8rojis</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/pb8rojis' target=\"_blank\">convnext_base_384_in22ft1k-0909-1546</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/pb8rojis' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/pb8rojis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ WandB Ïã§Ìóò ÏãúÏûë!\n",
      "üìä ÎåÄÏãúÎ≥¥Îìú: https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/pb8rojis\n",
      "üìã Ïã§ÌóòÎ™Ö: convnext_base_384_in22ft1k-0909-1546\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. Load Data & Start K-Fold Cross Validation with WandB\n",
    "# =============================================================================\n",
    "\n",
    "# Ï†ÑÏ≤¥ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "print(f\"ÌïôÏäµ Îç∞Ïù¥ÌÑ∞: {len(train_df)}Í∞ú ÏÉòÌîå\")\n",
    "\n",
    "# ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÌôïÏù∏\n",
    "class_counts = train_df['target'].value_counts().sort_index()\n",
    "print(f\" ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨: {dict(class_counts)}\")\n",
    "\n",
    "# K-Fold ÏÑ§Ï†ï\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# K-Fold Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏\n",
    "fold_results = []\n",
    "fold_models = []  # Í∞Å foldÏùò ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ÏùÑ Ï†ÄÏû•\n",
    "\n",
    "#  WandB Î©îÏù∏ Ïã§Ìóò ÏãúÏûë\n",
    "main_run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    entity=ENTITY,\n",
    "    name=f\"{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "    config=config,\n",
    "    tags=[\"k-fold-cv\", \"ensemble\", model_name, \"baseline\", \"main-experiment\"],\n",
    "    group=\"k-fold-experiment\",\n",
    "    job_type=\"cross-validation\",\n",
    "    notes=f\"{N_FOLDS}-Fold Cross Validation with {model_name}\",\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ WandB Ïã§Ìóò ÏãúÏûë!\")\n",
    "print(f\"üìä ÎåÄÏãúÎ≥¥Îìú: {main_run.url}\")\n",
    "print(f\"üìã Ïã§ÌóòÎ™Ö: {main_run.name}\")\n",
    "\n",
    "#  Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÎ≥¥ Î°úÍπÖ\n",
    "# wandb.log({\n",
    "#    \"dataset/total_samples\": len(train_df),\n",
    "#    \"dataset/num_classes\": 17,\n",
    "#   \"dataset/samples_per_fold\": len(train_df) // N_FOLDS,\n",
    "#})\n",
    "\n",
    "# ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî\n",
    "#class_dist_data = [[f\"Class_{i}\", count] for i, count in enumerate(class_counts)]\n",
    "#wandb.log({\n",
    "#    \"dataset/class_distribution\": wandb.plot.bar(\n",
    "#        wandb.Table(data=class_dist_data, columns=[\"Class\", \"Count\"]),\n",
    "#        \"Class\", \"Count\", \n",
    "#        title=\"Training Data Class Distribution\"\n",
    "#    )\n",
    "#})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c320bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " FOLD 1/5\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">convnext_base_384_in22ft1k-0909-1546</strong> at: <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/pb8rojis' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/pb8rojis</a><br> View project at: <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250909_154620-pb8rojis/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/computervisioncompetition-cv-1/mywork/experiments/wandb/run-20250909_154622-sghjdalm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/sghjdalm' target=\"_blank\">fold-1-convnext_base_384_in22ft1k-1546</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/sghjdalm' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/sghjdalm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Fold 1 Dashboard: https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/sghjdalm\n",
      "Train samples: 1256, Validation samples: 314\n",
      " Î™®Îç∏ ÌïôÏäµ ÏãúÏûë - Fold 1\n",
      "\n",
      "üìà Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.1663, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:26<00:00, 11.76it/s]\n",
      "Val Loss: 0.8301: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.4318\n",
      "  Class 7 F1: 0.0000\n",
      "  Class 14 F1: 0.0000\n",
      "  Problem Classes Avg F1: 0.1439\n",
      " Epoch  1 | Train Loss: 2.2833 | Train F1: 0.3424 | Val Loss: 1.5732 | Val F1: 0.6730 | LR: 2.00e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.1439 | ‚úÖ Problem classes performing well\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.6730 (Problem Classes: 0.1439)\n",
      "\n",
      "üìà Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5769, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.83it/s]\n",
      "Val Loss: 1.0684: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5238\n",
      "  Class 7 F1: 0.4912\n",
      "  Class 14 F1: 0.0000\n",
      "  Problem Classes Avg F1: 0.3383\n",
      " Epoch  2 | Train Loss: 1.7342 | Train F1: 0.5473 | Val Loss: 1.3656 | Val F1: 0.7831 | LR: 2.00e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.3383 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.7831 (Problem Classes: 0.3383)\n",
      "\n",
      "üìà Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5778, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.73it/s]\n",
      "Val Loss: 0.9672: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5588\n",
      "  Class 7 F1: 0.3125\n",
      "  Class 14 F1: 0.0000\n",
      "  Problem Classes Avg F1: 0.2904\n",
      " Epoch  3 | Train Loss: 1.6219 | Train F1: 0.6316 | Val Loss: 1.2651 | Val F1: 0.8338 | LR: 2.00e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.2904 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.8338 (Problem Classes: 0.2904)\n",
      "\n",
      "üìà Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.8777, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.82it/s]\n",
      "Val Loss: 1.6699: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.2500\n",
      "  Class 7 F1: 0.5143\n",
      "  Class 14 F1: 0.3333\n",
      "  Problem Classes Avg F1: 0.3659\n",
      " Epoch  4 | Train Loss: 1.5227 | Train F1: 0.6810 | Val Loss: 1.2429 | Val F1: 0.8470 | LR: 2.00e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.3659 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.8470 (Problem Classes: 0.3659)\n",
      "\n",
      "üìà Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5027, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:23<00:00, 13.61it/s]\n",
      "Val Loss: 1.5794: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.3200\n",
      "  Class 7 F1: 0.5588\n",
      "  Class 14 F1: 0.3333\n",
      "  Problem Classes Avg F1: 0.4041\n",
      " Epoch  5 | Train Loss: 1.5074 | Train F1: 0.7010 | Val Loss: 1.2187 | Val F1: 0.8546 | LR: 1.99e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.4041 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.8546 (Problem Classes: 0.4041)\n",
      "\n",
      "üìà Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7827, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.77it/s]\n",
      "Val Loss: 1.1345: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 22.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5490\n",
      "  Class 7 F1: 0.4545\n",
      "  Class 14 F1: 0.1818\n",
      "  Problem Classes Avg F1: 0.3951\n",
      " Epoch  6 | Train Loss: 1.4874 | Train F1: 0.7121 | Val Loss: 1.2709 | Val F1: 0.7961 | LR: 1.99e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.3951 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6221, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.78it/s]\n",
      "Val Loss: 0.8406: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7170\n",
      "  Class 7 F1: 0.5263\n",
      "  Class 14 F1: 0.8235\n",
      "  Problem Classes Avg F1: 0.6889\n",
      " Epoch  7 | Train Loss: 1.4949 | Train F1: 0.7248 | Val Loss: 1.1638 | Val F1: 0.9132 | LR: 1.98e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6889 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.9132 (Problem Classes: 0.6889)\n",
      "\n",
      "üìà Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.2155, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:23<00:00, 13.61it/s]\n",
      "Val Loss: 0.8841: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6182\n",
      "  Class 7 F1: 0.3571\n",
      "  Class 14 F1: 0.3333\n",
      "  Problem Classes Avg F1: 0.4362\n",
      " Epoch  8 | Train Loss: 1.4357 | Train F1: 0.7383 | Val Loss: 1.2070 | Val F1: 0.8720 | LR: 1.98e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.4362 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6132, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.67it/s]\n",
      "Val Loss: 0.9665: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.4848\n",
      "  Class 7 F1: 0.5926\n",
      "  Class 14 F1: 0.5714\n",
      "  Problem Classes Avg F1: 0.5496\n",
      " Epoch  9 | Train Loss: 1.4019 | Train F1: 0.7482 | Val Loss: 1.1052 | Val F1: 0.8997 | LR: 1.97e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5496 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.2554, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:23<00:00, 13.61it/s]\n",
      "Val Loss: 1.2134: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7200\n",
      "  Class 7 F1: 0.0952\n",
      "  Class 14 F1: 0.3333\n",
      "  Problem Classes Avg F1: 0.3829\n",
      " Epoch 10 | Train Loss: 1.4349 | Train F1: 0.7491 | Val Loss: 1.2365 | Val F1: 0.8571 | LR: 1.96e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.3829 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7437, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.85it/s]\n",
      "Val Loss: 1.3856: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5000\n",
      "  Class 7 F1: 0.6129\n",
      "  Class 14 F1: 0.5263\n",
      "  Problem Classes Avg F1: 0.5464\n",
      " Epoch 11 | Train Loss: 1.3847 | Train F1: 0.7544 | Val Loss: 1.1940 | Val F1: 0.8775 | LR: 1.95e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5464 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8575, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.72it/s]\n",
      "Val Loss: 0.8822: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6923\n",
      "  Class 7 F1: 0.6667\n",
      "  Class 14 F1: 0.5714\n",
      "  Problem Classes Avg F1: 0.6435\n",
      " Epoch 12 | Train Loss: 1.3627 | Train F1: 0.7625 | Val Loss: 1.1145 | Val F1: 0.9188 | LR: 1.94e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6435 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.9188 (Problem Classes: 0.6435)\n",
      "\n",
      "üìà Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.8809, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.67it/s]\n",
      "Val Loss: 0.7535: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5352\n",
      "  Class 7 F1: 0.2143\n",
      "  Class 14 F1: 0.1818\n",
      "  Problem Classes Avg F1: 0.3104\n",
      " Epoch 13 | Train Loss: 1.5039 | Train F1: 0.6909 | Val Loss: 1.4149 | Val F1: 0.8353 | LR: 1.93e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.3104 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1451, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:23<00:00, 13.61it/s]\n",
      "Val Loss: 1.2467: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6667\n",
      "  Class 7 F1: 0.6667\n",
      "  Class 14 F1: 0.5714\n",
      "  Problem Classes Avg F1: 0.6349\n",
      " Epoch 14 | Train Loss: 1.4355 | Train F1: 0.7541 | Val Loss: 1.1112 | Val F1: 0.9191 | LR: 1.92e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6349 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.9191 (Problem Classes: 0.6349)\n",
      "\n",
      "üìà Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6683, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.66it/s]\n",
      "Val Loss: 1.8902: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.1818\n",
      "  Class 7 F1: 0.5846\n",
      "  Class 14 F1: 0.7000\n",
      "  Problem Classes Avg F1: 0.4888\n",
      " Epoch 15 | Train Loss: 1.3071 | Train F1: 0.7862 | Val Loss: 1.2114 | Val F1: 0.8755 | LR: 1.90e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.4888 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7151, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.83it/s]\n",
      "Val Loss: 0.4945: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5672\n",
      "  Class 7 F1: 0.3333\n",
      "  Class 14 F1: 0.7059\n",
      "  Problem Classes Avg F1: 0.5355\n",
      " Epoch 16 | Train Loss: 1.3179 | Train F1: 0.7593 | Val Loss: 1.2387 | Val F1: 0.8949 | LR: 1.89e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5355 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.8421, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.72it/s]\n",
      "Val Loss: 1.3025: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6875\n",
      "  Class 7 F1: 0.7170\n",
      "  Class 14 F1: 0.5714\n",
      "  Problem Classes Avg F1: 0.6586\n",
      " Epoch 17 | Train Loss: 1.3118 | Train F1: 0.7759 | Val Loss: 1.1050 | Val F1: 0.9151 | LR: 1.88e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6586 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1452, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.66it/s]\n",
      "Val Loss: 0.9497: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 22.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7826\n",
      "  Class 7 F1: 0.3704\n",
      "  Class 14 F1: 0.7000\n",
      "  Problem Classes Avg F1: 0.6177\n",
      " Epoch 18 | Train Loss: 1.3467 | Train F1: 0.7615 | Val Loss: 1.1085 | Val F1: 0.9079 | LR: 1.86e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6177 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7222, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:23<00:00, 13.60it/s]\n",
      "Val Loss: 1.1657: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7442\n",
      "  Class 7 F1: 0.4706\n",
      "  Class 14 F1: 0.6667\n",
      "  Problem Classes Avg F1: 0.6271\n",
      " Epoch 19 | Train Loss: 1.3439 | Train F1: 0.7697 | Val Loss: 1.1077 | Val F1: 0.9114 | LR: 1.84e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6271 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1654, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.79it/s]\n",
      "Val Loss: 0.8467: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7692\n",
      "  Class 7 F1: 0.6471\n",
      "  Class 14 F1: 0.5833\n",
      "  Problem Classes Avg F1: 0.6665\n",
      " Epoch 20 | Train Loss: 1.3282 | Train F1: 0.7721 | Val Loss: 1.1116 | Val F1: 0.9239 | LR: 1.83e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6665 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.9239 (Problem Classes: 0.6665)\n",
      "\n",
      "üìà Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1446, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.68it/s]\n",
      "Val Loss: 0.8891: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6923\n",
      "  Class 7 F1: 0.5517\n",
      "  Class 14 F1: 0.3333\n",
      "  Problem Classes Avg F1: 0.5258\n",
      " Epoch 21 | Train Loss: 1.3522 | Train F1: 0.7721 | Val Loss: 1.1375 | Val F1: 0.8940 | LR: 1.81e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5258 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5040, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.72it/s]\n",
      "Val Loss: 1.5948: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.1818\n",
      "  Class 7 F1: 0.6250\n",
      "  Class 14 F1: 0.7059\n",
      "  Problem Classes Avg F1: 0.5042\n",
      " Epoch 22 | Train Loss: 1.3050 | Train F1: 0.7778 | Val Loss: 1.1745 | Val F1: 0.8965 | LR: 1.79e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5042 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6127, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.72it/s]\n",
      "Val Loss: 0.6354: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7170\n",
      "  Class 7 F1: 0.5333\n",
      "  Class 14 F1: 0.8421\n",
      "  Problem Classes Avg F1: 0.6975\n",
      " Epoch 23 | Train Loss: 1.2568 | Train F1: 0.7965 | Val Loss: 1.0931 | Val F1: 0.9323 | LR: 1.77e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6975 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.9323 (Problem Classes: 0.6975)\n",
      "\n",
      "üìà Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4811, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.67it/s]\n",
      "Val Loss: 0.5871: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6441\n",
      "  Class 7 F1: 0.3200\n",
      "  Class 14 F1: 0.4615\n",
      "  Problem Classes Avg F1: 0.4752\n",
      " Epoch 24 | Train Loss: 1.2888 | Train F1: 0.7959 | Val Loss: 1.1568 | Val F1: 0.8895 | LR: 1.75e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.4752 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1496, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.73it/s]\n",
      "Val Loss: 0.4996: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5333\n",
      "  Class 7 F1: 0.0000\n",
      "  Class 14 F1: 0.6667\n",
      "  Problem Classes Avg F1: 0.4000\n",
      " Epoch 25 | Train Loss: 1.3303 | Train F1: 0.7821 | Val Loss: 1.3371 | Val F1: 0.8605 | LR: 1.73e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.4000 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.2874, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.71it/s]\n",
      "Val Loss: 1.7277: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7429\n",
      "  Class 7 F1: 0.6522\n",
      "  Class 14 F1: 0.5600\n",
      "  Problem Classes Avg F1: 0.6517\n",
      " Epoch 26 | Train Loss: 1.3059 | Train F1: 0.7814 | Val Loss: 1.0910 | Val F1: 0.9171 | LR: 1.71e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6517 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5162, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.66it/s]\n",
      "Val Loss: 0.4967: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6250\n",
      "  Class 7 F1: 0.4000\n",
      "  Class 14 F1: 0.8235\n",
      "  Problem Classes Avg F1: 0.6162\n",
      " Epoch 27 | Train Loss: 1.2947 | Train F1: 0.7718 | Val Loss: 1.1894 | Val F1: 0.9106 | LR: 1.68e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6162 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1218, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.72it/s]\n",
      "Val Loss: 1.7024: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6111\n",
      "  Class 7 F1: 0.6667\n",
      "  Class 14 F1: 0.6364\n",
      "  Problem Classes Avg F1: 0.6380\n",
      " Epoch 28 | Train Loss: 1.3036 | Train F1: 0.7785 | Val Loss: 1.0922 | Val F1: 0.9177 | LR: 1.66e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6380 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.4312, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.83it/s]\n",
      "Val Loss: 0.4976: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7059\n",
      "  Class 7 F1: 0.5294\n",
      "  Class 14 F1: 0.6667\n",
      "  Problem Classes Avg F1: 0.6340\n",
      " Epoch 29 | Train Loss: 1.2708 | Train F1: 0.7879 | Val Loss: 1.1469 | Val F1: 0.9238 | LR: 1.64e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6340 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1449, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.78it/s]\n",
      "Val Loss: 1.8915: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.2609\n",
      "  Class 7 F1: 0.5556\n",
      "  Class 14 F1: 0.9474\n",
      "  Problem Classes Avg F1: 0.5879\n",
      " Epoch 30 | Train Loss: 1.2928 | Train F1: 0.7741 | Val Loss: 1.2994 | Val F1: 0.9085 | LR: 1.61e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5879 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1149, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.83it/s]\n",
      "Val Loss: 2.1820: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.2500\n",
      "  Class 7 F1: 0.6129\n",
      "  Class 14 F1: 0.7368\n",
      "  Problem Classes Avg F1: 0.5332\n",
      " Epoch 31 | Train Loss: 1.2398 | Train F1: 0.8162 | Val Loss: 1.3340 | Val F1: 0.8990 | LR: 1.59e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5332 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5002, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.84it/s]\n",
      "Val Loss: 1.9139: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6341\n",
      "  Class 7 F1: 0.3200\n",
      "  Class 14 F1: 0.5882\n",
      "  Problem Classes Avg F1: 0.5141\n",
      " Epoch 32 | Train Loss: 1.2983 | Train F1: 0.7742 | Val Loss: 1.2720 | Val F1: 0.8847 | LR: 1.56e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5141 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6631, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.72it/s]\n",
      "Val Loss: 0.5499: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7727\n",
      "  Class 7 F1: 0.7500\n",
      "  Class 14 F1: 0.8235\n",
      "  Problem Classes Avg F1: 0.7821\n",
      " Epoch 33 | Train Loss: 1.3157 | Train F1: 0.7749 | Val Loss: 1.0712 | Val F1: 0.9468 | LR: 1.54e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.7821 | ‚ö†Ô∏è Problem classes need attention\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.9468 (Problem Classes: 0.7821)\n",
      "\n",
      "üìà Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1239, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.66it/s]\n",
      "Val Loss: 0.8157: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7895\n",
      "  Class 7 F1: 0.7805\n",
      "  Class 14 F1: 0.6400\n",
      "  Problem Classes Avg F1: 0.7367\n",
      " Epoch 34 | Train Loss: 1.2406 | Train F1: 0.8131 | Val Loss: 1.0778 | Val F1: 0.9369 | LR: 1.51e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.7367 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4320, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.76it/s]\n",
      "Val Loss: 0.4773: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6667\n",
      "  Class 7 F1: 0.4667\n",
      "  Class 14 F1: 0.7619\n",
      "  Problem Classes Avg F1: 0.6317\n",
      " Epoch 35 | Train Loss: 1.2964 | Train F1: 0.7879 | Val Loss: 1.1801 | Val F1: 0.9176 | LR: 1.48e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6317 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.8036, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:23<00:00, 13.54it/s]\n",
      "Val Loss: 0.4783: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7451\n",
      "  Class 7 F1: 0.5806\n",
      "  Class 14 F1: 0.7778\n",
      "  Problem Classes Avg F1: 0.7012\n",
      " Epoch 36 | Train Loss: 1.2700 | Train F1: 0.7734 | Val Loss: 1.1460 | Val F1: 0.9345 | LR: 1.45e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.7012 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1227, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.71it/s]\n",
      "Val Loss: 3.1009: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6471\n",
      "  Class 7 F1: 0.6667\n",
      "  Class 14 F1: 0.6087\n",
      "  Problem Classes Avg F1: 0.6408\n",
      " Epoch 37 | Train Loss: 1.2566 | Train F1: 0.7745 | Val Loss: 1.1815 | Val F1: 0.9181 | LR: 1.43e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6408 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1726, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.73it/s]\n",
      "Val Loss: 1.8845: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7317\n",
      "  Class 7 F1: 0.6286\n",
      "  Class 14 F1: 0.7619\n",
      "  Problem Classes Avg F1: 0.7074\n",
      " Epoch 38 | Train Loss: 1.2873 | Train F1: 0.7954 | Val Loss: 1.0993 | Val F1: 0.9278 | LR: 1.40e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.7074 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.3313, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.79it/s]\n",
      "Val Loss: 2.9972: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.4828\n",
      "  Class 7 F1: 0.6923\n",
      "  Class 14 F1: 0.7368\n",
      "  Problem Classes Avg F1: 0.6373\n",
      " Epoch 39 | Train Loss: 1.2206 | Train F1: 0.8196 | Val Loss: 1.2013 | Val F1: 0.9195 | LR: 1.37e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6373 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.8210, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:23<00:00, 13.65it/s]\n",
      "Val Loss: 1.3740: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 22.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7391\n",
      "  Class 7 F1: 0.6250\n",
      "  Class 14 F1: 0.6667\n",
      "  Problem Classes Avg F1: 0.6769\n",
      " Epoch 40 | Train Loss: 1.2005 | Train F1: 0.8358 | Val Loss: 1.1731 | Val F1: 0.9198 | LR: 1.34e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6769 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6966, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.72it/s]\n",
      "Val Loss: 0.4896: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7500\n",
      "  Class 7 F1: 0.6250\n",
      "  Class 14 F1: 0.7500\n",
      "  Problem Classes Avg F1: 0.7083\n",
      " Epoch 41 | Train Loss: 1.2031 | Train F1: 0.8255 | Val Loss: 1.1348 | Val F1: 0.9287 | LR: 1.31e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.7083 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.3039, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.66it/s]\n",
      "Val Loss: 0.6463: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6977\n",
      "  Class 7 F1: 0.6842\n",
      "  Class 14 F1: 0.8421\n",
      "  Problem Classes Avg F1: 0.7413\n",
      " Epoch 42 | Train Loss: 1.1542 | Train F1: 0.8474 | Val Loss: 1.1161 | Val F1: 0.9358 | LR: 1.28e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.7413 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5998, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.76it/s]\n",
      "Val Loss: 3.9710: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.3704\n",
      "  Class 7 F1: 0.6038\n",
      "  Class 14 F1: 0.7059\n",
      "  Problem Classes Avg F1: 0.5600\n",
      " Epoch 43 | Train Loss: 1.2447 | Train F1: 0.8200 | Val Loss: 1.3506 | Val F1: 0.9047 | LR: 1.25e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5600 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4268, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.82it/s]\n",
      "Val Loss: 0.8064: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.7556\n",
      "  Class 7 F1: 0.7568\n",
      "  Class 14 F1: 0.8000\n",
      "  Problem Classes Avg F1: 0.7708\n",
      " Epoch 44 | Train Loss: 1.1921 | Train F1: 0.8327 | Val Loss: 1.0898 | Val F1: 0.9453 | LR: 1.22e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.7708 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6543, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.66it/s]\n",
      "Val Loss: 2.0927: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5882\n",
      "  Class 7 F1: 0.7059\n",
      "  Class 14 F1: 0.7059\n",
      "  Problem Classes Avg F1: 0.6667\n",
      " Epoch 45 | Train Loss: 1.1935 | Train F1: 0.8176 | Val Loss: 1.1642 | Val F1: 0.9269 | LR: 1.19e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6667 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4246, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.78it/s]\n",
      "Val Loss: 0.5019: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6512\n",
      "  Class 7 F1: 0.6842\n",
      "  Class 14 F1: 0.7500\n",
      "  Problem Classes Avg F1: 0.6951\n",
      " Epoch 46 | Train Loss: 1.1433 | Train F1: 0.8396 | Val Loss: 1.1026 | Val F1: 0.9295 | LR: 1.16e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6951 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1112, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.71it/s]\n",
      "Val Loss: 0.8539: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5517\n",
      "  Class 7 F1: 0.6667\n",
      "  Class 14 F1: 0.4615\n",
      "  Problem Classes Avg F1: 0.5600\n",
      " Epoch 47 | Train Loss: 1.1850 | Train F1: 0.8066 | Val Loss: 1.2620 | Val F1: 0.8972 | LR: 1.13e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5600 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6209, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.72it/s]\n",
      "Val Loss: 3.0203: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.4516\n",
      "  Class 7 F1: 0.6316\n",
      "  Class 14 F1: 0.5714\n",
      "  Problem Classes Avg F1: 0.5515\n",
      " Epoch 48 | Train Loss: 1.2225 | Train F1: 0.8036 | Val Loss: 1.2328 | Val F1: 0.9114 | LR: 1.09e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.5515 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7413, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.73it/s]\n",
      "Val Loss: 2.3180: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6667\n",
      "  Class 7 F1: 0.5294\n",
      "  Class 14 F1: 0.6667\n",
      "  Problem Classes Avg F1: 0.6209\n",
      " Epoch 49 | Train Loss: 1.1937 | Train F1: 0.8192 | Val Loss: 1.1869 | Val F1: 0.9169 | LR: 1.06e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6209 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6667, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:23<00:00, 13.60it/s]\n",
      "Val Loss: 3.4184: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.6061\n",
      "  Class 7 F1: 0.6667\n",
      "  Class 14 F1: 0.7500\n",
      "  Problem Classes Avg F1: 0.6742\n",
      " Epoch 50 | Train Loss: 1.1906 | Train F1: 0.8123 | Val Loss: 1.2242 | Val F1: 0.9304 | LR: 1.03e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6742 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.3243, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 314/314 [00:22<00:00, 13.82it/s]\n",
      "Val Loss: 3.9449: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:03<00:00, 23.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 3 F1: 0.5161\n",
      "  Class 7 F1: 0.6230\n",
      "  Class 14 F1: 0.7500\n",
      "  Problem Classes Avg F1: 0.6297\n",
      " Epoch 51 | Train Loss: 1.2100 | Train F1: 0.8004 | Val Loss: 1.3413 | Val F1: 0.9172 | LR: 1.00e-04\n",
      "         Problem Classes (3,7,14) Avg F1: 0.6297 | ‚ö†Ô∏è Problem classes need attention\n",
      "\n",
      "üìà Epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.2310, Mixup: True, Cutout: False, RandomCrop: False:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 200/314 [00:15<00:07, 15.38it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 10. K-Fold Cross Validation Loop with WandB\n",
    "# =============================================================================\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\" FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Í∞Å foldÎ≥Ñ child run ÏÉùÏÑ±\n",
    "    fold_run = wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        entity=ENTITY,\n",
    "        name=f\"fold-{fold+1}-{model_name}-{datetime.now().strftime('%H%M')}\",\n",
    "        config=config,\n",
    "        tags=[\"fold\", f\"fold-{fold+1}\", model_name, \"child-run\"],\n",
    "        group=\"k-fold-experiment\",\n",
    "        job_type=f\"fold-{fold+1}\",\n",
    "        reinit=True  # ÏÉàÎ°úÏö¥ run ÏãúÏûë ÌóàÏö©\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Fold {fold+1} Dashboard: {fold_run.url}\")\n",
    "    \n",
    "    # ÌòÑÏû¨ foldÏùò train/validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
    "    train_fold_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_fold_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Ï†ïÎ≥¥ Î°úÍπÖ\n",
    "    #wandb.log({\n",
    "    #    \"fold_info/fold_number\": fold + 1,\n",
    "    #    \"fold_info/train_samples\": len(train_fold_df),\n",
    "    #    \"fold_info/val_samples\": len(val_fold_df),\n",
    "    #    \"fold_info/train_ratio\": len(train_fold_df) / len(train_df),\n",
    "    #    \"fold_info/val_ratio\": len(val_fold_df) / len(train_df)\n",
    "    #})\n",
    "    \n",
    "    # ÌòÑÏû¨ foldÏùò Dataset ÏÉùÏÑ±\n",
    "    trn_dataset = ImageDataset(\n",
    "        train_fold_df,\n",
    "        \"../data/train/\",\n",
    "        transform=trn_transform,\n",
    "        problem_class_transform=problem_class_extra_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = ImageDataset(\n",
    "        val_fold_df,\n",
    "        \"../data/train/\",\n",
    "        transform=tst_transform  # Í≤ÄÏ¶ùÏóêÎäî Ï¶ùÍ∞ï Ï†ÅÏö© ÏïàÌï®\n",
    "    )\n",
    "    \n",
    "    # ÌòÑÏû¨ foldÏùò DataLoader ÏÉùÏÑ±\n",
    "    trn_loader = DataLoader(\n",
    "        trn_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(trn_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî (Í∞Å foldÎßàÎã§ ÏÉàÎ°úÏö¥ Î™®Îç∏)\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "\n",
    "    model = model.float()\n",
    "    \n",
    "    class_weights = torch.FloatTensor([\n",
    "    4 if i in [3, 7, 14] else 1.0 for i in range(17)]).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.15)\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Learning Rate Scheduler Ï∂îÍ∞Ä\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # ÌòÑÏû¨ foldÏùò ÏµúÍ≥† ÏÑ±Îä• Ï∂îÏ†Å\n",
    "    best_val_f1 = 0.0\n",
    "    best_model = None\n",
    "    patience = 0\n",
    "    max_patience = 50\n",
    "    \n",
    "    print(f\" Î™®Îç∏ ÌïôÏäµ ÏãúÏûë - Fold {fold+1}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 11. Training Loop for Current Fold\n",
    "    # =============================================================================\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nüìà Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        # Training\n",
    "        train_ret = train_one_epoch(\n",
    "            trn_loader, model, optimizer, loss_fn, device, \n",
    "            epoch=epoch, fold=fold+1\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_ret = validate_one_epoch(\n",
    "            val_loader, model, loss_fn, device, \n",
    "            epoch=epoch, fold=fold+1,\n",
    "            log_confusion=(epoch == EPOCHS-1)  # ÎßàÏßÄÎßâ epochÏóêÎßå confusion matrix\n",
    "        )\n",
    "        \n",
    "        # Learning rate Î°úÍπÖ\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä• Í∞úÏÑ† Ïó¨Î∂Ä ÌôïÏù∏\n",
    "        problem_classes_improved = \"\"\n",
    "        if 'avg_problem_f1' in val_ret:\n",
    "            if val_ret['avg_problem_f1'] > best_val_f1 * 0.95:  # Ï†ÑÏ≤¥ ÏÑ±Îä•Ïùò 95% Ïù¥ÏÉÅÏù¥Î©¥ ÏñëÌò∏\n",
    "                problem_classes_improved = \"‚úÖ Problem classes performing well\"\n",
    "            else:\n",
    "                problem_classes_improved = \"‚ö†Ô∏è Problem classes need attention\"\n",
    "\n",
    "\n",
    "        # WandBÏóê metrics Î°úÍπÖ\n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"fold\": fold + 1,\n",
    "            \"train/loss\": train_ret['train_loss'],\n",
    "            \"train/accuracy\": train_ret['train_acc'], \n",
    "            \"train/f1\": train_ret['train_f1'],\n",
    "            \"val/loss\": val_ret['val_loss'],\n",
    "            \"val/accuracy\": val_ret['val_acc'],\n",
    "            \"val/f1\": val_ret['val_f1'],\n",
    "            \"learning_rate\": current_lr,\n",
    "            \"optimizer/lr\": current_lr\n",
    "        }\n",
    "        \n",
    "        # GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Î°úÍπÖ\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated(0) / 1e9\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            log_data.update({\n",
    "                \"system/gpu_memory_used_gb\": gpu_memory_used,\n",
    "                \"system/gpu_memory_total_gb\": gpu_memory_total,\n",
    "                \"system/gpu_utilization_pct\": (gpu_memory_used / gpu_memory_total) * 100\n",
    "            })\n",
    "\n",
    "        if 'avg_problem_f1' in val_ret:\n",
    "            log_data[\"val/problem_classes_avg_f1\"] = val_ret['avg_problem_f1']\n",
    "        \n",
    "        # Í∞úÎ≥Ñ Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä•\n",
    "        for cls in [3, 7, 14]:\n",
    "            if f\"class_{cls}_f1\" in val_ret['problem_class_f1']:\n",
    "                log_data[f\"val/class_{cls}_f1\"] = val_ret['problem_class_f1'][f\"class_{cls}_f1\"]\n",
    "    \n",
    "        \n",
    "        #wandb.log(log_data)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\" Epoch {epoch+1:2d} | \"\n",
    "              f\"Train Loss: {train_ret['train_loss']:.4f} | \"\n",
    "              f\"Train F1: {train_ret['train_f1']:.4f} | \"\n",
    "              f\"Val Loss: {val_ret['val_loss']:.4f} | \"\n",
    "              f\"Val F1: {val_ret['val_f1']:.4f} | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä• Î≥ÑÎèÑ Ï∂úÎ†•\n",
    "        if 'avg_problem_f1' in val_ret:\n",
    "            print(f\"         Problem Classes (3,7,14) Avg F1: {val_ret['avg_problem_f1']:.4f} | {problem_classes_improved}\")\n",
    "\n",
    "        # ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ Ï†ÄÏû• (Ï†ÑÏ≤¥ F1Í≥º Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä• Î™®Îëê Í≥†Î†§)\n",
    "        current_val_f1 = val_ret['val_f1']\n",
    "        problem_class_bonus = 0\n",
    "    \n",
    "        # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä•Ïù¥ Ï¢ãÏúºÎ©¥ Î≥¥ÎÑàÏä§ Ï†êÏàò Î∂ÄÏó¨\n",
    "        if 'avg_problem_f1' in val_ret and val_ret['avg_problem_f1'] > 0.90:\n",
    "            problem_class_bonus = 0.001  # ÏûëÏùÄ Î≥¥ÎÑàÏä§Î°ú ÎèôÏ†êÏùº Îïå Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä• Ïö∞ÏÑ†\n",
    "        \n",
    "        adjusted_f1 = current_val_f1 + problem_class_bonus\n",
    "        \n",
    "        if adjusted_f1 > best_val_f1:\n",
    "            best_val_f1 = current_val_f1  # Ïã§Ï†ú F1 Ïä§ÏΩîÏñ¥Î°ú Ï†ÄÏû•\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            patience = 0\n",
    "            \n",
    "            # ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ ÏïÑÌã∞Ìå©Ìä∏Î°ú Ï†ÄÏû•\n",
    "            model_path = f'best_model_fold_{fold+1}.pth'\n",
    "            torch.save(best_model, model_path)\n",
    "            wandb.save(model_path, policy=\"now\")\n",
    "            \n",
    "            # ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä• Î°úÍπÖ\n",
    "            best_performance_log = {\n",
    "                f\"best_performance/epoch\": epoch + 1,\n",
    "                f\"best_performance/val_f1\": best_val_f1,\n",
    "                f\"best_performance/val_acc\": val_ret['val_acc'],\n",
    "                f\"best_performance/val_loss\": val_ret['val_loss'],\n",
    "            }\n",
    "            \n",
    "            # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä•ÎèÑ ÏµúÍ≥† ÏÑ±Îä•Ïóê Ìè¨Ìï®\n",
    "            if 'avg_problem_f1' in val_ret:\n",
    "                best_performance_log[f\"best_performance/problem_classes_f1\"] = val_ret['avg_problem_f1']\n",
    "            \n",
    "            wandb.log(best_performance_log)\n",
    "            \n",
    "            improvement_msg = f\"üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: {best_val_f1:.4f}\"\n",
    "            if 'avg_problem_f1' in val_ret:\n",
    "                improvement_msg += f\" (Problem Classes: {val_ret['avg_problem_f1']:.4f})\"\n",
    "            print(improvement_msg)\n",
    "        else:\n",
    "            patience += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if patience >= max_patience and epoch > EPOCHS // 2:\n",
    "            print(f\"‚è∏Ô∏è Early stopping at epoch {epoch+1} (patience: {patience})\")\n",
    "            wandb.log({\"early_stopping/epoch\": epoch + 1})\n",
    "            break\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 12. Fold Results Summary\n",
    "    # =============================================================================\n",
    "    \n",
    "    # ÌòÑÏû¨ fold Í≤∞Í≥º Ï†ÄÏû•\n",
    "    fold_result = {\n",
    "        'fold': fold + 1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'final_train_f1': train_ret['train_f1'],\n",
    "        'train_samples': len(trn_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'epochs_trained': epoch + 1,\n",
    "        'early_stopped': patience >= max_patience\n",
    "    }\n",
    "    \n",
    "    fold_results.append(fold_result)\n",
    "    fold_models.append(best_model)\n",
    "    \n",
    "    # Fold ÏµúÏ¢Ö ÏöîÏïΩ Î°úÍπÖ\n",
    "    wandb.log({\n",
    "        \"fold_summary/best_val_f1\": best_val_f1,\n",
    "        \"fold_summary/final_train_f1\": train_ret['train_f1'],\n",
    "        \"fold_summary/epochs_trained\": epoch + 1,\n",
    "        \"fold_summary/improvement\": best_val_f1 - val_ret['val_f1'],\n",
    "        \"fold_summary/early_stopped\": patience >= max_patience\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n Fold {fold + 1} ÏôÑÎ£å!\")\n",
    "    print(f\" ÏµúÍ≥† Validation F1: {best_val_f1:.4f}\")\n",
    "    print(f\" ÌïôÏäµÎêú ÏóêÌè≠: {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Fold run Ï¢ÖÎ£å\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "    del model, optimizer, scheduler, trn_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6085ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    import gc\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# ÏóêÌè¨ÌÅ¨ÎßàÎã§ Ïã§Ìñâ\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " K-FOLD CROSS VALIDATION ÏµúÏ¢Ö Í≤∞Í≥º\n",
      "============================================================\n",
      " ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ ÏÉàÎ°úÏö¥ summary runÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/computervisioncompetition-cv-1/mywork/experiments/wandb/run-20250909_143034-lixslm2x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/lixslm2x' target=\"_blank\">SUMMARY-convnext_base_384_in22ft1k-0909-1430</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/lixslm2x' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/lixslm2x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CV Í≤∞Í≥º Î°úÍπÖ ÏôÑÎ£å!\n",
      "Fold 1: 0.9366 (10 epochs)  Completed\n",
      "Fold 2: 0.8998 (10 epochs)  Completed\n",
      "Fold 3: 0.9013 (10 epochs)  Completed\n",
      "Fold 4: 0.8891 (10 epochs)  Completed\n",
      "Fold 5: 0.9149 (10 epochs)  Completed\n",
      "\n",
      " ÌèâÍ∑† CV F1: 0.9083 ¬± 0.0163\n",
      " ÏµúÍ≥† Fold: 0.9366\n",
      " ÏµúÏïÖ Fold: 0.8891\n",
      " ÏÑ±Îä• Î≤îÏúÑ: 0.0475\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 13. K-Fold Cross Validation Results Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" K-FOLD CROSS VALIDATION ÏµúÏ¢Ö Í≤∞Í≥º\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "val_f1_scores = [result['best_val_f1'] for result in fold_results]\n",
    "mean_f1 = np.mean(val_f1_scores)\n",
    "std_f1 = np.std(val_f1_scores)\n",
    "\n",
    "try:\n",
    "    # wandb.runÏù¥ ÌòÑÏû¨ ÌôúÏÑ±ÌôîÎêú runÏùÑ Í∞ÄÎ¶¨ÌÇ¥\n",
    "    if wandb.run is None:\n",
    "        print(\" ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ ÏÉàÎ°úÏö¥ summary runÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\")\n",
    "        active_run = wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            name=f\"SUMMARY-{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "            config=config,\n",
    "            tags=[\"summary\", \"cv-results\", model_name],\n",
    "            group=\"k-fold-experiment\",\n",
    "            job_type=\"summary\",\n",
    "            reinit=True\n",
    "        )\n",
    "    else:\n",
    "        print(\" Í∏∞Ï°¥ runÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "        active_run = wandb.run\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Run ÏÉÅÌÉú ÌôïÏù∏ Ï§ë ÏóêÎü¨: {e}\")\n",
    "    # ÏÉàÎ°úÏö¥ run ÏÉùÏÑ±\n",
    "    active_run = wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        name=f\"SUMMARY-{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "        config=config,\n",
    "        tags=[\"summary\", \"cv-results\", model_name],\n",
    "        group=\"k-fold-experiment\",\n",
    "        job_type=\"summary\",\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "# CV ÏöîÏïΩ ÌÖåÏù¥Î∏î ÏÉùÏÑ±\n",
    "fold_table = wandb.Table(columns=[\n",
    "    \"Fold\", \"Best_Val_F1\", \"Final_Train_F1\", \"Train_Samples\", \n",
    "    \"Val_Samples\", \"Epochs_Trained\", \"Early_Stopped\"\n",
    "])\n",
    "\n",
    "for result in fold_results:\n",
    "    fold_table.add_data(\n",
    "        result['fold'], \n",
    "        result['best_val_f1'], \n",
    "        result['final_train_f1'],\n",
    "        result['train_samples'], \n",
    "        result['val_samples'],\n",
    "        result['epochs_trained'],\n",
    "        result['early_stopped']\n",
    "    )\n",
    "\n",
    "# ÏïàÏ†ÑÌïú Î°úÍπÖ\n",
    "try:\n",
    "    active_run.log({\n",
    "        \"cv_results/mean_f1\": mean_f1,\n",
    "        \"cv_results/std_f1\": std_f1,\n",
    "        \"cv_results/best_fold_f1\": max(val_f1_scores),\n",
    "        \"cv_results/worst_fold_f1\": min(val_f1_scores),\n",
    "        \"cv_results/f1_range\": max(val_f1_scores) - min(val_f1_scores),\n",
    "        \"cv_results/fold_results_table\": fold_table,\n",
    "        \"cv_results/n_folds\": N_FOLDS,\n",
    "        \"cv_results/total_epochs\": sum([r['epochs_trained'] for r in fold_results]),\n",
    "        \"cv_results/avg_epochs_per_fold\": np.mean([r['epochs_trained'] for r in fold_results]),\n",
    "        \"cv_results/early_stopped_folds\": sum([r['early_stopped'] for r in fold_results])\n",
    "    })\n",
    "    \n",
    "    # FoldÎ≥Ñ ÏÑ±Îä• Î∞îÏ∞®Ìä∏ ÏÉùÏÑ±\n",
    "    fold_performance_data = [[f\"Fold {i+1}\", score] for i, score in enumerate(val_f1_scores)]\n",
    "    active_run.log({\n",
    "        \"cv_results/fold_performance_chart\": wandb.plot.bar(\n",
    "            wandb.Table(data=fold_performance_data, columns=[\"Fold\", \"F1_Score\"]),\n",
    "            \"Fold\", \"F1_Score\", \n",
    "            title=\"K-Fold Cross Validation Performance\"\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    print(\" CV Í≤∞Í≥º Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" WandB Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "    print(\" Í≤∞Í≥ºÎ•º ÏΩòÏÜîÏóê Ï∂úÎ†•Ìï©ÎãàÎã§:\")\n",
    "\n",
    "# Ïñ¥Îñ§ Í≤ΩÏö∞Îì† ÏΩòÏÜîÏóêÎäî Í≤∞Í≥º Ï∂úÎ†•\n",
    "for result in fold_results:\n",
    "    status = \" Early Stopped\" if result['early_stopped'] else \" Completed\"\n",
    "    print(f\"Fold {result['fold']}: {result['best_val_f1']:.4f} \"\n",
    "          f\"({result['epochs_trained']} epochs) {status}\")\n",
    "\n",
    "print(f\"\\n ÌèâÍ∑† CV F1: {mean_f1:.4f} ¬± {std_f1:.4f}\")\n",
    "print(f\" ÏµúÍ≥† Fold: {max(val_f1_scores):.4f}\")\n",
    "print(f\" ÏµúÏïÖ Fold: {min(val_f1_scores):.4f}\")\n",
    "print(f\" ÏÑ±Îä• Î≤îÏúÑ: {max(val_f1_scores) - min(val_f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb258fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß ÏïôÏÉÅÎ∏î Î™®Îç∏ Ï§ÄÎπÑ Ï§ë...\n",
      "Fold 1 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "Fold 2 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "Fold 3 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "Fold 4 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "Fold 5 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      " Ï¥ù 5Í∞ú Î™®Îç∏Î°ú ÏïôÏÉÅÎ∏î Íµ¨ÏÑ±\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 14. Ensemble Models Preparation\n",
    "# =============================================================================\n",
    "\n",
    "# 5-Fold ÏïôÏÉÅÎ∏î Î™®Îç∏ Ï§ÄÎπÑ\n",
    "ensemble_models = []\n",
    "print(f\"\\nüîß ÏïôÏÉÅÎ∏î Î™®Îç∏ Ï§ÄÎπÑ Ï§ë...\")\n",
    "\n",
    "for i, state_dict in enumerate(fold_models):\n",
    "    fold_model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    fold_model.load_state_dict(state_dict)\n",
    "    fold_model.eval()\n",
    "    ensemble_models.append(fold_model)\n",
    "    print(f\"Fold {i+1} Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\")\n",
    "\n",
    "print(f\" Ï¥ù {len(ensemble_models)}Í∞ú Î™®Îç∏Î°ú ÏïôÏÉÅÎ∏î Íµ¨ÏÑ±\")\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"ensemble/num_models\": len(ensemble_models),\n",
    "            \"ensemble/model_architecture\": model_name,\n",
    "            \"ensemble/ensemble_type\": \"simple_average\"\n",
    "        })\n",
    "    else:\n",
    "        print(\"üìä ÏïôÏÉÅÎ∏î Ï†ïÎ≥¥:\")\n",
    "        print(f\"  - Î™®Îç∏ Í∞úÏàò: {len(ensemble_models)}\")\n",
    "        print(f\"  - ÏïÑÌÇ§ÌÖçÏ≤ò: {model_name}\")\n",
    "        print(f\"  - ÏïôÏÉÅÎ∏î ÌÉÄÏûÖ: simple_average\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ÏïôÏÉÅÎ∏î Ï†ïÎ≥¥ Î°úÍπÖ Ïã§Ìå®: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb2ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TTA (Test Time Augmentation) ÏÑ§Ï†ï...\n",
      "TTA Î≥ÄÌôò 5Í∞ú Ï§ÄÎπÑ ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 15. TTA (Test Time Augmentation) Setup\n",
    "# =============================================================================\n",
    "\n",
    "# Temperature Scaling ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self, temperature=1.5):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * temperature)\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "print(f\"\\n TTA (Test Time Augmentation) ÏÑ§Ï†ï...\")\n",
    "\n",
    "# Essential TTA transforms\n",
    "essential_tta_transforms = [\n",
    "    # ÏõêÎ≥∏\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 90ÎèÑ ÌöåÏ†ÑÎì§\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[90, 90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[180, 180], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[-90, -90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # Î∞ùÍ∏∞ Í∞úÏÑ†\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[0.3, 0.3], contrast_limit=[0.3, 0.3], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "print(f\"TTA Î≥ÄÌôò {len(essential_tta_transforms)}Í∞ú Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"tta/num_transforms\": len(essential_tta_transforms),\n",
    "            \"tta/transforms_used\": [\"original\", \"rot_90\", \"rot_180\", \"rot_270\", \"brightness\"],\n",
    "            \"tta/batch_size\": 64  # TTAÏö© Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "        })\n",
    "    else:\n",
    "        print(\"üìä TTA ÏÑ§Ï†ï Ï†ïÎ≥¥:\")\n",
    "        print(f\"  - Î≥ÄÌòï Í∞úÏàò: {len(essential_tta_transforms)}\")\n",
    "        print(f\"  - Î≥ÄÌòï Ï¢ÖÎ•ò: original, rot_90, rot_180, rot_270, brightness\")\n",
    "        print(f\"  - Î∞∞Ïπò ÌÅ¨Í∏∞: 64\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è TTA ÏÑ§Ï†ï Î°úÍπÖ Ïã§Ìå®: {e}\")\n",
    "    print(\"üìä TTA ÏÑ§Ï†ï Ï†ïÎ≥¥:\")\n",
    "    print(f\"  - Î≥ÄÌòï Í∞úÏàò: {len(essential_tta_transforms)}\")\n",
    "    print(f\"  - Î∞∞Ïπò ÌÅ¨Í∏∞: 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TTA Dataset: 3140Í∞ú ÌÖåÏä§Ìä∏ ÏÉòÌîå\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 16. TTA Dataset and DataLoader\n",
    "# =============================================================================\n",
    "\n",
    "class TTAImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms  # Ïó¨Îü¨ transformÏùÑ Î¶¨Ïä§Ìä∏Î°ú Î∞õÏùå\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        \n",
    "        # Î™®Îì† transformÏùÑ Ï†ÅÏö©Ìïú Í≤∞Í≥ºÎ•º Î¶¨Ïä§Ìä∏Î°ú Î∞òÌôò\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            aug_img = transform(image=img)['image']\n",
    "            augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target\n",
    "\n",
    "# TTA Dataset ÏÉùÏÑ±\n",
    "tta_dataset = TTAImageDataset(\n",
    "    \"../data/sample_submission.csv\",\n",
    "    \"../data/test/\",\n",
    "    essential_tta_transforms\n",
    ")\n",
    "\n",
    "# TTA DataLoader (Î∞∞Ïπò ÌÅ¨Í∏∞Î•º Ï§ÑÏó¨ÏÑú Î©îÎ™®Î¶¨ Ï†àÏïΩ)\n",
    "tta_loader = DataLoader(\n",
    "    tta_dataset,\n",
    "    batch_size=64,  # TTAÎäî Î©îÎ™®Î¶¨Î•º ÎßéÏù¥ ÏÇ¨Ïö©ÌïòÎØÄÎ°ú Î∞∞Ïπò ÌÅ¨Í∏∞ Ï§ÑÏûÑ\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\" TTA Dataset: {len(tta_dataset)}Í∞ú ÌÖåÏä§Ìä∏ ÏÉòÌîå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ÏµúÏ¢Ö Ï∂îÎ°† - ÏïôÏÉÅÎ∏î + TTA\n",
      "============================================================\n",
      "ÏïôÏÉÅÎ∏î TTA Ï∂îÎ°† ÏãúÏûë...\n",
      "5Í∞ú Î™®Îç∏ √ó 5Í∞ú TTA Î≥ÄÌòï = 25Í∞ú ÏòàÏ∏° ÌèâÍ∑†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble TTA:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble TTA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:29<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ÏïôÏÉÅÎ∏î TTA Ï∂îÎ°† ÏôÑÎ£å!\n",
      "Ï¥ù ÏÜåÏöîÏãúÍ∞Ñ: 2.5Î∂Ñ\n",
      " ÌèâÍ∑† Ïã†Î¢∞ÎèÑ: 0.4079 ¬± 0.0771\n",
      " Í≥†Ïã†Î¢∞ÎèÑ ÏÉòÌîå: 0/3140 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 17. Ensemble + TTA Inference with WandB Logging\n",
    "# =============================================================================\n",
    "\n",
    "def ensemble_tta_inference_with_logging(models, loader, transforms, confidence_threshold=0.9):\n",
    "    \"\"\"5-Fold Î™®Îç∏ ÏïôÏÉÅÎ∏î + TTA Ï∂îÎ°† with WandB Î°úÍπÖ\"\"\"\n",
    "    all_predictions = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    # TTA ÏßÑÌñâÏÉÅÌô© Î°úÍπÖÏùÑ ÏúÑÌïú ÌÖåÏù¥Î∏î\n",
    "    tta_progress = wandb.Table(columns=[\"Batch\", \"Avg_Confidence\", \"Low_Conf_Count\", \"High_Conf_Count\"])\n",
    "    \n",
    "    # Temperature scaling Ï¥àÍ∏∞Ìôî\n",
    "    temp_scaling = TemperatureScaling().to(device)\n",
    "    \n",
    "    print(f\"ÏïôÏÉÅÎ∏î TTA Ï∂îÎ°† ÏãúÏûë...\")\n",
    "    print(f\"{len(models)}Í∞ú Î™®Îç∏ √ó {len(transforms)}Í∞ú TTA Î≥ÄÌòï = {len(models) * len(transforms)}Í∞ú ÏòàÏ∏° ÌèâÍ∑†\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Ensemble TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        ensemble_probs = torch.zeros(batch_size, 17).to(device)\n",
    "        \n",
    "        # Í∞Å fold Î™®Îç∏Î≥Ñ ÏòàÏ∏°\n",
    "        for model_idx, model in enumerate(models):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Í∞Å TTA Î≥ÄÌòïÎ≥Ñ ÏòàÏ∏°\n",
    "                for tta_idx, images in enumerate(images_list):\n",
    "                    images = images.to(device)\n",
    "                    preds = model(images)\n",
    "                    \n",
    "                    # Temperature scaling Ï†ÅÏö©\n",
    "                    preds = temp_scaling(preds)\n",
    "                    probs = torch.softmax(preds, dim=1)\n",
    "                    \n",
    "                    # ÏïôÏÉÅÎ∏î ÌôïÎ•†Ïóê ÎàÑÏ†Å (ÌèâÍ∑†)\n",
    "                    ensemble_probs += probs / (len(models) * len(images_list))\n",
    "        \n",
    "        # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞\n",
    "        max_probs = torch.max(ensemble_probs, dim=1)[0]\n",
    "        batch_confidences = max_probs.cpu().numpy()\n",
    "        all_confidences.extend(batch_confidences)\n",
    "        \n",
    "        final_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "        all_predictions.extend(final_preds.cpu().numpy())\n",
    "        \n",
    "        # Î∞∞ÏπòÎ≥Ñ Ïã†Î¢∞ÎèÑ Î∂ÑÏÑù\n",
    "        high_conf_count = np.sum(batch_confidences >= confidence_threshold)\n",
    "        low_conf_count = batch_size - high_conf_count\n",
    "        avg_confidence = np.mean(batch_confidences)\n",
    "        \n",
    "        # ÏßÑÌñâÏÉÅÌô© ÌÖåÏù¥Î∏îÏóê Ï∂îÍ∞Ä\n",
    "        tta_progress.add_data(batch_idx, avg_confidence, low_conf_count, high_conf_count)\n",
    "        \n",
    "        # Î∞∞ÏπòÎ≥Ñ ÏÉÅÏÑ∏ Î°úÍπÖ (20Î∞∞ÏπòÎßàÎã§)\n",
    "        if batch_idx % 20 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            estimated_total = elapsed_time * len(loader) / (batch_idx + 1)\n",
    "            remaining_time = estimated_total - elapsed_time\n",
    "            \n",
    "            wandb.log({\n",
    "                \"tta_progress/batch\": batch_idx,\n",
    "                \"tta_progress/avg_confidence\": avg_confidence,\n",
    "                \"tta_progress/high_confidence_ratio\": high_conf_count / batch_size,\n",
    "                \"tta_progress/low_confidence_count\": low_conf_count,\n",
    "                \"tta_progress/elapsed_time_min\": elapsed_time / 60,\n",
    "                \"tta_progress/estimated_remaining_min\": remaining_time / 60,\n",
    "                \"tta_progress/samples_processed\": (batch_idx + 1) * batch_size,\n",
    "            })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # TTA ÏµúÏ¢Ö Í≤∞Í≥º Î°úÍπÖ\n",
    "    final_avg_confidence = np.mean(all_confidences)\n",
    "    confidence_std = np.std(all_confidences)\n",
    "    high_conf_samples = np.sum(np.array(all_confidences) >= confidence_threshold)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"tta_results/total_time_min\": total_time / 60,\n",
    "        \"tta_results/samples_per_second\": len(all_predictions) / total_time,\n",
    "        \"tta_results/final_avg_confidence\": final_avg_confidence,\n",
    "        \"tta_results/confidence_std\": confidence_std,\n",
    "        \"tta_results/high_confidence_samples\": high_conf_samples,\n",
    "        \"tta_results/high_confidence_ratio\": high_conf_samples / len(all_predictions),\n",
    "        \"tta_results/total_predictions\": len(all_predictions),\n",
    "        \"tta_results/confidence_histogram\": wandb.Histogram(all_confidences),\n",
    "        \"tta_results/progress_table\": tta_progress\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n ÏïôÏÉÅÎ∏î TTA Ï∂îÎ°† ÏôÑÎ£å!\")\n",
    "    print(f\"Ï¥ù ÏÜåÏöîÏãúÍ∞Ñ: {total_time/60:.1f}Î∂Ñ\")\n",
    "    print(f\" ÌèâÍ∑† Ïã†Î¢∞ÎèÑ: {final_avg_confidence:.4f} ¬± {confidence_std:.4f}\")\n",
    "    print(f\" Í≥†Ïã†Î¢∞ÎèÑ ÏÉòÌîå: {high_conf_samples}/{len(all_predictions)} ({high_conf_samples/len(all_predictions)*100:.1f}%)\")\n",
    "    \n",
    "    return all_predictions, all_confidences\n",
    "\n",
    "# ÏïôÏÉÅÎ∏î TTA Ïã§Ìñâ\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" ÏµúÏ¢Ö Ï∂îÎ°† - ÏïôÏÉÅÎ∏î + TTA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "tta_predictions, confidences = ensemble_tta_inference_with_logging(\n",
    "    models=ensemble_models, \n",
    "    loader=tta_loader, \n",
    "    transforms=essential_tta_transforms,\n",
    "    confidence_threshold=0.9\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98e388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ÏµúÏ¢Ö Í≤∞Í≥º Ï†ïÎ¶¨ Ï§ë...\n",
      "\n",
      "üìä ÏòàÏ∏° Í≤∞Í≥º Î∂ÑÌè¨:\n",
      "Class  0:  200 (  6.4%)\n",
      "Class  1:   90 (  2.9%)\n",
      "Class  2:  200 (  6.4%)\n",
      "Class  3:  236 (  7.5%)\n",
      "Class  4:  171 (  5.4%)\n",
      "Class  5:  200 (  6.4%)\n",
      "Class  6:  204 (  6.5%)\n",
      "Class  7:  178 (  5.7%)\n",
      "Class  8:  200 (  6.4%)\n",
      "Class  9:  200 (  6.4%)\n",
      "Class 10:  201 (  6.4%)\n",
      "Class 11:  190 (  6.1%)\n",
      "Class 12:  199 (  6.3%)\n",
      "Class 13:  156 (  5.0%)\n",
      "Class 14:  115 (  3.7%)\n",
      "Class 15:  200 (  6.4%)\n",
      "Class 16:  200 (  6.4%)\n",
      "ÏµúÏ¢Ö Í≤∞Í≥º WandB Î°úÍπÖ ÏôÑÎ£å!\n",
      "Ï¥ù ÏòàÏ∏° Ïàò: 3140\n",
      "ÏòàÏ∏°Îêú ÌÅ¥ÎûòÏä§ Ïàò: 17\n",
      "ÌèâÍ∑† Ïã†Î¢∞ÎèÑ: 0.4079\n",
      "Ïã†Î¢∞ÎèÑ Î≤îÏúÑ: 0.1379 ~ 0.7925\n",
      "ÏòàÏ∏° Î∂ÑÌè¨ Ï∞®Ìä∏ Î°úÍπÖ ÏôÑÎ£å!\n",
      "Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ ÏôÑÎ£å!\n",
      "\n",
      " ÏµúÏ¢Ö Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å!\n",
      " ÌååÏùº ÏúÑÏπò: ../data/output/choice12.csv\n",
      " Ï¥ù ÏòàÏ∏° Ïàò: 3140\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 18. Final Results and Submission\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n ÏµúÏ¢Ö Í≤∞Í≥º Ï†ïÎ¶¨ Ï§ë...\")\n",
    "\n",
    "# TTA Í≤∞Í≥ºÎ°ú submission ÌååÏùº ÏÉùÏÑ±\n",
    "tta_pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n",
    "tta_pred_df['target'] = tta_predictions\n",
    "\n",
    "# Í∏∞Ï°¥ submissionÍ≥º ÎèôÏùºÌïú ÏàúÏÑúÏù∏ÏßÄ ÌôïÏù∏\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == tta_pred_df['ID']).all(), \"ID ÏàúÏÑú Î∂àÏùºÏπò!\"\n",
    "\n",
    "# ÏòàÏ∏° Î∂ÑÌè¨ Î∂ÑÏÑù\n",
    "pred_distribution = tta_pred_df['target'].value_counts().sort_index()\n",
    "pred_table = wandb.Table(columns=[\"Class\", \"Count\", \"Percentage\"])\n",
    "\n",
    "print(f\"\\nüìä ÏòàÏ∏° Í≤∞Í≥º Î∂ÑÌè¨:\")\n",
    "for class_id in range(17):\n",
    "    count = pred_distribution.get(class_id, 0)\n",
    "    percentage = count / len(tta_pred_df) * 100\n",
    "    pred_table.add_data(class_id, count, percentage)\n",
    "    print(f\"Class {class_id:2d}: {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "# Ïã†Î¢∞ÎèÑ Î∂ÑÏÑù\n",
    "confidence_bins = [0.5, 0.7, 0.8, 0.9, 0.95, 1.0]\n",
    "confidence_analysis = {}\n",
    "for i, threshold in enumerate(confidence_bins):\n",
    "    if i == 0:\n",
    "        count = np.sum(np.array(confidences) >= threshold)\n",
    "    else:\n",
    "        prev_threshold = confidence_bins[i-1]\n",
    "        count = np.sum((np.array(confidences) >= prev_threshold) & (np.array(confidences) < threshold))\n",
    "    confidence_analysis[f\"conf_{threshold}\"] = count\n",
    "\n",
    "# ÏµúÏ¢Ö Í≤∞Í≥º Î°úÍπÖ\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"final_results/total_predictions\": len(tta_predictions),\n",
    "            \"final_results/unique_classes_predicted\": len(np.unique(tta_predictions)),\n",
    "            \"final_results/prediction_distribution_table\": pred_table,\n",
    "            \"final_results/avg_confidence\": np.mean(confidences),\n",
    "            \"final_results/median_confidence\": np.median(confidences),\n",
    "            \"final_results/min_confidence\": np.min(confidences),\n",
    "            \"final_results/max_confidence\": np.max(confidences),\n",
    "            \"final_results/confidence_distribution\": wandb.Histogram(confidences),\n",
    "            **confidence_analysis\n",
    "        })\n",
    "        print(\"ÏµúÏ¢Ö Í≤∞Í≥º WandB Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"WandB Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "# ÏΩòÏÜî Ï∂úÎ†•ÏùÄ Ìï≠ÏÉÅ Ïã§Ìñâ\n",
    "print(f\"Ï¥ù ÏòàÏ∏° Ïàò: {len(tta_predictions)}\")\n",
    "print(f\"ÏòàÏ∏°Îêú ÌÅ¥ÎûòÏä§ Ïàò: {len(np.unique(tta_predictions))}\")\n",
    "print(f\"ÌèâÍ∑† Ïã†Î¢∞ÎèÑ: {np.mean(confidences):.4f}\")\n",
    "print(f\"Ïã†Î¢∞ÎèÑ Î≤îÏúÑ: {np.min(confidences):.4f} ~ {np.max(confidences):.4f}\")\n",
    "\n",
    "\n",
    "# ÏòàÏ∏° Î∂ÑÌè¨ Î∞îÏ∞®Ìä∏\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        pred_dist_data = [[f\"Class_{i}\", pred_distribution.get(i, 0)] for i in range(17)]\n",
    "        wandb.run.log({\n",
    "            \"final_results/prediction_distribution_chart\": wandb.plot.bar(\n",
    "                wandb.Table(data=pred_dist_data, columns=[\"Class\", \"Count\"]),\n",
    "                \"Class\", \"Count\", \n",
    "                title=\"Final Prediction Distribution\"\n",
    "            )\n",
    "        })\n",
    "        print(\"ÏòàÏ∏° Î∂ÑÌè¨ Ï∞®Ìä∏ Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"Ï∞®Ìä∏ Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ï∞®Ìä∏ Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû•\n",
    "output_path = \"../data/output/convnext_2.csv\"\n",
    "tta_pred_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Í≤∞Í≥º ÌååÏùºÏùÑ WandB ÏïÑÌã∞Ìå©Ìä∏Î°ú Ï†ÄÏû•\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"final_predictions\",\n",
    "    type=\"predictions\",\n",
    "    description=f\"Final ensemble predictions with {N_FOLDS}-fold CV + TTA\"\n",
    ")\n",
    "artifact.add_file(output_path)\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log_artifact(artifact)\n",
    "        print(\"Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\n ÏµúÏ¢Ö Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å!\")\n",
    "print(f\" ÌååÏùº ÏúÑÏπò: {output_path}\")\n",
    "print(f\" Ï¥ù ÏòàÏ∏° Ïàò: {len(tta_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ ÏôÑÎ£å!\n",
      "ÏµúÏ¢Ö ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å!\n",
      "\n",
      "Ïã§Ìóò ÏôÑÎ£å ÏãúÍ∞Ñ: 2025-09-09 01:22:54\n",
      "\n",
      "============================================================\n",
      "Ïã§Ìóò ÏôÑÎ£å!\n",
      "============================================================\n",
      " K-Fold CV Í≤∞Í≥º: 0.9381 ¬± 0.0035\n",
      " ÏµúÍ≥† ÏÑ±Îä• Fold: 0.9420\n",
      " ÏïôÏÉÅÎ∏î Î™®Îç∏: 5Í∞ú\n",
      " TTA Î≥ÄÌòï: 5Í∞ú\n",
      " ÌèâÍ∑† ÏòàÏ∏° Ïã†Î¢∞ÎèÑ: 0.4079\n",
      " WandB ÎåÄÏãúÎ≥¥Îìú: https://wandb.ai/kimsunmin0227-hufs/document-classification-team-CV/runs/l87qyesj\n",
      "\n",
      " ÏòàÏ∏° Í≤∞Í≥º ÏÉòÌîå:\n",
      "                     ID  target\n",
      "0  0008fdb22ddce0ce.jpg       2\n",
      "1  00091bffdffd83de.jpg      12\n",
      "2  00396fbc1f6cc21d.jpg       5\n",
      "3  00471f8038d9c4b6.jpg      12\n",
      "4  00901f504008d884.jpg       2\n",
      "5  009b22decbc7220c.jpg      15\n",
      "6  00b33e0ee6d59427.jpg       0\n",
      "7  00bbdcfbbdb3e131.jpg       8\n",
      "8  00c03047e0fbef40.jpg      15\n",
      "9  00c0dabb63ca7a16.jpg      11\n",
      "\n",
      " Î™®Îì† ÏûëÏóÖ ÏôÑÎ£å!\n",
      " Í≤∞Í≥º ÌååÏùº: ../data/output/choice12.csv\n",
      " WandBÏóêÏÑú Ï†ÑÏ≤¥ Ïã§Ìóò Í≤∞Í≥ºÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 19. Experiment Summary and Cleanup\n",
    "# =============================================================================\n",
    "\n",
    "# Ïã§Ìóò ÏöîÏïΩ ÏÉùÏÑ±\n",
    "experiment_summary = {\n",
    "    \"experiment_name\": main_run.name,\n",
    "    \"model_architecture\": model_name,\n",
    "    \"image_size\": img_size,\n",
    "    \"cv_strategy\": f\"{N_FOLDS}-Fold StratifiedKFold\",\n",
    "    \"cv_mean_f1\": mean_f1,\n",
    "    \"cv_std_f1\": std_f1,\n",
    "    \"cv_best_fold\": max(val_f1_scores),\n",
    "    \"ensemble_models\": len(ensemble_models),\n",
    "    \"tta_transforms\": len(essential_tta_transforms),\n",
    "    \"total_training_time_min\": sum([r['epochs_trained'] for r in fold_results]) * 2,  # Ï∂îÏ†ïÏπò\n",
    "    \"avg_prediction_confidence\": np.mean(confidences),\n",
    "    \"high_confidence_predictions\": np.sum(np.array(confidences) >= 0.9),\n",
    "    \"experiment_tags\": [\"baseline\", \"efficientnet-b4\", \"k-fold-cv\", \"tta\", \"ensemble\"]\n",
    "}\n",
    "\n",
    "# Ïã§Ìóò ÏöîÏïΩ\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\"experiment_summary\": experiment_summary})\n",
    "        print(\"Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "\n",
    "# ÎßàÏßÄÎßâ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"status\": \"completed\",\n",
    "            \"completion_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"total_runtime_hours\": 0  # start_time ÏÜçÏÑ± Î¨∏Ï†úÎ°ú ÏùºÎã® 0ÏúºÎ°ú ÏÑ§Ï†ï\n",
    "        })\n",
    "        print(\"ÏµúÏ¢Ö ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏Î•º Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "print(f\"\\nÏã§Ìóò ÏôÑÎ£å ÏãúÍ∞Ñ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Ïã§Ìóò ÏôÑÎ£å!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\" K-Fold CV Í≤∞Í≥º: {mean_f1:.4f} ¬± {std_f1:.4f}\")\n",
    "print(f\" ÏµúÍ≥† ÏÑ±Îä• Fold: {max(val_f1_scores):.4f}\")\n",
    "print(f\" ÏïôÏÉÅÎ∏î Î™®Îç∏: {len(ensemble_models)}Í∞ú\")\n",
    "print(f\" TTA Î≥ÄÌòï: {len(essential_tta_transforms)}Í∞ú\")\n",
    "print(f\" ÌèâÍ∑† ÏòàÏ∏° Ïã†Î¢∞ÎèÑ: {np.mean(confidences):.4f}\")\n",
    "print(f\" WandB ÎåÄÏãúÎ≥¥Îìú: {main_run.url}\")\n",
    "\n",
    "# Sample predictions Ï∂úÎ†•\n",
    "print(f\"\\n ÏòàÏ∏° Í≤∞Í≥º ÏÉòÌîå:\")\n",
    "print(tta_pred_df.head(10))\n",
    "\n",
    "# Î©îÏù∏ run Ï¢ÖÎ£å\n",
    "main_run.finish()\n",
    "\n",
    "print(f\"\\n Î™®Îì† ÏûëÏóÖ ÏôÑÎ£å!\")\n",
    "print(f\" Í≤∞Í≥º ÌååÏùº: {output_path}\")\n",
    "print(f\" WandBÏóêÏÑú Ï†ÑÏ≤¥ Ïã§Ìóò Í≤∞Í≥ºÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî!\")\n",
    "\n",
    "# Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "del ensemble_models\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf1b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
