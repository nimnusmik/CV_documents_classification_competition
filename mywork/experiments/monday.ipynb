{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bbe02e",
   "metadata": {},
   "source": [
    "# üìÑ Document type classification baseline code with WandB Integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92dc69ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations==1.3.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: ipykernel==6.27.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (6.27.1)\n",
      "Requirement already satisfied: ipython==8.15.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (8.15.0)\n",
      "Requirement already satisfied: ipywidgets==8.1.1 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (8.1.1)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (0.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (1.26.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 8)) (2.1.4)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 9)) (9.4.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: timm==0.9.12 in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 11)) (0.9.12)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 12)) (6.3.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 13)) (3.10.6)\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from -r ../requirements.txt (line 14)) (4.5.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.11.4)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.22.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations==1.3.1->-r ../requirements.txt (line 1)) (4.8.1.78)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.5.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (1.5.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel==6.27.1->-r ../requirements.txt (line 2)) (5.7.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython==8.15.0->-r ../requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1->-r ../requirements.txt (line 4)) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1->-r ../requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.0.6)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.10/site-packages (from jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.12.0)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.16.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.19.4)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.12->-r ../requirements.txt (line 11)) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r ../requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.10/site-packages (from plotly->-r ../requirements.txt (line 12)) (2.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (3.2.3)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (1.16.5)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (2.0.43)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna->-r ../requirements.txt (line 14)) (4.65.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (4.15.0)\n",
      "Requirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r ../requirements.txt (line 14)) (2.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.27.1->-r ../requirements.txt (line 2)) (4.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 8)) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (2023.12.9)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ../requirements.txt (line 1)) (0.3)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna->-r ../requirements.txt (line 14)) (3.2.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (3.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (2023.9.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (5.9.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.12.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.25.2)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.0.9)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.2.3)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython==8.15.0->-r ../requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.1.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.5.0)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (4.20.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.12->-r ../requirements.txt (line 11)) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1->-r ../requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm==0.9.12->-r ../requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (21.2.0)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.1)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->-r ../requirements.txt (line 5)) (2.8.19.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 0. Prepare Environments & Install Libraries\n",
    "# =============================================================================\n",
    "\n",
    "# ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÑ§ÏπòÌï©ÎãàÎã§.\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "773408ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Import Libraries & Define Functions\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import optuna, math\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed PrecisionÏö©\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# WandB Í¥ÄÎ†® import Ï∂îÍ∞Ä\n",
    "import wandb\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e142354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB Î°úÍ∑∏Ïù∏ ÏÉÅÌÉú: kimsunmin0227\n",
      "ÌîÑÎ°úÏ†ùÌä∏: document-classification-team-CV\n",
      "Ïã§ÌóòÎ™Ö: efficientnet-b3-baseline\n",
      "ÌåÄÏõêÎì§ÏùÄ EXPERIMENT_NAMEÏùÑ Í∞ÅÏûê Îã§Î•¥Í≤å Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1-1. WandB Login and Configuration\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "üöÄ ÌåÄÏõê ÏÇ¨Ïö© Í∞ÄÏù¥Îìú:\n",
    "\n",
    "1. WandB Í≥ÑÏ†ï ÏÉùÏÑ±: https://wandb.ai/signup\n",
    "2. Ïù¥ ÏÖÄ Ïã§Ìñâ Ïãú Î°úÍ∑∏Ïù∏ ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä ÎÇòÌÉÄÎÇòÎ©¥ Í∞úÏù∏ API ÌÇ§ ÏûÖÎ†•\n",
    "3. EXPERIMENT_NAMEÏùÑ Îã§ÏùåÍ≥º Í∞ôÏù¥ Î≥ÄÍ≤Ω:\n",
    "   - \"member1-baseline\"\n",
    "   - \"member2-augmentation-test\"  \n",
    "   - \"member3-hyperparameter-tuning\"\n",
    "   Îì±Îì± Í∞ÅÏûê Îã§Î•∏ Ïù¥Î¶Ñ ÏÇ¨Ïö©\n",
    "\n",
    "4. ÌåÄ ÎåÄÏãúÎ≥¥Îìú URL: [Ïó¨Í∏∞Ïóê ÎãπÏã†Ïùò ÌîÑÎ°úÏ†ùÌä∏ URL Ï∂îÍ∞Ä]\n",
    "\n",
    "‚ö†Ô∏è Ï£ºÏùòÏÇ¨Ìï≠:\n",
    "- Ï†àÎåÄ API ÌÇ§Î•º ÏΩîÎìúÏóê ÌïòÎìúÏΩîÎî©ÌïòÏßÄ ÎßàÏÑ∏Ïöî\n",
    "- EXPERIMENT_NAMEÎßå Î≥ÄÍ≤ΩÌïòÍ≥† PROJECT_NAMEÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÏÑ∏Ïöî\n",
    "- Í∞ÅÏûê Í∞úÏù∏ Í≥ÑÏ†ïÏúºÎ°ú Î°úÍ∑∏Ïù∏Ìï¥ÏÑú Ïã§ÌóòÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî\n",
    "\"\"\"\n",
    "\n",
    "# WandB Î°úÍ∑∏Ïù∏ (Í∞ÅÏûê Ïã§Ìñâ)\n",
    "try:\n",
    "    if wandb.api.api_key is None:\n",
    "        print(\"WandBÏóê Î°úÍ∑∏Ïù∏Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.\")\n",
    "        wandb.login()\n",
    "    else:\n",
    "        print(f\"WandB Î°úÍ∑∏Ïù∏ ÏÉÅÌÉú: {wandb.api.viewer()['username']}\")\n",
    "except:\n",
    "    print(\"WandB Î°úÍ∑∏Ïù∏ÏùÑ ÏßÑÌñâÌï©ÎãàÎã§...\")\n",
    "    wandb.login()\n",
    "\n",
    "# ÌîÑÎ°úÏ†ùÌä∏ ÏÑ§Ï†ï (Í∞ÅÏûê ÏàòÏ†ïÌï† Î∂ÄÎ∂Ñ)\n",
    "PROJECT_NAME = \"document-classification-team-CV\"  # Î™®Îì† ÌåÄÏõê ÎèôÏùº\n",
    "ENTITY = None  # Í∞ÅÏûê Í∞úÏù∏ Í≥ÑÏ†ï ÏÇ¨Ïö©\n",
    "EXPERIMENT_NAME = \"efficientnet-b3-baseline\"  # ÌåÄÏõêÎ≥ÑÎ°ú Î≥ÄÍ≤Ω (Ïòà: \"member1-hyperopt\", \"member2-augmentation\")\n",
    "\n",
    "print(f\"ÌîÑÎ°úÏ†ùÌä∏: {PROJECT_NAME}\")\n",
    "print(f\"Ïã§ÌóòÎ™Ö: {EXPERIMENT_NAME}\")\n",
    "print(\"ÌåÄÏõêÎì§ÏùÄ EXPERIMENT_NAMEÏùÑ Í∞ÅÏûê Îã§Î•¥Í≤å Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. Seed & basic augmentations (Mixup)\n",
    "# =============================================================================\n",
    "\n",
    "# ÏãúÎìúÎ•º Í≥†Ï†ïÌï©ÎãàÎã§.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. Dataset Class\n",
    "# =============================================================================\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transform=None, problem_class_transform=None):\n",
    "        # CSV ÌååÏùºÏù¥Î©¥ ÏùΩÍ≥†, DataFrameÏù¥Î©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values  # DataFrameÏùÑ numpy arrayÎ°ú Î≥ÄÌôò\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.problem_class_transform = problem_class_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        \n",
    "        # Í∏∞Î≥∏ transform Ï†ÅÏö©\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        \n",
    "        # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌï¥ÏÑúÎßå Ï∂îÍ∞Ä augmentation Ï†ÅÏö©\n",
    "        if self.problem_class_transform:\n",
    "            \n",
    "            # tensorÎ•º Îã§Ïãú numpyÎ°ú Î≥ÄÌôò (Ï∂îÍ∞Ä augmentationÏùÑ ÏúÑÌï¥)\n",
    "            img_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "            # Ï†ïÍ∑úÌôî Ìï¥Ï†ú\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img_np = (img_np * std + mean) * 255\n",
    "            img_np = np.clip(img_np, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Ï∂îÍ∞Ä augmentation Ï†ÅÏö© (Í≤∞Í≥ºÎäî numpy array, H,W,C ÌòïÌÉú)\n",
    "            extra_aug = self.problem_class_transform(image=img_np)['image']\n",
    "            \n",
    "            # numpy arrayÎ•º torch tensorÎ°ú Î≥ÄÌôòÌïòÍ≥† Ï∞®Ïõê ÏàúÏÑú Î≥ÄÍ≤Ω (H,W,C ‚Üí C,H,W)\n",
    "            extra_aug = torch.from_numpy(extra_aug).float()\n",
    "            extra_aug = extra_aug.permute(2, 0, 1)  # (H,W,C) ‚Üí (C,H,W)Î°ú Î≥ÄÍ≤Ω\n",
    "            \n",
    "            # Ï†ïÍ∑úÌôî Ï†ÅÏö© (Ïù¥Ï†ú Ï∞®ÏõêÏù¥ ÎßûÏùå)\n",
    "            extra_aug = extra_aug / 255.0\n",
    "            extra_aug = (extra_aug - torch.tensor(mean).view(3, 1, 1)) / torch.tensor(std).view(3, 1, 1)\n",
    "            img = extra_aug\n",
    "            \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cacecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Cutout (Random Erasing) Ìï®Ïàò Ï†ïÏùò\n",
    "def random_erasing(image, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)):\n",
    "    if random.random() > p:\n",
    "        return image\n",
    "    img_c, img_h, img_w = image.shape[1], image.shape[2], image.shape[3]\n",
    "    area = img_h * img_w\n",
    "    \n",
    "    target_area = random.uniform(scale[0], scale[1]) * area\n",
    "    aspect_ratio = random.uniform(ratio[0], ratio[1])\n",
    "    h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "    w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "    \n",
    "    if h < img_h and w < img_w:\n",
    "        x = random.randint(0, img_w - w)\n",
    "        y = random.randint(0, img_h - h)\n",
    "        image[:, :, y:y+h, x:x+w] = 0.0  # Ï†úÍ±∞Îêú ÏòÅÏó≠ÏùÑ 0ÏúºÎ°ú ÏÑ§Ï†ï\n",
    "    return image\n",
    "\n",
    "# RandomCrop Ìï®Ïàò Ï†ïÏùò\n",
    "def random_crop(image, crop_size=0.8):\n",
    "    img_c, img_h, img_w = image.shape[1], image.shape[2], image.shape[3]\n",
    "    crop_h = int(img_h * crop_size)\n",
    "    crop_w = int(img_w * crop_size)\n",
    "    \n",
    "    if crop_h >= img_h or crop_w >= img_w:\n",
    "        return image\n",
    "    \n",
    "    x = random.randint(0, img_w - crop_w)\n",
    "    y = random.randint(0, img_h - crop_h)\n",
    "    cropped_image = image[:, :, y:y+crop_h, x:x+crop_w]\n",
    "    \n",
    "    # ÏõêÎûò Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î°ú Î≥µÏõê (Ìå®Îî© ÎòêÎäî Î¶¨ÏÇ¨Ïù¥Ï¶à)\n",
    "    cropped_image = torch.nn.functional.interpolate(cropped_image, size=(img_h, img_w), mode='bilinear', align_corners=False)\n",
    "    return cropped_image\n",
    "\n",
    "# Mixup Ìï®Ïàò Ï†ïÏùò\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, epoch=None, fold=None):\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Training Epoch {epoch+1 if epoch else '?'}\")\n",
    "    batch_count = 0\n",
    "    \n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Ï¶ùÍ∞ï Í∏∞Î≤ï ÏÑ†ÌÉù (Mixup 25%, Cutout 25%, RandomCrop 50%)\n",
    "        aug_type = random.choices(['mixup', 'cutout', 'random_crop'], weights=[0.3, 0.3, 0.3])[0]\n",
    "        mixup_applied = False\n",
    "        cutout_applied = False\n",
    "        random_crop_applied = False\n",
    "        \n",
    "        if aug_type == 'mixup':\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(image, targets, alpha=1.0)\n",
    "            with autocast(): \n",
    "                preds = model(mixed_x)\n",
    "            loss = lam * loss_fn(preds.float(), y_a) + (1 - lam) * loss_fn(preds.float(), y_b)\n",
    "            mixup_applied = True\n",
    "        elif aug_type == 'cutout':\n",
    "            image = random_erasing(image, p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds.float(), targets)  # ‚Üê .float() Ï∂îÍ∞Ä\n",
    "            cutout_applied = True\n",
    "        elif aug_type == 'random_crop':\n",
    "            image = random_crop(image, crop_size=0.8)\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds.float(), targets)  # ‚Üê .float() Ï∂îÍ∞Ä\n",
    "            random_crop_applied = True\n",
    "        else:\n",
    "            with autocast(): \n",
    "                preds = model(image)\n",
    "            loss = loss_fn(preds.float(), targets)  # ‚Üê .float() Ï∂îÍ∞Ä\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        # Î∞∞ÏπòÎ≥Ñ ÏÉÅÏÑ∏ Î°úÍπÖ (100 Î∞∞ÏπòÎßàÎã§)\n",
    "        if batch_count % 100 == 0 and wandb.run is not None:\n",
    "            step = epoch * len(loader) + batch_count if epoch is not None else batch_count\n",
    "            wandb.log({\n",
    "                f\"fold_{fold}/train_batch_loss\": loss.item(),\n",
    "                f\"fold_{fold}/mixup_applied\": int(mixup_applied),\n",
    "                f\"fold_{fold}/cutout_applied\": int(cutout_applied),\n",
    "                f\"fold_{fold}/random_crop_applied\": int(random_crop_applied),\n",
    "                f\"fold_{fold}/batch_step\": step\n",
    "            })\n",
    "        \n",
    "        batch_count += 1\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}, Mixup: {mixup_applied}, Cutout: {cutout_applied}, RandomCrop: {random_crop_applied}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def validate_one_epoch(loader, model, loss_fn, device, epoch=None, fold=None, log_confusion=False):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=f\"Validating Epoch {epoch+1 if epoch else '?'}\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "            \n",
    "            pbar.set_description(f\"Val Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "    \n",
    "    # üéØ Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Îì§Ïùò ÏÑ±Îä• Î≥ÑÎèÑ Í≥ÑÏÇ∞ Î∞è Ï∂îÏ†Å\n",
    "    target_classes = [3, 7, 14]\n",
    "    \n",
    "    # Ï†ÑÏ≤¥ ÌÅ¥ÎûòÏä§Î≥Ñ F1 Ïä§ÏΩîÏñ¥ Í≥ÑÏÇ∞\n",
    "    class_f1_scores = f1_score(targets_list, preds_list, average=None, labels=list(range(17)), zero_division=0)\n",
    "    \n",
    "    # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Îì§Ïùò F1 Ïä§ÏΩîÏñ¥ Ï∂îÏ∂ú\n",
    "    problem_class_f1 = {}\n",
    "    problem_class_performance = []\n",
    "    \n",
    "    for cls in target_classes:\n",
    "        if cls < len(class_f1_scores):\n",
    "            cls_f1 = class_f1_scores[cls]\n",
    "            problem_class_f1[f\"class_{cls}_f1\"] = cls_f1\n",
    "            problem_class_performance.append(cls_f1)\n",
    "            \n",
    "            # ÏΩòÏÜîÏóê Ï∂úÎ†•\n",
    "            print(f\"  Class {cls} F1: {cls_f1:.4f}\")\n",
    "    \n",
    "    # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Îì§Ïùò ÌèâÍ∑† F1 Í≥ÑÏÇ∞\n",
    "    avg_problem_f1 = np.mean(problem_class_performance) if problem_class_performance else 0.0\n",
    "    print(f\"  Problem Classes Avg F1: {avg_problem_f1:.4f}\")\n",
    "    \n",
    "    # WandB Î°úÍπÖ (Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä•)\n",
    "    if wandb.run is not None:\n",
    "        log_dict = {\n",
    "            f\"fold_{fold}/val_loss\": val_loss,\n",
    "            f\"fold_{fold}/val_acc\": val_acc,\n",
    "            f\"fold_{fold}/val_f1\": val_f1,\n",
    "            f\"fold_{fold}/problem_classes_avg_f1\": avg_problem_f1,\n",
    "        }\n",
    "        \n",
    "        # Í∞Å Î¨∏Ï†ú ÌÅ¥ÎûòÏä§Î≥Ñ F1 Ïä§ÏΩîÏñ¥ Î°úÍπÖ\n",
    "        for cls in target_classes:\n",
    "            if cls < len(class_f1_scores):\n",
    "                log_dict[f\"fold_{fold}/class_{cls}_f1\"] = class_f1_scores[cls]\n",
    "        \n",
    "        wandb.log(log_dict)\n",
    "    \n",
    "    # Confusion Matrix Î°úÍπÖ (ÎßàÏßÄÎßâ epochÏóêÎßå)\n",
    "    if log_confusion and wandb.run is not None:\n",
    "        try:\n",
    "            wandb.log({\n",
    "                f\"fold_{fold}/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=targets_list,\n",
    "                    preds=preds_list,\n",
    "                    class_names=[f\"Class_{i}\" for i in range(17)]\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            # Ï†ÑÏ≤¥ ÌÅ¥ÎûòÏä§Î≥Ñ F1 Ïä§ÏΩîÏñ¥ Î°úÍπÖ\n",
    "            for i, class_f1 in enumerate(class_f1_scores):\n",
    "                wandb.log({f\"fold_{fold}/all_class_{i}_f1\": class_f1})\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Confusion matrix Î°úÍπÖ Ïã§Ìå®: {e}\")\n",
    "    \n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,  \n",
    "        \"val_f1\": val_f1,\n",
    "        \"problem_class_f1\": problem_class_f1,  # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ F1 Ïä§ÏΩîÏñ¥ Ï∂îÍ∞Ä\n",
    "        \"avg_problem_f1\": avg_problem_f1,      # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÌèâÍ∑† F1 Ï∂îÍ∞Ä\n",
    "    }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193dae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using device: cuda\n",
      " ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï ÏôÑÎ£å!\n",
      " Î™®Îç∏: efficientnet_b3\n",
      " Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞: 384x384\n",
      " Î∞∞Ïπò ÌÅ¨Í∏∞: 64\n",
      " ÌïôÏäµÎ•†: 0.0005\n",
      " ÏóêÌè≠: 50\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. Hyper-parameters with WandB Config\n",
    "# =============================================================================\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "\n",
    "# model config\n",
    "model_name = 'efficientnet_b3' # 'resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "# training config\n",
    "img_size = 384\n",
    "LR = 5e-4\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "num_workers = 30\n",
    "\n",
    "# K-Fold config\n",
    "N_FOLDS = 5  # 5-foldÎ°ú ÏÑ§Ï†ï\n",
    "\n",
    "# WandB Config ÏÑ§Ï†ï\n",
    "config = {\n",
    "    # Model config\n",
    "    \"model_name\": model_name,\n",
    "    \"img_size\": img_size,\n",
    "    \"num_classes\": 17,\n",
    "    \"architecture\": \"EfficientNet-B3\",\n",
    "    \n",
    "    # Training config  \n",
    "    \"lr\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"device\": str(device),\n",
    "    \n",
    "    # K-Fold config\n",
    "    \"n_folds\": N_FOLDS,\n",
    "    \"seed\": SEED,\n",
    "    \"cv_strategy\": \"StratifiedKFold\",\n",
    "    \n",
    "    # Augmentation & Training techniques\n",
    "    \"mixup_alpha\": 1.0,\n",
    "    \"mixup_prob\": 0.3,\n",
    "    \"label_smoothing\": 0.2,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"mixed_precision\": True,\n",
    "    \n",
    "    # Optimizer & Scheduler\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \n",
    "    # Data\n",
    "    \"data_path\": data_path,\n",
    "    \"train_transforms\": \"Advanced\",\n",
    "    \"test_transforms\": \"Basic\",\n",
    "}\n",
    "\n",
    "print(\" ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï ÏôÑÎ£å!\")\n",
    "print(f\" Î™®Îç∏: {model_name}\")\n",
    "print(f\" Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞: {img_size}x{img_size}\")\n",
    "print(f\" Î∞∞Ïπò ÌÅ¨Í∏∞: {BATCH_SIZE}\")\n",
    "print(f\" ÌïôÏäµÎ•†: {LR}\")\n",
    "print(f\" ÏóêÌè≠: {EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e28f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2370869770.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    USE_OPTUNA = \b„Ñπ  # TrueÎ°ú Î∞îÍæ∏Î©¥ ÌäúÎãù Ïã§Ìñâ\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 7. Optuna Hyperparameter Tuning (ÏÑ†ÌÉùÏ†Å)\n",
    "# =============================================================================\n",
    "\n",
    "USE_OPTUNA = False  # TrueÎ°ú Î∞îÍæ∏Î©¥ ÌäúÎãù Ïã§Ìñâ\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    print(\"üîç Optuna ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù ÏãúÏûë...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "        \n",
    "        # WandBÏóê Optuna ÏãúÌñâ Î°úÍπÖ\n",
    "        optuna_run = wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            entity=ENTITY,\n",
    "            name=f\"optuna-trial-{trial.number}\",\n",
    "            config={**config, \"lr\": lr, \"batch_size\": batch_size},\n",
    "            tags=[\"optuna\", \"hyperparameter-tuning\"],\n",
    "            group=\"optuna-study\",\n",
    "            job_type=\"hyperparameter-optimization\",\n",
    "            reinit=True\n",
    "        )\n",
    "        \n",
    "        # Í∞ÑÎã®Ìïú 3-fold CVÎ°ú Îπ†Î•∏ ÌèâÍ∞Ä\n",
    "        skf_simple = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        fold_scores = []\n",
    "        \n",
    "        # Í∞ÑÎã®Ìïú ÌèâÍ∞Ä Î°úÏßÅ (Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îçî Îã®ÏàúÌôî)\n",
    "        # ... (Optuna Î°úÏßÅÏùÄ Î≥µÏû°ÌïòÎØÄÎ°ú Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÎπÑÌôúÏÑ±Ìôî)\n",
    "        \n",
    "        optuna_run.finish()\n",
    "        return np.random.random()  # placeholder\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    \n",
    "    # ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞ Ï†ÅÏö©\n",
    "    best_params = study.best_params\n",
    "    LR = best_params.get('lr', LR)\n",
    "    BATCH_SIZE = best_params.get('batch_size', BATCH_SIZE)\n",
    "    config.update(best_params)\n",
    "    print(f\"üéØ Optuna ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞: {best_params}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Optuna ÌäúÎãù Í±¥ÎÑàÎõ∞Í∏∞ (USE_OPTUNA=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optuna objective function Ï†ïÏùò ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. Data Transforms\n",
    "# =============================================================================\n",
    "\n",
    "# augmentationÏùÑ ÏúÑÌïú transform ÏΩîÎìú\n",
    "trn_transform = A.Compose([\n",
    "    # ÎπÑÏú® Î≥¥Ï°¥ Î¶¨ÏÇ¨Ïù¥Ïßï (ÌïµÏã¨ Í∞úÏÑ†)\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "    \n",
    "    # Î¨∏ÏÑú ÌäπÌôî ÌöåÏ†Ñ + ÎØ∏ÏÑ∏ ÌöåÏ†Ñ Ï∂îÍ∞Ä\n",
    "    A.OneOf([\n",
    "        A.Rotate(limit=[90,90], p=1.0),\n",
    "        A.Rotate(limit=[180,180], p=1.0),\n",
    "        A.Rotate(limit=[270,270], p=1.0),\n",
    "        A.Rotate(limit=(-15, 15), p=1.0),  # ÎØ∏ÏÑ∏ ÌöåÏ†Ñ Ï∂îÍ∞Ä\n",
    "    ], p=0.7),\n",
    "    \n",
    "    # Í∏∞ÌïòÌïôÏ†Å Î≥ÄÌôò Í∞ïÌôî\n",
    "    A.OneOf([\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=5, p=1.0),\n",
    "        A.ElasticTransform(alpha=50, sigma=5, p=1.0),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.2, p=1.0),\n",
    "        A.OpticalDistortion(distort_limit=0.2, shift_limit=0.1, p=1.0),\n",
    "    ], p=0.6),\n",
    "    \n",
    "    # ÏÉâÏÉÅ Î∞è Ï°∞Î™Ö Î≥ÄÌôò Í∞ïÌôî\n",
    "    A.OneOf([\n",
    "        A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1, p=1.0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=1.0),\n",
    "        A.RandomGamma(gamma_limit=(70, 130), p=1.0),\n",
    "    ], p=0.9),\n",
    "    \n",
    "    # Î∏îÎü¨ Î∞è ÎÖ∏Ïù¥Ï¶à Í∞ïÌôî\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=(5, 15), p=1.0),\n",
    "        A.GaussianBlur(blur_limit=(3, 15), p=1.0),\n",
    "        A.MedianBlur(blur_limit=7, p=1.0),\n",
    "        A.Blur(blur_limit=7, p=1.0),\n",
    "    ], p=0.8),\n",
    "    \n",
    "    # Îã§ÏñëÌïú ÎÖ∏Ïù¥Ï¶à Ï∂îÍ∞Ä\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 150.0), p=1.0),\n",
    "        A.ISONoise(color_shift=(0.01, 0.08), intensity=(0.1, 0.8), p=1.0),\n",
    "        A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n",
    "    ], p=0.8),\n",
    "    \n",
    "    # Î¨∏ÏÑú ÌíàÏßà ÏãúÎÆ¨Î†àÏù¥ÏÖò (Ïä§Ï∫î/Î≥µÏÇ¨ Ìö®Í≥º)\n",
    "    A.OneOf([\n",
    "        A.Downscale(scale_min=0.7, scale_max=0.9, p=1.0),\n",
    "        A.ImageCompression(quality_lower=60, quality_upper=95, p=1.0),\n",
    "        A.Posterize(num_bits=6, p=1.0),\n",
    "    ], p=0.5),\n",
    "    \n",
    "    # ÌîΩÏÖÄ Î†àÎ≤® Î≥ÄÌôò\n",
    "    A.OneOf([\n",
    "        A.ChannelShuffle(p=1.0),\n",
    "        A.InvertImg(p=1.0),\n",
    "        A.Solarize(threshold=128, p=1.0),\n",
    "        A.Equalize(p=1.0),\n",
    "    ], p=0.3),\n",
    "    \n",
    "    # Í≥µÍ∞Ñ Î≥ÄÌôò\n",
    "    A.OneOf([\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),  # Î¨∏ÏÑúÏóêÏÑúÎèÑ Ïú†Ïö©Ìï† Ïàò ÏûàÏùå\n",
    "        A.Transpose(p=1.0),\n",
    "    ], p=0.6),\n",
    "    \n",
    "    # Ï°∞Í∞Å Ï†úÍ±∞ (Cutout Í≥ÑÏó¥)\n",
    "    A.OneOf([\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, \n",
    "                       min_holes=1, min_height=8, min_width=8, \n",
    "                       fill_value=0, p=1.0),\n",
    "        A.GridDropout(ratio=0.3, unit_size_min=8, unit_size_max=32, \n",
    "                     holes_number_x=5, holes_number_y=5, p=1.0),\n",
    "    ], p=0.4),\n",
    "    \n",
    "    # ÏµúÏ¢Ö Ï†ïÍ∑úÌôî\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Î¨∏Ï†ú ÌÅ¥ÎûòÏä§(3, 7, 14) Ï†ÑÏö© Ï∂îÍ∞Ä augmentation\n",
    "problem_class_extra_transform = A.Compose([\n",
    "    # Ï∂îÍ∞Ä ÌöåÏ†Ñ Î≥ÄÌòï (Îçî ÏûêÏ£º Ï†ÅÏö©)\n",
    "    A.OneOf([\n",
    "        A.Rotate(limit=(-20, 20), p=1.0),  # Îçî ÎÑìÏùÄ Î≤îÏúÑ\n",
    "        A.Rotate(limit=(-10, 10), p=1.0),\n",
    "    ], p=0.5),  # 50% ÌôïÎ•†Î°ú Ï∂îÍ∞Ä ÌöåÏ†Ñ\n",
    "    \n",
    "    # Ï∂îÍ∞Ä ÏÉâÏÉÅ Î≥ÄÌòï\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=1.0),\n",
    "    ], p=0.4),\n",
    "    \n",
    "    # Ï∂îÍ∞Ä Ïä§ÏºÄÏùº Î≥ÄÌòï\n",
    "    A.OneOf([\n",
    "        A.RandomScale(scale_limit=0.1, p=1.0),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=3, p=1.0),\n",
    "    ], p=0.3),\n",
    "\n",
    "    # ÌÅ¨Í∏∞Î•º Îã§Ïãú ÎßûÏ∂∞Ï£ºÍ∏∞ (Ï§ëÏöî!)\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "])\n",
    "\n",
    "# test image Î≥ÄÌôòÏùÑ ÏúÑÌïú transform ÏΩîÎìú\n",
    "tst_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, \n",
    "                  border_mode=0, value=0),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ÏÑ§Ï†ï ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f420558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-06 14:59:38,760] A new study created in memory with name: no-name-1e736f5e-ec0e-4b52-b144-6521f8ae3086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Optuna ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù ÏãúÏûë!\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66d30b5d38b460fbac5b9284ca2a324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Trial 0: Testing hyperparameters...\n",
      "   LR: 0.000133\n",
      "   Batch Size: 16\n",
      "   Epochs: 29\n",
      "   Image Size: 448\n",
      "   Focal Alpha: 1.749\n",
      "   Focal Gamma: 1.425\n",
      "   üìä Fold 1/5 ÏãúÏûë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.5391, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:16<00:00,  4.65it/s]\n",
      "Val Loss: 5.5611: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.11it/s]\n",
      "Loss: 4.6797, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.69it/s]\n",
      "Val Loss: 5.2895: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.61it/s]\n",
      "Loss: 3.4219, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.76it/s]\n",
      "Val Loss: 5.2376: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.14it/s]\n",
      "Loss: 3.4141, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.69it/s]\n",
      "Val Loss: 4.1323: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.47it/s]\n",
      "Loss: 4.2188, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.78it/s]\n",
      "Val Loss: 3.8510: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.47it/s]\n",
      "Loss: 3.5293, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.73it/s]\n",
      "Val Loss: 3.5639: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.58it/s]\n",
      "Loss: 2.9648, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.72it/s]\n",
      "Val Loss: 3.1203: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.60it/s]\n",
      "Loss: 2.9355, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.78it/s]\n",
      "Val Loss: 3.3092: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.46it/s]\n",
      "Loss: 4.0820, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.77it/s]\n",
      "Val Loss: 3.2539: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.58it/s]\n",
      "Loss: 2.6426, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.73it/s]\n",
      "Val Loss: 2.7821: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.47it/s]\n",
      "Loss: 2.5547, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.77it/s]\n",
      "Val Loss: 2.1593: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.10it/s]\n",
      "Loss: 3.7070, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.70it/s]\n",
      "Val Loss: 2.6669: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.46it/s]\n",
      "Loss: 3.0332, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.65it/s]\n",
      "Val Loss: 2.3922: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.07it/s]\n",
      "Loss: 2.7363, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:14<00:00,  5.60it/s]\n",
      "Val Loss: 2.2877: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.35it/s]\n",
      "Loss: 2.8594, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.66it/s]\n",
      "Val Loss: 1.9191: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.49it/s]\n",
      "Loss: 2.5215, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.73it/s]\n",
      "Val Loss: 2.5915: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.53it/s]\n",
      "Loss: 2.5762, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.81it/s]\n",
      "Val Loss: 1.6418: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.56it/s]\n",
      "Loss: 4.3594, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.68it/s]\n",
      "Val Loss: 1.4647: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.16it/s]\n",
      "Loss: 3.2070, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.77it/s]\n",
      "Val Loss: 1.4792: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# wandb.finish()\n",
    "# =============================================================================\n",
    "# WandB MailboxClosedError Ìï¥Í≤∞ÏùÑ ÏúÑÌïú Ï†ïÎ¶¨ Î∞è Ïû¨ÏãúÏûë\n",
    "# =============================================================================\n",
    "\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 1. Í∏∞Ï°¥ WandB Îü∞ Í∞ïÏ†ú Ï¢ÖÎ£å\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        print(\"Í∏∞Ï°¥ WandB Îü∞ Ï¢ÖÎ£å Ï§ë...\")\n",
    "        wandb.finish()\n",
    "        time.sleep(2)  # Ï¢ÖÎ£å ÎåÄÍ∏∞\n",
    "except Exception as e:\n",
    "    print(f\"Í∏∞Ï°¥ Îü∞ Ï¢ÖÎ£å Ï§ë ÏóêÎü¨ (Î¨¥Ïãú Í∞ÄÎä•): {e}\")\n",
    "\n",
    "# 2. WandB ÌîÑÎ°úÏÑ∏Ïä§ Ï†ïÎ¶¨\n",
    "try:\n",
    "    # WandB ÎÇ¥Î∂Ä ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî\n",
    "    wandb.teardown()\n",
    "    time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(f\"WandB teardown Ï§ë ÏóêÎü¨ (Î¨¥Ïãú Í∞ÄÎä•): {e}\")\n",
    "\n",
    "# 3. ÌôòÍ≤Ω Î≥ÄÏàò Ïû¨ÏÑ§Ï†ï (ÏÑ†ÌÉùÏÇ¨Ìï≠)\n",
    "os.environ['WANDB_START_METHOD'] = 'thread'  # ÌîÑÎ°úÏÑ∏Ïä§ Ï∂©Îèå Î∞©ÏßÄ\n",
    "\n",
    "# 4. ÏïàÏ†ÑÌïú WandB Ï¥àÍ∏∞Ìôî Ìï®Ïàò\n",
    "def safe_wandb_init(project, name, config, **kwargs):\n",
    "    \"\"\"ÏïàÏ†ÑÌïú WandB Ï¥àÍ∏∞Ìôî\"\"\"\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"WandB Ï¥àÍ∏∞Ìôî ÏãúÎèÑ {attempt + 1}/{max_retries}...\")\n",
    "            \n",
    "            # Í∏∞Ï°¥ Îü∞Ïù¥ ÏûàÎã§Î©¥ Ï¢ÖÎ£å\n",
    "            if wandb.run is not None:\n",
    "                wandb.finish()\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # ÏÉàÎ°úÏö¥ Îü∞ ÏãúÏûë\n",
    "            run = wandb.init(\n",
    "                project=project,\n",
    "                name=name,\n",
    "                config=config,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ WandB Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ!\")\n",
    "            return run\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ÏãúÎèÑ {attempt + 1} Ïã§Ìå®: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"‚è≥ {retry_delay}Ï¥à ÌõÑ Ïû¨ÏãúÎèÑ...\")\n",
    "                time.sleep(retry_delay)\n",
    "                \n",
    "                # WandB ÌîÑÎ°úÏÑ∏Ïä§ Í∞ïÏ†ú Ï†ïÎ¶¨\n",
    "                try:\n",
    "                    wandb.teardown()\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                print(\"‚ùå Î™®Îì† Ïû¨ÏãúÎèÑ Ïã§Ìå®. WandB ÏóÜÏù¥ ÏßÑÌñâÌï©ÎãàÎã§.\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 5. ÏàòÏ†ïÎêú Î©îÏù∏ Ï¥àÍ∏∞Ìôî ÏΩîÎìú\n",
    "print(\"üîß WandB Ïó∞Í≤∞ Î¨∏Ï†ú Ìï¥Í≤∞ Ï§ë...\")\n",
    "\n",
    "# Í∏∞Ï°¥ ÏΩîÎìú ÎåÄÏ≤¥\n",
    "main_run = safe_wandb_init(\n",
    "    project=PROJECT_NAME,\n",
    "    name=f\"{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "    config=config,\n",
    "    entity=ENTITY,\n",
    "    tags=[\"k-fold-cv\", \"ensemble\", model_name, \"baseline\", \"main-experiment\"],\n",
    "    group=\"k-fold-experiment\", \n",
    "    job_type=\"cross-validation\",\n",
    "    notes=f\"{N_FOLDS}-Fold Cross Validation with {model_name}\"\n",
    ")\n",
    "\n",
    "if main_run is not None:\n",
    "    print(f\"\\nüöÄ WandB Ïã§Ìóò ÏãúÏûë!\")\n",
    "    print(f\"üìä ÎåÄÏãúÎ≥¥Îìú: {main_run.url}\")\n",
    "    print(f\"üìã Ïã§ÌóòÎ™Ö: {main_run.name}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WandB ÏóÜÏù¥ Ïã§ÌóòÏùÑ ÏßÑÌñâÌï©ÎãàÎã§.\")\n",
    "    print(\"Í≤∞Í≥ºÎäî ÏΩòÏÜîÍ≥º Î°úÏª¨ ÌååÏùºÎ°úÎßå Ï†ÄÏû•Îê©ÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ÏÑ§Ï†ï ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. Load Data & Start K-Fold Cross Validation with WandB\n",
    "# =============================================================================\n",
    "\n",
    "# Ï†ÑÏ≤¥ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "print(f\"ÌïôÏäµ Îç∞Ïù¥ÌÑ∞: {len(train_df)}Í∞ú ÏÉòÌîå\")\n",
    "\n",
    "# ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÌôïÏù∏\n",
    "class_counts = train_df['target'].value_counts().sort_index()\n",
    "print(f\" ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨: {dict(class_counts)}\")\n",
    "\n",
    "# K-Fold ÏÑ§Ï†ï\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# K-Fold Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏\n",
    "fold_results = []\n",
    "fold_models = []  # Í∞Å foldÏùò ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ÏùÑ Ï†ÄÏû•\n",
    "\n",
    "#  WandB Î©îÏù∏ Ïã§Ìóò ÏãúÏûë\n",
    "main_run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    entity=ENTITY,\n",
    "    name=f\"{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "    config=config,\n",
    "    tags=[\"k-fold-cv\", \"ensemble\", model_name, \"baseline\", \"main-experiment\"],\n",
    "    group=\"k-fold-experiment\",\n",
    "    job_type=\"cross-validation\",\n",
    "    notes=f\"{N_FOLDS}-Fold Cross Validation with {model_name}\",\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ WandB Ïã§Ìóò ÏãúÏûë!\")\n",
    "print(f\"üìä ÎåÄÏãúÎ≥¥Îìú: {main_run.url}\")\n",
    "print(f\"üìã Ïã§ÌóòÎ™Ö: {main_run.name}\")\n",
    "\n",
    "#  Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÎ≥¥ Î°úÍπÖ\n",
    "wandb.log({\n",
    "    \"dataset/total_samples\": len(train_df),\n",
    "    \"dataset/num_classes\": 17,\n",
    "    \"dataset/samples_per_fold\": len(train_df) // N_FOLDS,\n",
    "})\n",
    "\n",
    "# ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî\n",
    "class_dist_data = [[f\"Class_{i}\", count] for i, count in enumerate(class_counts)]\n",
    "wandb.log({\n",
    "    \"dataset/class_distribution\": wandb.plot.bar(\n",
    "        wandb.Table(data=class_dist_data, columns=[\"Class\", \"Count\"]),\n",
    "        \"Class\", \"Count\", \n",
    "        title=\"Training Data Class Distribution\"\n",
    "    )\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üéØ {N_FOLDS}-FOLD CROSS VALIDATION ÏãúÏûë\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c320bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïôÏäµ Îç∞Ïù¥ÌÑ∞: 1570Í∞ú ÏÉòÌîå\n",
      " ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨: {0: 100, 1: 46, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100, 10: 100, 11: 100, 12: 100, 13: 74, 14: 50, 15: 100, 16: 100}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/computervisioncompetition-cv-1/experiments/wandb/run-20250906_144106-pzrkpkjc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc' target=\"_blank\">efficientnet-b3-baseline-0906-1441</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ WandB Ïã§Ìóò ÏãúÏûë!\n",
      "üìä ÎåÄÏãúÎ≥¥Îìú: https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc\n",
      "üìã Ïã§ÌóòÎ™Ö: efficientnet-b3-baseline-0906-1441\n",
      "\n",
      "============================================================\n",
      "üéØ 5-FOLD CROSS VALIDATION ÏãúÏûë\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 10. K-Fold Cross Validation Loop with WandB\n",
    "# =============================================================================\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\" FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Í∞Å foldÎ≥Ñ child run ÏÉùÏÑ±\n",
    "    fold_run = wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        entity=ENTITY,\n",
    "        name=f\"fold-{fold+1}-{model_name}-{datetime.now().strftime('%H%M')}\",\n",
    "        config=config,\n",
    "        tags=[\"fold\", f\"fold-{fold+1}\", model_name, \"child-run\"],\n",
    "        group=\"k-fold-experiment\",\n",
    "        job_type=f\"fold-{fold+1}\",\n",
    "        reinit=True  # ÏÉàÎ°úÏö¥ run ÏãúÏûë ÌóàÏö©\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Fold {fold+1} Dashboard: {fold_run.url}\")\n",
    "    \n",
    "    # ÌòÑÏû¨ foldÏùò train/validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
    "    train_fold_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_fold_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Ï†ïÎ≥¥ Î°úÍπÖ\n",
    "    wandb.log({\n",
    "        \"fold_info/fold_number\": fold + 1,\n",
    "        \"fold_info/train_samples\": len(train_fold_df),\n",
    "        \"fold_info/val_samples\": len(val_fold_df),\n",
    "        \"fold_info/train_ratio\": len(train_fold_df) / len(train_df),\n",
    "        \"fold_info/val_ratio\": len(val_fold_df) / len(train_df)\n",
    "    })\n",
    "    \n",
    "    # ÌòÑÏû¨ foldÏùò Dataset ÏÉùÏÑ±\n",
    "    trn_dataset = ImageDataset(\n",
    "        train_fold_df,\n",
    "        \"../data/train/\",\n",
    "        transform=trn_transform,\n",
    "        problem_class_transform=problem_class_extra_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = ImageDataset(\n",
    "        val_fold_df,\n",
    "        \"../data/train/\",\n",
    "        transform=tst_transform  # Í≤ÄÏ¶ùÏóêÎäî Ï¶ùÍ∞ï Ï†ÅÏö© ÏïàÌï®\n",
    "    )\n",
    "    \n",
    "    # ÌòÑÏû¨ foldÏùò DataLoader ÏÉùÏÑ±\n",
    "    trn_loader = DataLoader(\n",
    "        trn_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(trn_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî (Í∞Å foldÎßàÎã§ ÏÉàÎ°úÏö¥ Î™®Îç∏)\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "    \n",
    "    class_weights = torch.FloatTensor([\n",
    "    4 if i in [3, 7, 14] else 1.0 for i in range(17)]).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.15)\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Learning Rate Scheduler Ï∂îÍ∞Ä\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # ÌòÑÏû¨ foldÏùò ÏµúÍ≥† ÏÑ±Îä• Ï∂îÏ†Å\n",
    "    best_val_f1 = 0.0\n",
    "    best_model = None\n",
    "    patience = 0\n",
    "    max_patience = 7\n",
    "    \n",
    "    print(f\" Î™®Îç∏ ÌïôÏäµ ÏãúÏûë - Fold {fold+1}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 11. Training Loop for Current Fold\n",
    "    # =============================================================================\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nüìà Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        # Training\n",
    "        train_ret = train_one_epoch(\n",
    "            trn_loader, model, optimizer, loss_fn, device, \n",
    "            epoch=epoch, fold=fold+1\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_ret = validate_one_epoch(\n",
    "            val_loader, model, loss_fn, device, \n",
    "            epoch=epoch, fold=fold+1,\n",
    "            log_confusion=(epoch == EPOCHS-1)  # ÎßàÏßÄÎßâ epochÏóêÎßå confusion matrix\n",
    "        )\n",
    "        \n",
    "        # Learning rate Î°úÍπÖ\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä• Í∞úÏÑ† Ïó¨Î∂Ä ÌôïÏù∏\n",
    "        problem_classes_improved = \"\"\n",
    "        if 'avg_problem_f1' in val_ret:\n",
    "            if val_ret['avg_problem_f1'] > best_val_f1 * 0.95:  # Ï†ÑÏ≤¥ ÏÑ±Îä•Ïùò 95% Ïù¥ÏÉÅÏù¥Î©¥ ÏñëÌò∏\n",
    "                problem_classes_improved = \"‚úÖ Problem classes performing well\"\n",
    "            else:\n",
    "                problem_classes_improved = \"‚ö†Ô∏è Problem classes need attention\"\n",
    "\n",
    "\n",
    "        # WandBÏóê metrics Î°úÍπÖ\n",
    "        log_data = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"fold\": fold + 1,\n",
    "            \"train/loss\": train_ret['train_loss'],\n",
    "            \"train/accuracy\": train_ret['train_acc'], \n",
    "            \"train/f1\": train_ret['train_f1'],\n",
    "            \"val/loss\": val_ret['val_loss'],\n",
    "            \"val/accuracy\": val_ret['val_acc'],\n",
    "            \"val/f1\": val_ret['val_f1'],\n",
    "            \"learning_rate\": current_lr,\n",
    "            \"optimizer/lr\": current_lr\n",
    "        }\n",
    "        \n",
    "        # GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Î°úÍπÖ\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated(0) / 1e9\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            log_data.update({\n",
    "                \"system/gpu_memory_used_gb\": gpu_memory_used,\n",
    "                \"system/gpu_memory_total_gb\": gpu_memory_total,\n",
    "                \"system/gpu_utilization_pct\": (gpu_memory_used / gpu_memory_total) * 100\n",
    "            })\n",
    "\n",
    "        if 'avg_problem_f1' in val_ret:\n",
    "            log_data[\"val/problem_classes_avg_f1\"] = val_ret['avg_problem_f1']\n",
    "        \n",
    "        # Í∞úÎ≥Ñ Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä•\n",
    "        for cls in [3, 7, 14]:\n",
    "            if f\"class_{cls}_f1\" in val_ret['problem_class_f1']:\n",
    "                log_data[f\"val/class_{cls}_f1\"] = val_ret['problem_class_f1'][f\"class_{cls}_f1\"]\n",
    "    \n",
    "        \n",
    "        wandb.log(log_data)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\" Epoch {epoch+1:2d} | \"\n",
    "              f\"Train Loss: {train_ret['train_loss']:.4f} | \"\n",
    "              f\"Train F1: {train_ret['train_f1']:.4f} | \"\n",
    "              f\"Val Loss: {val_ret['val_loss']:.4f} | \"\n",
    "              f\"Val F1: {val_ret['val_f1']:.4f} | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä• Î≥ÑÎèÑ Ï∂úÎ†•\n",
    "        if 'avg_problem_f1' in val_ret:\n",
    "            print(f\"         Problem Classes (3,7,14) Avg F1: {val_ret['avg_problem_f1']:.4f} | {problem_classes_improved}\")\n",
    "\n",
    "        # ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ Ï†ÄÏû• (Ï†ÑÏ≤¥ F1Í≥º Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä• Î™®Îëê Í≥†Î†§)\n",
    "        current_val_f1 = val_ret['val_f1']\n",
    "        problem_class_bonus = 0\n",
    "    \n",
    "        # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä•Ïù¥ Ï¢ãÏúºÎ©¥ Î≥¥ÎÑàÏä§ Ï†êÏàò Î∂ÄÏó¨\n",
    "        if 'avg_problem_f1' in val_ret and val_ret['avg_problem_f1'] > 0.90:\n",
    "            problem_class_bonus = 0.001  # ÏûëÏùÄ Î≥¥ÎÑàÏä§Î°ú ÎèôÏ†êÏùº Îïå Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä• Ïö∞ÏÑ†\n",
    "        \n",
    "        adjusted_f1 = current_val_f1 + problem_class_bonus\n",
    "        \n",
    "        if adjusted_f1 > best_val_f1:\n",
    "            best_val_f1 = current_val_f1  # Ïã§Ï†ú F1 Ïä§ÏΩîÏñ¥Î°ú Ï†ÄÏû•\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            patience = 0\n",
    "            \n",
    "            # ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ ÏïÑÌã∞Ìå©Ìä∏Î°ú Ï†ÄÏû•\n",
    "            model_path = f'best_model_fold_{fold+1}.pth'\n",
    "            torch.save(best_model, model_path)\n",
    "            wandb.save(model_path, policy=\"now\")\n",
    "            \n",
    "            # ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä• Î°úÍπÖ\n",
    "            best_performance_log = {\n",
    "                f\"best_performance/epoch\": epoch + 1,\n",
    "                f\"best_performance/val_f1\": best_val_f1,\n",
    "                f\"best_performance/val_acc\": val_ret['val_acc'],\n",
    "                f\"best_performance/val_loss\": val_ret['val_loss'],\n",
    "            }\n",
    "            \n",
    "            # Î¨∏Ï†ú ÌÅ¥ÎûòÏä§ ÏÑ±Îä•ÎèÑ ÏµúÍ≥† ÏÑ±Îä•Ïóê Ìè¨Ìï®\n",
    "            if 'avg_problem_f1' in val_ret:\n",
    "                best_performance_log[f\"best_performance/problem_classes_f1\"] = val_ret['avg_problem_f1']\n",
    "            \n",
    "            wandb.log(best_performance_log)\n",
    "            \n",
    "            improvement_msg = f\"üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: {best_val_f1:.4f}\"\n",
    "            if 'avg_problem_f1' in val_ret:\n",
    "                improvement_msg += f\" (Problem Classes: {val_ret['avg_problem_f1']:.4f})\"\n",
    "            print(improvement_msg)\n",
    "        else:\n",
    "            patience += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if patience >= max_patience and epoch > EPOCHS // 2:\n",
    "            print(f\"‚è∏Ô∏è Early stopping at epoch {epoch+1} (patience: {patience})\")\n",
    "            wandb.log({\"early_stopping/epoch\": epoch + 1})\n",
    "            break\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 12. Fold Results Summary\n",
    "    # =============================================================================\n",
    "    \n",
    "    # ÌòÑÏû¨ fold Í≤∞Í≥º Ï†ÄÏû•\n",
    "    fold_result = {\n",
    "        'fold': fold + 1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'final_train_f1': train_ret['train_f1'],\n",
    "        'train_samples': len(trn_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'epochs_trained': epoch + 1,\n",
    "        'early_stopped': patience >= max_patience\n",
    "    }\n",
    "    \n",
    "    fold_results.append(fold_result)\n",
    "    fold_models.append(best_model)\n",
    "    \n",
    "    # Fold ÏµúÏ¢Ö ÏöîÏïΩ Î°úÍπÖ\n",
    "    wandb.log({\n",
    "        \"fold_summary/best_val_f1\": best_val_f1,\n",
    "        \"fold_summary/final_train_f1\": train_ret['train_f1'],\n",
    "        \"fold_summary/epochs_trained\": epoch + 1,\n",
    "        \"fold_summary/improvement\": best_val_f1 - val_ret['val_f1'],\n",
    "        \"fold_summary/early_stopped\": patience >= max_patience\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n Fold {fold + 1} ÏôÑÎ£å!\")\n",
    "    print(f\" ÏµúÍ≥† Validation F1: {best_val_f1:.4f}\")\n",
    "    print(f\" ÌïôÏäµÎêú ÏóêÌè≠: {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Fold run Ï¢ÖÎ£å\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "    del model, optimizer, scheduler, trn_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " FOLD 1/5\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dataset/num_classes</td><td>‚ñÅ</td></tr><tr><td>dataset/samples_per_fold</td><td>‚ñÅ</td></tr><tr><td>dataset/total_samples</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dataset/num_classes</td><td>17</td></tr><tr><td>dataset/samples_per_fold</td><td>314</td></tr><tr><td>dataset/total_samples</td><td>1570</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficientnet-b3-baseline-0906-1441</strong> at: <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/pzrkpkjc</a><br> View project at: <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250906_144106-pzrkpkjc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/computervisioncompetition-cv-1/experiments/wandb/run-20250906_144107-2tjtce0l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/2tjtce0l' target=\"_blank\">fold-1-efficientnet_b3-1441</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/2tjtce0l' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/2tjtce0l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Fold 1 Dashboard: https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/2tjtce0l\n",
      "Train samples: 1256, Validation samples: 314\n",
      " Î™®Îç∏ ÌïôÏäµ ÏãúÏûë - Fold 1\n",
      "\n",
      "üìà Epoch 1/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6.0430, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:20<00:00,  3.95it/s]\n",
      "Val Loss: 6.6661: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  1 | Train Loss: 5.5644 | Train F1: 0.0499 | Val Loss: 5.3148 | Val F1: 0.0848 | LR: 1.33e-04\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.0848\n",
      "\n",
      "üìà Epoch 2/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.7266, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:14<00:00,  5.63it/s]\n",
      "Val Loss: 6.4441: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  2 | Train Loss: 5.3612 | Train F1: 0.0506 | Val Loss: 4.8685 | Val F1: 0.1196 | LR: 1.33e-04\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.1196\n",
      "\n",
      "üìà Epoch 3/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.8223, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.66it/s]\n",
      "Val Loss: 6.2088: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  3 | Train Loss: 5.0298 | Train F1: 0.0837 | Val Loss: 4.8058 | Val F1: 0.1404 | LR: 1.31e-04\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.1404\n",
      "\n",
      "üìà Epoch 4/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.5508, Mixup: False, Cutout: True, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.64it/s]\n",
      "Val Loss: 5.8374: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  4 | Train Loss: 4.8693 | Train F1: 0.0790 | Val Loss: 4.6522 | Val F1: 0.1316 | LR: 1.29e-04\n",
      "\n",
      "üìà Epoch 5/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.6504, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.69it/s]\n",
      "Val Loss: 5.1404: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  5 | Train Loss: 4.5866 | Train F1: 0.1068 | Val Loss: 4.3914 | Val F1: 0.1477 | LR: 1.27e-04\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.1477\n",
      "\n",
      "üìà Epoch 6/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.1328, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.70it/s]\n",
      "Val Loss: 5.1456: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  6 | Train Loss: 4.4928 | Train F1: 0.1131 | Val Loss: 4.3188 | Val F1: 0.1324 | LR: 1.24e-04\n",
      "\n",
      "üìà Epoch 7/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.0547, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.69it/s]\n",
      "Val Loss: 4.6002: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  7 | Train Loss: 4.3326 | Train F1: 0.1251 | Val Loss: 4.0417 | Val F1: 0.1553 | LR: 1.20e-04\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.1553\n",
      "\n",
      "üìà Epoch 8/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.8828, Mixup: False, Cutout: False, RandomCrop: True: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.67it/s]\n",
      "Val Loss: 4.5807: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  8 | Train Loss: 4.2297 | Train F1: 0.1406 | Val Loss: 4.2853 | Val F1: 0.1144 | LR: 1.15e-04\n",
      "\n",
      "üìà Epoch 9/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.4297, Mixup: True, Cutout: False, RandomCrop: False: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:13<00:00,  5.70it/s]\n",
      "Val Loss: 4.1792: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  9 | Train Loss: 4.2773 | Train F1: 0.1368 | Val Loss: 3.7489 | Val F1: 0.1984 | LR: 1.10e-04\n",
      "üéâ ÏÉàÎ°úÏö¥ ÏµúÍ≥† ÏÑ±Îä•! F1: 0.1984\n",
      "\n",
      "üìà Epoch 10/29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.8477, Mixup: True, Cutout: False, RandomCrop: False:   4%|‚ñç         | 3/79 [00:06<02:50,  2.25s/it]\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìà Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m train_ret \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrn_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m    108\u001b[0m val_ret \u001b[38;5;241m=\u001b[39m validate_one_epoch(\n\u001b[1;32m    109\u001b[0m     val_loader, model, loss_fn, device, \n\u001b[1;32m    110\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch, fold\u001b[38;5;241m=\u001b[39mfold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    111\u001b[0m     log_confusion\u001b[38;5;241m=\u001b[39m(epoch \u001b[38;5;241m==\u001b[39m EPOCHS\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# ÎßàÏßÄÎßâ epochÏóêÎßå confusion matrix\u001b[39;00m\n\u001b[1;32m    112\u001b[0m )\n",
      "Cell \u001b[0;32mIn[24], line 129\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, optimizer, loss_fn, device, epoch, fold)\u001b[0m\n\u001b[1;32m    127\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    128\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    132\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:315\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 315\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:565\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    563\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    568\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 13. K-Fold Cross Validation Results Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" K-FOLD CROSS VALIDATION ÏµúÏ¢Ö Í≤∞Í≥º\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "val_f1_scores = [result['best_val_f1'] for result in fold_results]\n",
    "mean_f1 = np.mean(val_f1_scores)\n",
    "std_f1 = np.std(val_f1_scores)\n",
    "\n",
    "try:\n",
    "    # wandb.runÏù¥ ÌòÑÏû¨ ÌôúÏÑ±ÌôîÎêú runÏùÑ Í∞ÄÎ¶¨ÌÇ¥\n",
    "    if wandb.run is None:\n",
    "        print(\" ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ ÏÉàÎ°úÏö¥ summary runÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\")\n",
    "        active_run = wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            name=f\"SUMMARY-{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "            config=config,\n",
    "            tags=[\"summary\", \"cv-results\", model_name],\n",
    "            group=\"k-fold-experiment\",\n",
    "            job_type=\"summary\",\n",
    "            reinit=True\n",
    "        )\n",
    "    else:\n",
    "        print(\" Í∏∞Ï°¥ runÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "        active_run = wandb.run\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Run ÏÉÅÌÉú ÌôïÏù∏ Ï§ë ÏóêÎü¨: {e}\")\n",
    "    # ÏÉàÎ°úÏö¥ run ÏÉùÏÑ±\n",
    "    active_run = wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        name=f\"SUMMARY-{EXPERIMENT_NAME}-{datetime.now().strftime('%m%d-%H%M')}\",\n",
    "        config=config,\n",
    "        tags=[\"summary\", \"cv-results\", model_name],\n",
    "        group=\"k-fold-experiment\",\n",
    "        job_type=\"summary\",\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "# CV ÏöîÏïΩ ÌÖåÏù¥Î∏î ÏÉùÏÑ±\n",
    "fold_table = wandb.Table(columns=[\n",
    "    \"Fold\", \"Best_Val_F1\", \"Final_Train_F1\", \"Train_Samples\", \n",
    "    \"Val_Samples\", \"Epochs_Trained\", \"Early_Stopped\"\n",
    "])\n",
    "\n",
    "for result in fold_results:\n",
    "    fold_table.add_data(\n",
    "        result['fold'], \n",
    "        result['best_val_f1'], \n",
    "        result['final_train_f1'],\n",
    "        result['train_samples'], \n",
    "        result['val_samples'],\n",
    "        result['epochs_trained'],\n",
    "        result['early_stopped']\n",
    "    )\n",
    "\n",
    "# ÏïàÏ†ÑÌïú Î°úÍπÖ\n",
    "try:\n",
    "    active_run.log({\n",
    "        \"cv_results/mean_f1\": mean_f1,\n",
    "        \"cv_results/std_f1\": std_f1,\n",
    "        \"cv_results/best_fold_f1\": max(val_f1_scores),\n",
    "        \"cv_results/worst_fold_f1\": min(val_f1_scores),\n",
    "        \"cv_results/f1_range\": max(val_f1_scores) - min(val_f1_scores),\n",
    "        \"cv_results/fold_results_table\": fold_table,\n",
    "        \"cv_results/n_folds\": N_FOLDS,\n",
    "        \"cv_results/total_epochs\": sum([r['epochs_trained'] for r in fold_results]),\n",
    "        \"cv_results/avg_epochs_per_fold\": np.mean([r['epochs_trained'] for r in fold_results]),\n",
    "        \"cv_results/early_stopped_folds\": sum([r['early_stopped'] for r in fold_results])\n",
    "    })\n",
    "    \n",
    "    # FoldÎ≥Ñ ÏÑ±Îä• Î∞îÏ∞®Ìä∏ ÏÉùÏÑ±\n",
    "    fold_performance_data = [[f\"Fold {i+1}\", score] for i, score in enumerate(val_f1_scores)]\n",
    "    active_run.log({\n",
    "        \"cv_results/fold_performance_chart\": wandb.plot.bar(\n",
    "            wandb.Table(data=fold_performance_data, columns=[\"Fold\", \"F1_Score\"]),\n",
    "            \"Fold\", \"F1_Score\", \n",
    "            title=\"K-Fold Cross Validation Performance\"\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    print(\" CV Í≤∞Í≥º Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" WandB Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "    print(\" Í≤∞Í≥ºÎ•º ÏΩòÏÜîÏóê Ï∂úÎ†•Ìï©ÎãàÎã§:\")\n",
    "\n",
    "# Ïñ¥Îñ§ Í≤ΩÏö∞Îì† ÏΩòÏÜîÏóêÎäî Í≤∞Í≥º Ï∂úÎ†•\n",
    "for result in fold_results:\n",
    "    status = \" Early Stopped\" if result['early_stopped'] else \" Completed\"\n",
    "    print(f\"Fold {result['fold']}: {result['best_val_f1']:.4f} \"\n",
    "          f\"({result['epochs_trained']} epochs) {status}\")\n",
    "\n",
    "print(f\"\\n ÌèâÍ∑† CV F1: {mean_f1:.4f} ¬± {std_f1:.4f}\")\n",
    "print(f\" ÏµúÍ≥† Fold: {max(val_f1_scores):.4f}\")\n",
    "print(f\" ÏµúÏïÖ Fold: {min(val_f1_scores):.4f}\")\n",
    "print(f\" ÏÑ±Îä• Î≤îÏúÑ: {max(val_f1_scores) - min(val_f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb258fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 14. Ensemble Models Preparation\n",
    "# =============================================================================\n",
    "\n",
    "# 5-Fold ÏïôÏÉÅÎ∏î Î™®Îç∏ Ï§ÄÎπÑ\n",
    "ensemble_models = []\n",
    "print(f\"\\nüîß ÏïôÏÉÅÎ∏î Î™®Îç∏ Ï§ÄÎπÑ Ï§ë...\")\n",
    "\n",
    "for i, state_dict in enumerate(fold_models):\n",
    "    fold_model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    fold_model.load_state_dict(state_dict)\n",
    "    fold_model.eval()\n",
    "    ensemble_models.append(fold_model)\n",
    "    print(f\"Fold {i+1} Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\")\n",
    "\n",
    "print(f\" Ï¥ù {len(ensemble_models)}Í∞ú Î™®Îç∏Î°ú ÏïôÏÉÅÎ∏î Íµ¨ÏÑ±\")\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"ensemble/num_models\": len(ensemble_models),\n",
    "            \"ensemble/model_architecture\": model_name,\n",
    "            \"ensemble/ensemble_type\": \"simple_average\"\n",
    "        })\n",
    "    else:\n",
    "        print(\"üìä ÏïôÏÉÅÎ∏î Ï†ïÎ≥¥:\")\n",
    "        print(f\"  - Î™®Îç∏ Í∞úÏàò: {len(ensemble_models)}\")\n",
    "        print(f\"  - ÏïÑÌÇ§ÌÖçÏ≤ò: {model_name}\")\n",
    "        print(f\"  - ÏïôÏÉÅÎ∏î ÌÉÄÏûÖ: simple_average\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ÏïôÏÉÅÎ∏î Ï†ïÎ≥¥ Î°úÍπÖ Ïã§Ìå®: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 15. TTA (Test Time Augmentation) Setup\n",
    "# =============================================================================\n",
    "\n",
    "# Temperature Scaling ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self, temperature=1.5):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * temperature)\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "print(f\"\\n TTA (Test Time Augmentation) ÏÑ§Ï†ï...\")\n",
    "\n",
    "# Essential TTA transforms\n",
    "essential_tta_transforms = [\n",
    "    # ÏõêÎ≥∏\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 90ÎèÑ ÌöåÏ†ÑÎì§\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[90, 90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[180, 180], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[-90, -90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # Î∞ùÍ∏∞ Í∞úÏÑ†\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[0.3, 0.3], contrast_limit=[0.3, 0.3], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "print(f\"TTA Î≥ÄÌôò {len(essential_tta_transforms)}Í∞ú Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"tta/num_transforms\": len(essential_tta_transforms),\n",
    "            \"tta/transforms_used\": [\"original\", \"rot_90\", \"rot_180\", \"rot_270\", \"brightness\"],\n",
    "            \"tta/batch_size\": 64  # TTAÏö© Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "        })\n",
    "    else:\n",
    "        print(\"üìä TTA ÏÑ§Ï†ï Ï†ïÎ≥¥:\")\n",
    "        print(f\"  - Î≥ÄÌòï Í∞úÏàò: {len(essential_tta_transforms)}\")\n",
    "        print(f\"  - Î≥ÄÌòï Ï¢ÖÎ•ò: original, rot_90, rot_180, rot_270, brightness\")\n",
    "        print(f\"  - Î∞∞Ïπò ÌÅ¨Í∏∞: 64\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è TTA ÏÑ§Ï†ï Î°úÍπÖ Ïã§Ìå®: {e}\")\n",
    "    print(\"üìä TTA ÏÑ§Ï†ï Ï†ïÎ≥¥:\")\n",
    "    print(f\"  - Î≥ÄÌòï Í∞úÏàò: {len(essential_tta_transforms)}\")\n",
    "    print(f\"  - Î∞∞Ïπò ÌÅ¨Í∏∞: 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " K-FOLD CROSS VALIDATION ÏµúÏ¢Ö Í≤∞Í≥º\n",
      "============================================================\n",
      " ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ ÏÉàÎ°úÏö¥ summary runÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/computervisioncompetition-cv-1/experiments/wandb/run-20250906_142625-bh62ochz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/bh62ochz' target=\"_blank\">SUMMARY-efficientnet-b3-baseline-0906-1426</a></strong> to <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/bh62ochz' target=\"_blank\">https://wandb.ai/kimsunmin0227-hufs/document-classification-team/runs/bh62ochz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CV Í≤∞Í≥º Î°úÍπÖ ÏôÑÎ£å!\n",
      "Fold 1: 0.9143 (28 epochs)  Early Stopped\n",
      "Fold 2: 0.9087 (27 epochs)  Early Stopped\n",
      "Fold 3: 0.9319 (36 epochs)  Early Stopped\n",
      "Fold 4: 0.9083 (33 epochs)  Early Stopped\n",
      "Fold 5: 0.9321 (49 epochs)  Early Stopped\n",
      "\n",
      " ÌèâÍ∑† CV F1: 0.9191 ¬± 0.0108\n",
      " ÏµúÍ≥† Fold: 0.9321\n",
      " ÏµúÏïÖ Fold: 0.9083\n",
      " ÏÑ±Îä• Î≤îÏúÑ: 0.0238\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 16. TTA Dataset and DataLoader\n",
    "# =============================================================================\n",
    "\n",
    "class TTAImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms  # Ïó¨Îü¨ transformÏùÑ Î¶¨Ïä§Ìä∏Î°ú Î∞õÏùå\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        \n",
    "        # Î™®Îì† transformÏùÑ Ï†ÅÏö©Ìïú Í≤∞Í≥ºÎ•º Î¶¨Ïä§Ìä∏Î°ú Î∞òÌôò\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            aug_img = transform(image=img)['image']\n",
    "            augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target\n",
    "\n",
    "# TTA Dataset ÏÉùÏÑ±\n",
    "tta_dataset = TTAImageDataset(\n",
    "    \"../data/sample_submission.csv\",\n",
    "    \"../data/test/\",\n",
    "    essential_tta_transforms\n",
    ")\n",
    "\n",
    "# TTA DataLoader (Î∞∞Ïπò ÌÅ¨Í∏∞Î•º Ï§ÑÏó¨ÏÑú Î©îÎ™®Î¶¨ Ï†àÏïΩ)\n",
    "tta_loader = DataLoader(\n",
    "    tta_dataset,\n",
    "    batch_size=64,  # TTAÎäî Î©îÎ™®Î¶¨Î•º ÎßéÏù¥ ÏÇ¨Ïö©ÌïòÎØÄÎ°ú Î∞∞Ïπò ÌÅ¨Í∏∞ Ï§ÑÏûÑ\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\" TTA Dataset: {len(tta_dataset)}Í∞ú ÌÖåÏä§Ìä∏ ÏÉòÌîå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß ÏïôÏÉÅÎ∏î Î™®Îç∏ Ï§ÄÎπÑ Ï§ë...\n",
      "Fold 1 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "Fold 2 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "Fold 3 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "Fold 4 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "Fold 5 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      " Ï¥ù 5Í∞ú Î™®Îç∏Î°ú ÏïôÏÉÅÎ∏î Íµ¨ÏÑ±\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 17. Ensemble + TTA Inference with WandB Logging\n",
    "# =============================================================================\n",
    "\n",
    "def ensemble_tta_inference_with_logging(models, loader, transforms, confidence_threshold=0.9):\n",
    "    \"\"\"5-Fold Î™®Îç∏ ÏïôÏÉÅÎ∏î + TTA Ï∂îÎ°† with WandB Î°úÍπÖ\"\"\"\n",
    "    all_predictions = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    # TTA ÏßÑÌñâÏÉÅÌô© Î°úÍπÖÏùÑ ÏúÑÌïú ÌÖåÏù¥Î∏î\n",
    "    tta_progress = wandb.Table(columns=[\"Batch\", \"Avg_Confidence\", \"Low_Conf_Count\", \"High_Conf_Count\"])\n",
    "    \n",
    "    # Temperature scaling Ï¥àÍ∏∞Ìôî\n",
    "    temp_scaling = TemperatureScaling().to(device)\n",
    "    \n",
    "    print(f\"ÏïôÏÉÅÎ∏î TTA Ï∂îÎ°† ÏãúÏûë...\")\n",
    "    print(f\"{len(models)}Í∞ú Î™®Îç∏ √ó {len(transforms)}Í∞ú TTA Î≥ÄÌòï = {len(models) * len(transforms)}Í∞ú ÏòàÏ∏° ÌèâÍ∑†\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Ensemble TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        ensemble_probs = torch.zeros(batch_size, 17).to(device)\n",
    "        \n",
    "        # Í∞Å fold Î™®Îç∏Î≥Ñ ÏòàÏ∏°\n",
    "        for model_idx, model in enumerate(models):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Í∞Å TTA Î≥ÄÌòïÎ≥Ñ ÏòàÏ∏°\n",
    "                for tta_idx, images in enumerate(images_list):\n",
    "                    images = images.to(device)\n",
    "                    preds = model(images)\n",
    "                    \n",
    "                    # Temperature scaling Ï†ÅÏö©\n",
    "                    preds = temp_scaling(preds)\n",
    "                    probs = torch.softmax(preds, dim=1)\n",
    "                    \n",
    "                    # ÏïôÏÉÅÎ∏î ÌôïÎ•†Ïóê ÎàÑÏ†Å (ÌèâÍ∑†)\n",
    "                    ensemble_probs += probs / (len(models) * len(images_list))\n",
    "        \n",
    "        # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞\n",
    "        max_probs = torch.max(ensemble_probs, dim=1)[0]\n",
    "        batch_confidences = max_probs.cpu().numpy()\n",
    "        all_confidences.extend(batch_confidences)\n",
    "        \n",
    "        final_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "        all_predictions.extend(final_preds.cpu().numpy())\n",
    "        \n",
    "        # Î∞∞ÏπòÎ≥Ñ Ïã†Î¢∞ÎèÑ Î∂ÑÏÑù\n",
    "        high_conf_count = np.sum(batch_confidences >= confidence_threshold)\n",
    "        low_conf_count = batch_size - high_conf_count\n",
    "        avg_confidence = np.mean(batch_confidences)\n",
    "        \n",
    "        # ÏßÑÌñâÏÉÅÌô© ÌÖåÏù¥Î∏îÏóê Ï∂îÍ∞Ä\n",
    "        tta_progress.add_data(batch_idx, avg_confidence, low_conf_count, high_conf_count)\n",
    "        \n",
    "        # Î∞∞ÏπòÎ≥Ñ ÏÉÅÏÑ∏ Î°úÍπÖ (20Î∞∞ÏπòÎßàÎã§)\n",
    "        if batch_idx % 20 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            estimated_total = elapsed_time * len(loader) / (batch_idx + 1)\n",
    "            remaining_time = estimated_total - elapsed_time\n",
    "            \n",
    "            wandb.log({\n",
    "                \"tta_progress/batch\": batch_idx,\n",
    "                \"tta_progress/avg_confidence\": avg_confidence,\n",
    "                \"tta_progress/high_confidence_ratio\": high_conf_count / batch_size,\n",
    "                \"tta_progress/low_confidence_count\": low_conf_count,\n",
    "                \"tta_progress/elapsed_time_min\": elapsed_time / 60,\n",
    "                \"tta_progress/estimated_remaining_min\": remaining_time / 60,\n",
    "                \"tta_progress/samples_processed\": (batch_idx + 1) * batch_size,\n",
    "            })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # TTA ÏµúÏ¢Ö Í≤∞Í≥º Î°úÍπÖ\n",
    "    final_avg_confidence = np.mean(all_confidences)\n",
    "    confidence_std = np.std(all_confidences)\n",
    "    high_conf_samples = np.sum(np.array(all_confidences) >= confidence_threshold)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"tta_results/total_time_min\": total_time / 60,\n",
    "        \"tta_results/samples_per_second\": len(all_predictions) / total_time,\n",
    "        \"tta_results/final_avg_confidence\": final_avg_confidence,\n",
    "        \"tta_results/confidence_std\": confidence_std,\n",
    "        \"tta_results/high_confidence_samples\": high_conf_samples,\n",
    "        \"tta_results/high_confidence_ratio\": high_conf_samples / len(all_predictions),\n",
    "        \"tta_results/total_predictions\": len(all_predictions),\n",
    "        \"tta_results/confidence_histogram\": wandb.Histogram(all_confidences),\n",
    "        \"tta_results/progress_table\": tta_progress\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n ÏïôÏÉÅÎ∏î TTA Ï∂îÎ°† ÏôÑÎ£å!\")\n",
    "    print(f\"Ï¥ù ÏÜåÏöîÏãúÍ∞Ñ: {total_time/60:.1f}Î∂Ñ\")\n",
    "    print(f\" ÌèâÍ∑† Ïã†Î¢∞ÎèÑ: {final_avg_confidence:.4f} ¬± {confidence_std:.4f}\")\n",
    "    print(f\" Í≥†Ïã†Î¢∞ÎèÑ ÏÉòÌîå: {high_conf_samples}/{len(all_predictions)} ({high_conf_samples/len(all_predictions)*100:.1f}%)\")\n",
    "    \n",
    "    return all_predictions, all_confidences\n",
    "\n",
    "# ÏïôÏÉÅÎ∏î TTA Ïã§Ìñâ\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" ÏµúÏ¢Ö Ï∂îÎ°† - ÏïôÏÉÅÎ∏î + TTA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "tta_predictions, confidences = ensemble_tta_inference_with_logging(\n",
    "    models=ensemble_models, \n",
    "    loader=tta_loader, \n",
    "    transforms=essential_tta_transforms,\n",
    "    confidence_threshold=0.9\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98e388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TTA (Test Time Augmentation) ÏÑ§Ï†ï...\n",
      "TTA Î≥ÄÌôò 5Í∞ú Ï§ÄÎπÑ ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 18. Final Results and Submission\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n ÏµúÏ¢Ö Í≤∞Í≥º Ï†ïÎ¶¨ Ï§ë...\")\n",
    "\n",
    "# TTA Í≤∞Í≥ºÎ°ú submission ÌååÏùº ÏÉùÏÑ±\n",
    "tta_pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n",
    "tta_pred_df['target'] = tta_predictions\n",
    "\n",
    "# Í∏∞Ï°¥ submissionÍ≥º ÎèôÏùºÌïú ÏàúÏÑúÏù∏ÏßÄ ÌôïÏù∏\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == tta_pred_df['ID']).all(), \"ID ÏàúÏÑú Î∂àÏùºÏπò!\"\n",
    "\n",
    "# ÏòàÏ∏° Î∂ÑÌè¨ Î∂ÑÏÑù\n",
    "pred_distribution = tta_pred_df['target'].value_counts().sort_index()\n",
    "pred_table = wandb.Table(columns=[\"Class\", \"Count\", \"Percentage\"])\n",
    "\n",
    "print(f\"\\nüìä ÏòàÏ∏° Í≤∞Í≥º Î∂ÑÌè¨:\")\n",
    "for class_id in range(17):\n",
    "    count = pred_distribution.get(class_id, 0)\n",
    "    percentage = count / len(tta_pred_df) * 100\n",
    "    pred_table.add_data(class_id, count, percentage)\n",
    "    print(f\"Class {class_id:2d}: {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "# Ïã†Î¢∞ÎèÑ Î∂ÑÏÑù\n",
    "confidence_bins = [0.5, 0.7, 0.8, 0.9, 0.95, 1.0]\n",
    "confidence_analysis = {}\n",
    "for i, threshold in enumerate(confidence_bins):\n",
    "    if i == 0:\n",
    "        count = np.sum(np.array(confidences) >= threshold)\n",
    "    else:\n",
    "        prev_threshold = confidence_bins[i-1]\n",
    "        count = np.sum((np.array(confidences) >= prev_threshold) & (np.array(confidences) < threshold))\n",
    "    confidence_analysis[f\"conf_{threshold}\"] = count\n",
    "\n",
    "# ÏµúÏ¢Ö Í≤∞Í≥º Î°úÍπÖ\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"final_results/total_predictions\": len(tta_predictions),\n",
    "            \"final_results/unique_classes_predicted\": len(np.unique(tta_predictions)),\n",
    "            \"final_results/prediction_distribution_table\": pred_table,\n",
    "            \"final_results/avg_confidence\": np.mean(confidences),\n",
    "            \"final_results/median_confidence\": np.median(confidences),\n",
    "            \"final_results/min_confidence\": np.min(confidences),\n",
    "            \"final_results/max_confidence\": np.max(confidences),\n",
    "            \"final_results/confidence_distribution\": wandb.Histogram(confidences),\n",
    "            **confidence_analysis\n",
    "        })\n",
    "        print(\"ÏµúÏ¢Ö Í≤∞Í≥º WandB Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"WandB Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "# ÏΩòÏÜî Ï∂úÎ†•ÏùÄ Ìï≠ÏÉÅ Ïã§Ìñâ\n",
    "print(f\"Ï¥ù ÏòàÏ∏° Ïàò: {len(tta_predictions)}\")\n",
    "print(f\"ÏòàÏ∏°Îêú ÌÅ¥ÎûòÏä§ Ïàò: {len(np.unique(tta_predictions))}\")\n",
    "print(f\"ÌèâÍ∑† Ïã†Î¢∞ÎèÑ: {np.mean(confidences):.4f}\")\n",
    "print(f\"Ïã†Î¢∞ÎèÑ Î≤îÏúÑ: {np.min(confidences):.4f} ~ {np.max(confidences):.4f}\")\n",
    "\n",
    "\n",
    "# ÏòàÏ∏° Î∂ÑÌè¨ Î∞îÏ∞®Ìä∏\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        pred_dist_data = [[f\"Class_{i}\", pred_distribution.get(i, 0)] for i in range(17)]\n",
    "        wandb.run.log({\n",
    "            \"final_results/prediction_distribution_chart\": wandb.plot.bar(\n",
    "                wandb.Table(data=pred_dist_data, columns=[\"Class\", \"Count\"]),\n",
    "                \"Class\", \"Count\", \n",
    "                title=\"Final Prediction Distribution\"\n",
    "            )\n",
    "        })\n",
    "        print(\"ÏòàÏ∏° Î∂ÑÌè¨ Ï∞®Ìä∏ Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"Ï∞®Ìä∏ Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ï∞®Ìä∏ Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû•\n",
    "output_path = \"../output/choice4.csv\"\n",
    "tta_pred_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Í≤∞Í≥º ÌååÏùºÏùÑ WandB ÏïÑÌã∞Ìå©Ìä∏Î°ú Ï†ÄÏû•\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"final_predictions\",\n",
    "    type=\"predictions\",\n",
    "    description=f\"Final ensemble predictions with {N_FOLDS}-fold CV + TTA\"\n",
    ")\n",
    "artifact.add_file(output_path)\n",
    "\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log_artifact(artifact)\n",
    "        print(\"Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\n ÏµúÏ¢Ö Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å!\")\n",
    "print(f\" ÌååÏùº ÏúÑÏπò: {output_path}\")\n",
    "print(f\" Ï¥ù ÏòàÏ∏° Ïàò: {len(tta_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TTA Dataset: 3140Í∞ú ÌÖåÏä§Ìä∏ ÏÉòÌîå\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 19. Experiment Summary and Cleanup\n",
    "# =============================================================================\n",
    "\n",
    "# Ïã§Ìóò ÏöîÏïΩ ÏÉùÏÑ±\n",
    "experiment_summary = {\n",
    "    \"experiment_name\": main_run.name,\n",
    "    \"model_architecture\": model_name,\n",
    "    \"image_size\": img_size,\n",
    "    \"cv_strategy\": f\"{N_FOLDS}-Fold StratifiedKFold\",\n",
    "    \"cv_mean_f1\": mean_f1,\n",
    "    \"cv_std_f1\": std_f1,\n",
    "    \"cv_best_fold\": max(val_f1_scores),\n",
    "    \"ensemble_models\": len(ensemble_models),\n",
    "    \"tta_transforms\": len(essential_tta_transforms),\n",
    "    \"total_training_time_min\": sum([r['epochs_trained'] for r in fold_results]) * 2,  # Ï∂îÏ†ïÏπò\n",
    "    \"avg_prediction_confidence\": np.mean(confidences),\n",
    "    \"high_confidence_predictions\": np.sum(np.array(confidences) >= 0.9),\n",
    "    \"experiment_tags\": [\"baseline\", \"efficientnet-b3\", \"k-fold-cv\", \"tta\", \"ensemble\"]\n",
    "}\n",
    "\n",
    "# Ïã§Ìóò ÏöîÏïΩ\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\"experiment_summary\": experiment_summary})\n",
    "        print(\"Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ïã§Ìóò ÏöîÏïΩ Î°úÍπÖ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "\n",
    "# ÎßàÏßÄÎßâ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏\n",
    "try:\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.log({\n",
    "            \"status\": \"completed\",\n",
    "            \"completion_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"total_runtime_hours\": 0  # start_time ÏÜçÏÑ± Î¨∏Ï†úÎ°ú ÏùºÎã® 0ÏúºÎ°ú ÏÑ§Ï†ï\n",
    "        })\n",
    "        print(\"ÏµúÏ¢Ö ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        print(\"ÌôúÏÑ±ÌôîÎêú runÏù¥ ÏóÜÏñ¥ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏Î•º Í±¥ÎÑàÎúÅÎãàÎã§.\")\n",
    "except Exception as e:\n",
    "    print(f\"ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ Ï§ë ÏóêÎü¨: {e}\")\n",
    "\n",
    "print(f\"\\nÏã§Ìóò ÏôÑÎ£å ÏãúÍ∞Ñ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Ïã§Ìóò ÏôÑÎ£å!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\" K-Fold CV Í≤∞Í≥º: {mean_f1:.4f} ¬± {std_f1:.4f}\")\n",
    "print(f\" ÏµúÍ≥† ÏÑ±Îä• Fold: {max(val_f1_scores):.4f}\")\n",
    "print(f\" ÏïôÏÉÅÎ∏î Î™®Îç∏: {len(ensemble_models)}Í∞ú\")\n",
    "print(f\" TTA Î≥ÄÌòï: {len(essential_tta_transforms)}Í∞ú\")\n",
    "print(f\" ÌèâÍ∑† ÏòàÏ∏° Ïã†Î¢∞ÎèÑ: {np.mean(confidences):.4f}\")\n",
    "print(f\" WandB ÎåÄÏãúÎ≥¥Îìú: {main_run.url}\")\n",
    "\n",
    "# Sample predictions Ï∂úÎ†•\n",
    "print(f\"\\n ÏòàÏ∏° Í≤∞Í≥º ÏÉòÌîå:\")\n",
    "print(tta_pred_df.head(10))\n",
    "\n",
    "# Î©îÏù∏ run Ï¢ÖÎ£å\n",
    "main_run.finish()\n",
    "\n",
    "print(f\"\\n Î™®Îì† ÏûëÏóÖ ÏôÑÎ£å!\")\n",
    "print(f\" Í≤∞Í≥º ÌååÏùº: {output_path}\")\n",
    "print(f\" WandBÏóêÏÑú Ï†ÑÏ≤¥ Ïã§Ìóò Í≤∞Í≥ºÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî!\")\n",
    "\n",
    "# Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "del ensemble_models\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
